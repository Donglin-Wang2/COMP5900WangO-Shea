{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(f\"gpu: { len(tf.config.list_physical_devices('GPU')) }\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_adapted import VAEAdapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(model_name, batch_size=64):\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated_%s_feat.npy' % model_name)\n",
    "    imgs = np.load('./data/LFSD_imgs_%s_feat.npy' % model_name)\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        dataset.append((img_batch, depth_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'vgg': 512,\n",
    "        'efficientnet': 1280,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAEAdapted(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Training Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "\n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(f\"Epoch {i} Total loss: { losses_across_epochs['loss'][-1]}\")\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_adapted/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        np.save('./results/vae_adapted/%s_%s' % (model_name, k), np.array(v))\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_adapted/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_adapted/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "    from skimage.transform import resize\n",
    "    from tensorflow.keras import datasets\n",
    "\n",
    "    train_dataset, test_dataset = None, None\n",
    " \n",
    "    train_feats = np.load('./data/CIFAR100_%s_train_feat.npy' % model_name)\n",
    "    test_feats = np.load('./data/CIFAR100_%s_test_feat.npy' % model_name)\n",
    "\n",
    "    train_result, _, _ = vae.encode(train_feats[:128], tf.random.normal(train_feats[:128].shape))\n",
    "    for i in range(128, len(train_feats), 128):\n",
    "        activation, _, _ = vae.encode(train_feats[i:i+128], tf.random.normal(train_feats[i:i+128].shape))\n",
    "        train_result = tf.concat((train_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_adapted_%s_encoding_train.npy' % model_name, np.array(train_result))\n",
    "\n",
    "    test_result, _, _ = vae.encode(test_feats[:128], tf.random.normal(test_feats[:128].shape))\n",
    "    for i in range(128, len(test_feats), 128):\n",
    "        activation, _, _ = vae.encode(test_feats[i:i+128], tf.random.normal(test_feats[i:i+128].shape))\n",
    "        test_result = tf.concat((test_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_adapted_%s_encoding_test.npy' % model_name, np.array(test_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 6701287708.444446\n",
      "Epoch 1 Total loss: 1822572771.5555556\n",
      "Epoch 2 Total loss: 1087946837.3333335\n",
      "Epoch 3 Total loss: 777895637.3333335\n",
      "Epoch 4 Total loss: 606177358.2222222\n",
      "Epoch 5 Total loss: 496903875.5555555\n",
      "Epoch 6 Total loss: 421198531.5555556\n",
      "Epoch 7 Total loss: 365632465.7777778\n",
      "Epoch 8 Total loss: 323101504.0\n",
      "Epoch 9 Total loss: 289487164.4444444\n",
      "Epoch 10 Total loss: 262235896.8888889\n",
      "Epoch 11 Total loss: 239691393.77777782\n",
      "Epoch 12 Total loss: 220732826.6666667\n",
      "Epoch 13 Total loss: 204569015.1111111\n",
      "Epoch 14 Total loss: 190625360.0\n",
      "Epoch 15 Total loss: 178474538.6666667\n",
      "Epoch 16 Total loss: 167791751.1111111\n",
      "Epoch 17 Total loss: 158326161.7777778\n",
      "Epoch 18 Total loss: 149881168.0\n",
      "Epoch 19 Total loss: 142300074.66666666\n",
      "Epoch 20 Total loss: 135456757.33333334\n",
      "Epoch 21 Total loss: 129248542.22222222\n",
      "Epoch 22 Total loss: 123590904.88888888\n",
      "Epoch 23 Total loss: 118413798.22222222\n",
      "Epoch 24 Total loss: 113658283.55555555\n",
      "Epoch 25 Total loss: 109274858.66666667\n",
      "Epoch 26 Total loss: 105221627.55555555\n",
      "Epoch 27 Total loss: 101462550.22222222\n",
      "Epoch 28 Total loss: 97966781.33333334\n",
      "Epoch 29 Total loss: 94707537.77777776\n",
      "Epoch 30 Total loss: 91661627.55555557\n",
      "Epoch 31 Total loss: 88808722.66666666\n",
      "Epoch 32 Total loss: 86131166.22222222\n",
      "Epoch 33 Total loss: 83613180.44444445\n",
      "Epoch 34 Total loss: 81240894.22222222\n",
      "Epoch 35 Total loss: 79002056.0\n",
      "Epoch 36 Total loss: 76885697.77777779\n",
      "Epoch 37 Total loss: 74882078.22222222\n",
      "Epoch 38 Total loss: 72982382.22222222\n",
      "Epoch 39 Total loss: 71178806.22222222\n",
      "Epoch 40 Total loss: 69464165.33333334\n",
      "Epoch 41 Total loss: 67832022.22222222\n",
      "Epoch 42 Total loss: 66276515.11111111\n",
      "Epoch 43 Total loss: 64792445.33333333\n",
      "Epoch 44 Total loss: 63374992.44444443\n",
      "Epoch 45 Total loss: 62019763.111111104\n",
      "Epoch 46 Total loss: 60722743.111111104\n",
      "Epoch 47 Total loss: 59480258.222222224\n",
      "Epoch 48 Total loss: 58288976.888888896\n",
      "Epoch 49 Total loss: 57145751.55555556\n",
      "Epoch 50 Total loss: 56047739.111111104\n",
      "Epoch 51 Total loss: 54992271.11111112\n",
      "Epoch 52 Total loss: 53977090.66666667\n",
      "Epoch 53 Total loss: 52999832.00000001\n",
      "Epoch 54 Total loss: 52058384.88888889\n",
      "Epoch 55 Total loss: 51150831.99999999\n",
      "Epoch 56 Total loss: 50275366.222222224\n",
      "Epoch 57 Total loss: 49430375.11111111\n",
      "Epoch 58 Total loss: 48614177.333333336\n",
      "Epoch 59 Total loss: 47825322.222222224\n",
      "Epoch 60 Total loss: 47062746.666666664\n",
      "Epoch 61 Total loss: 46324832.0\n",
      "Epoch 62 Total loss: 45610416.00000001\n",
      "Epoch 63 Total loss: 44918442.666666664\n",
      "Epoch 64 Total loss: 44247914.666666664\n",
      "Epoch 65 Total loss: 43597810.666666664\n",
      "Epoch 66 Total loss: 42967296.0\n",
      "Epoch 67 Total loss: 42355426.222222224\n",
      "Epoch 68 Total loss: 41761349.777777776\n",
      "Epoch 69 Total loss: 41184358.66666667\n",
      "Epoch 70 Total loss: 40623743.11111111\n",
      "Epoch 71 Total loss: 40078785.777777776\n",
      "Epoch 72 Total loss: 39548876.0\n",
      "Epoch 73 Total loss: 39033365.77777778\n",
      "Epoch 74 Total loss: 38531693.777777776\n",
      "Epoch 75 Total loss: 38043282.66666667\n",
      "Epoch 76 Total loss: 37567624.44444444\n",
      "Epoch 77 Total loss: 37104200.0\n",
      "Epoch 78 Total loss: 36652598.666666664\n",
      "Epoch 79 Total loss: 36212401.333333336\n",
      "Epoch 80 Total loss: 35783093.777777776\n",
      "Epoch 81 Total loss: 35364356.888888896\n",
      "Epoch 82 Total loss: 34955768.44444444\n",
      "Epoch 83 Total loss: 34556927.55555556\n",
      "Epoch 84 Total loss: 34167507.55555555\n",
      "Epoch 85 Total loss: 33787189.333333336\n",
      "Epoch 86 Total loss: 33415614.666666668\n",
      "Epoch 87 Total loss: 33052563.777777772\n",
      "Epoch 88 Total loss: 32697749.333333332\n",
      "Epoch 89 Total loss: 32350835.555555552\n",
      "Epoch 90 Total loss: 32011587.555555552\n",
      "Epoch 91 Total loss: 31679807.111111112\n",
      "Epoch 92 Total loss: 31355145.77777778\n",
      "Epoch 93 Total loss: 31037457.777777776\n",
      "Epoch 94 Total loss: 30726504.0\n",
      "Epoch 95 Total loss: 30421996.888888888\n",
      "Epoch 96 Total loss: 30123756.444444448\n",
      "Epoch 97 Total loss: 29831595.777777776\n",
      "Epoch 98 Total loss: 29545363.111111112\n",
      "Epoch 99 Total loss: 29264877.555555556\n",
      "Gen encoding...\n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2678393.611111111\n",
      "Epoch 1 Total loss: 2877136.166666667\n",
      "Epoch 2 Total loss: 2823665.472222222\n",
      "Epoch 3 Total loss: 2713825.9166666665\n",
      "Epoch 4 Total loss: 2550008.3333333335\n",
      "Epoch 5 Total loss: 2362995.8055555555\n",
      "Epoch 6 Total loss: 2182455.75\n",
      "Epoch 7 Total loss: 2027049.5694444445\n",
      "Epoch 8 Total loss: 1899011.9444444445\n",
      "Epoch 9 Total loss: 1786097.4444444445\n",
      "Epoch 10 Total loss: 1685046.6666666665\n",
      "Epoch 11 Total loss: 1598726.0277777778\n",
      "Epoch 12 Total loss: 1522063.8472222222\n",
      "Epoch 13 Total loss: 1931677.1527777775\n",
      "Epoch 14 Total loss: 1921891.6249999995\n",
      "Epoch 15 Total loss: 1843871.8194444443\n",
      "Epoch 16 Total loss: 1772784.125\n",
      "Epoch 17 Total loss: 1706228.25\n",
      "Epoch 18 Total loss: 1643510.652777778\n",
      "Epoch 19 Total loss: 1585482.1388888885\n",
      "Epoch 20 Total loss: 1532683.3472222225\n",
      "Epoch 21 Total loss: 1485591.5694444445\n",
      "Epoch 22 Total loss: 1441897.9722222222\n",
      "Epoch 23 Total loss: 1401952.3611111112\n",
      "Epoch 24 Total loss: 1364673.4583333333\n",
      "Epoch 25 Total loss: 1329465.1388888888\n",
      "Epoch 26 Total loss: 1296200.8472222222\n",
      "Epoch 27 Total loss: 1263854.5555555555\n",
      "Epoch 28 Total loss: 1233048.0555555555\n",
      "Epoch 29 Total loss: 1204114.5833333333\n",
      "Epoch 30 Total loss: 1177060.0555555555\n",
      "Epoch 31 Total loss: 1151310.6666666665\n",
      "Epoch 32 Total loss: 1127111.7916666665\n",
      "Epoch 33 Total loss: 1104537.5555555555\n",
      "Epoch 34 Total loss: 1083808.986111111\n",
      "Epoch 35 Total loss: 1064108.638888889\n",
      "Epoch 36 Total loss: 1044986.8819444444\n",
      "Epoch 37 Total loss: 1026242.6597222222\n",
      "Epoch 38 Total loss: 1008155.7430555556\n",
      "Epoch 39 Total loss: 990859.4375\n",
      "Epoch 40 Total loss: 974379.4999999999\n",
      "Epoch 41 Total loss: 958560.4583333333\n",
      "Epoch 42 Total loss: 943248.3888888888\n",
      "Epoch 43 Total loss: 928614.1597222221\n",
      "Epoch 44 Total loss: 914724.1180555555\n",
      "Epoch 45 Total loss: 901683.1666666667\n",
      "Epoch 46 Total loss: 889127.7222222222\n",
      "Epoch 47 Total loss: 876803.1736111111\n",
      "Epoch 48 Total loss: 864815.2708333333\n",
      "Epoch 49 Total loss: 853286.2916666667\n",
      "Epoch 50 Total loss: 842389.9930555555\n",
      "Epoch 51 Total loss: 831837.9652777778\n",
      "Epoch 52 Total loss: 821546.0347222222\n",
      "Epoch 53 Total loss: 811682.0902777779\n",
      "Epoch 54 Total loss: 802103.2986111111\n",
      "Epoch 55 Total loss: 792656.4444444445\n",
      "Epoch 56 Total loss: 783381.6041666667\n",
      "Epoch 57 Total loss: 774361.826388889\n",
      "Epoch 58 Total loss: 765605.9722222222\n",
      "Epoch 59 Total loss: 757074.8819444445\n",
      "Epoch 60 Total loss: 748786.8472222222\n",
      "Epoch 61 Total loss: 740778.9097222222\n",
      "Epoch 62 Total loss: 733079.2499999999\n",
      "Epoch 63 Total loss: 725578.9513888889\n",
      "Epoch 64 Total loss: 718318.1666666667\n",
      "Epoch 65 Total loss: 711326.2222222222\n",
      "Epoch 66 Total loss: 704553.1458333333\n",
      "Epoch 67 Total loss: 697909.3958333333\n",
      "Epoch 68 Total loss: 691319.5\n",
      "Epoch 69 Total loss: 684844.5416666667\n",
      "Epoch 70 Total loss: 678495.6944444444\n",
      "Epoch 71 Total loss: 672298.9791666666\n",
      "Epoch 72 Total loss: 666353.6527777778\n",
      "Epoch 73 Total loss: 660739.6041666667\n",
      "Epoch 74 Total loss: 655479.9791666665\n",
      "Epoch 75 Total loss: 650137.2777777779\n",
      "Epoch 76 Total loss: 644756.3611111111\n",
      "Epoch 77 Total loss: 639440.2708333333\n",
      "Epoch 78 Total loss: 634246.3402777778\n",
      "Epoch 79 Total loss: 629129.8472222222\n",
      "Epoch 80 Total loss: 624052.1736111111\n",
      "Epoch 81 Total loss: 619076.673611111\n",
      "Epoch 82 Total loss: 614195.7847222222\n",
      "Epoch 83 Total loss: 609489.1180555555\n",
      "Epoch 84 Total loss: 604855.7013888888\n",
      "Epoch 85 Total loss: 600349.8402777776\n",
      "Epoch 86 Total loss: 595986.125\n",
      "Epoch 87 Total loss: 591739.1180555556\n",
      "Epoch 88 Total loss: 587537.9722222222\n",
      "Epoch 89 Total loss: 583365.3611111111\n",
      "Epoch 90 Total loss: 579241.1666666666\n",
      "Epoch 91 Total loss: 575227.3263888889\n",
      "Epoch 92 Total loss: 571345.5625\n",
      "Epoch 93 Total loss: 567482.7152777778\n",
      "Epoch 94 Total loss: 563661.1249999999\n",
      "Epoch 95 Total loss: 559912.0208333333\n",
      "Epoch 96 Total loss: 556250.3541666666\n",
      "Epoch 97 Total loss: 552647.4375\n",
      "Epoch 98 Total loss: 549107.3055555556\n",
      "Epoch 99 Total loss: 545619.7777777778\n",
      "Gen encoding...\n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 5015013.138888888\n",
      "Epoch 1 Total loss: 3599627.888888889\n",
      "Epoch 2 Total loss: 3360553.111111111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Total loss: 3223564.916666667\n",
      "Epoch 4 Total loss: 3066423.4166666665\n",
      "Epoch 5 Total loss: 2877127.361111111\n",
      "Epoch 6 Total loss: 2680900.0\n",
      "Epoch 7 Total loss: 2494427.5833333335\n",
      "Epoch 8 Total loss: 2323502.0555555555\n",
      "Epoch 9 Total loss: 2168953.5833333335\n",
      "Epoch 10 Total loss: 2033768.3055555557\n",
      "Epoch 11 Total loss: 1917383.1944444445\n",
      "Epoch 12 Total loss: 1816448.5555555555\n",
      "Epoch 13 Total loss: 1728045.9305555557\n",
      "Epoch 14 Total loss: 1649026.1805555553\n",
      "Epoch 15 Total loss: 1577662.2777777778\n",
      "Epoch 16 Total loss: 1512059.1805555555\n",
      "Epoch 17 Total loss: 1452265.9722222222\n",
      "Epoch 18 Total loss: 1397472.7916666665\n",
      "Epoch 19 Total loss: 1347080.236111111\n",
      "Epoch 20 Total loss: 1300477.4861111115\n",
      "Epoch 21 Total loss: 1257620.4166666665\n",
      "Epoch 22 Total loss: 1219028.1666666667\n",
      "Epoch 23 Total loss: 1183870.7083333333\n",
      "Epoch 24 Total loss: 1150981.5277777778\n",
      "Epoch 25 Total loss: 1119828.4305555555\n",
      "Epoch 26 Total loss: 1090188.3611111112\n",
      "Epoch 27 Total loss: 1062228.625\n",
      "Epoch 28 Total loss: 1036089.0972222222\n",
      "Epoch 29 Total loss: 1011758.3958333333\n",
      "Epoch 30 Total loss: 989107.6666666667\n",
      "Epoch 31 Total loss: 967824.9444444444\n",
      "Epoch 32 Total loss: 947515.486111111\n",
      "Epoch 33 Total loss: 927955.9375\n",
      "Epoch 34 Total loss: 909318.3402777778\n",
      "Epoch 35 Total loss: 891751.2361111111\n",
      "Epoch 36 Total loss: 875228.7291666666\n",
      "Epoch 37 Total loss: 859543.4444444444\n",
      "Epoch 38 Total loss: 844675.7430555555\n",
      "Epoch 39 Total loss: 830678.7847222222\n",
      "Epoch 40 Total loss: 817179.1875\n",
      "Epoch 41 Total loss: 803917.5208333333\n",
      "Epoch 42 Total loss: 791071.6736111112\n",
      "Epoch 43 Total loss: 778733.0555555556\n",
      "Epoch 44 Total loss: 767045.9930555555\n",
      "Epoch 45 Total loss: 755936.1180555556\n",
      "Epoch 46 Total loss: 745358.1944444445\n",
      "Epoch 47 Total loss: 735168.3888888889\n",
      "Epoch 48 Total loss: 725349.3819444445\n",
      "Epoch 49 Total loss: 715894.9027777778\n",
      "Epoch 50 Total loss: 706731.4791666666\n",
      "Epoch 51 Total loss: 697820.2291666666\n",
      "Epoch 52 Total loss: 689201.1041666667\n",
      "Epoch 53 Total loss: 680822.7916666666\n",
      "Epoch 54 Total loss: 672655.3819444445\n",
      "Epoch 55 Total loss: 664703.6597222222\n",
      "Epoch 56 Total loss: 657004.9305555555\n",
      "Epoch 57 Total loss: 649549.9166666666\n",
      "Epoch 58 Total loss: 642407.25\n",
      "Epoch 59 Total loss: 635572.0972222222\n",
      "Epoch 60 Total loss: 628902.0694444444\n",
      "Epoch 61 Total loss: 622349.1736111111\n",
      "Epoch 62 Total loss: 615983.9513888889\n",
      "Epoch 63 Total loss: 609795.736111111\n",
      "Epoch 64 Total loss: 603809.1041666666\n",
      "Epoch 65 Total loss: 598053.125\n",
      "Epoch 66 Total loss: 592475.388888889\n",
      "Epoch 67 Total loss: 586910.4305555556\n",
      "Epoch 68 Total loss: 581420.6111111111\n",
      "Epoch 69 Total loss: 576017.0625\n",
      "Epoch 70 Total loss: 570759.9166666666\n",
      "Epoch 71 Total loss: 565650.5694444445\n",
      "Epoch 72 Total loss: 560703.1875\n",
      "Epoch 73 Total loss: 555928.4166666666\n",
      "Epoch 74 Total loss: 551283.0763888889\n",
      "Epoch 75 Total loss: 546688.5208333333\n",
      "Epoch 76 Total loss: 542186.2430555556\n",
      "Epoch 77 Total loss: 537825.4722222222\n",
      "Epoch 78 Total loss: 533558.0833333334\n",
      "Epoch 79 Total loss: 529367.4375\n",
      "Epoch 80 Total loss: 525235.5347222222\n",
      "Epoch 81 Total loss: 521176.90972222225\n",
      "Epoch 82 Total loss: 517167.74652777775\n",
      "Epoch 83 Total loss: 513279.06944444444\n",
      "Epoch 84 Total loss: 509486.6875\n",
      "Epoch 85 Total loss: 505778.5\n",
      "Epoch 86 Total loss: 502115.4444444444\n",
      "Epoch 87 Total loss: 498519.10069444455\n",
      "Epoch 88 Total loss: 495007.4826388889\n",
      "Epoch 89 Total loss: 491578.6666666667\n",
      "Epoch 90 Total loss: 488188.6909722222\n",
      "Epoch 91 Total loss: 484889.8263888889\n",
      "Epoch 92 Total loss: 481646.5347222222\n",
      "Epoch 93 Total loss: 478444.19097222213\n",
      "Epoch 94 Total loss: 475293.5798611111\n",
      "Epoch 95 Total loss: 472215.5798611111\n",
      "Epoch 96 Total loss: 469198.40277777775\n",
      "Epoch 97 Total loss: 466235.8298611111\n",
      "Epoch 98 Total loss: 463319.0520833333\n",
      "Epoch 99 Total loss: 460456.2604166667\n",
      "Gen encoding...\n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2943811.3888888885\n",
      "Epoch 1 Total loss: 2733725.916666667\n",
      "Epoch 2 Total loss: 2695135.777777778\n",
      "Epoch 3 Total loss: 2636694.944444445\n",
      "Epoch 4 Total loss: 2535986.666666667\n",
      "Epoch 5 Total loss: 2421494.3333333335\n",
      "Epoch 6 Total loss: 2320646.0000000005\n",
      "Epoch 7 Total loss: 2238319.3333333335\n",
      "Epoch 8 Total loss: 2171206.027777778\n",
      "Epoch 9 Total loss: 2115847.083333333\n",
      "Epoch 10 Total loss: 2069779.6805555557\n",
      "Epoch 11 Total loss: 2030518.4166666667\n",
      "Epoch 12 Total loss: 1996880.263888889\n",
      "Epoch 13 Total loss: 1967623.9583333335\n",
      "Epoch 14 Total loss: 1942096.5694444445\n",
      "Epoch 15 Total loss: 1919309.6944444447\n",
      "Epoch 16 Total loss: 1899188.1666666665\n",
      "Epoch 17 Total loss: 1881143.3055555555\n",
      "Epoch 18 Total loss: 1865323.5000000002\n",
      "Epoch 19 Total loss: 1851245.9583333335\n",
      "Epoch 20 Total loss: 1837680.597222222\n",
      "Epoch 21 Total loss: 1825559.236111111\n",
      "Epoch 22 Total loss: 1813562.013888889\n",
      "Epoch 23 Total loss: 1802748.0416666667\n",
      "Epoch 24 Total loss: 1792863.6111111112\n",
      "Epoch 25 Total loss: 1783459.5833333333\n",
      "Epoch 26 Total loss: 1774536.6944444447\n",
      "Epoch 27 Total loss: 1767014.9305555557\n",
      "Epoch 28 Total loss: 1759927.138888889\n",
      "Epoch 29 Total loss: 1752262.861111111\n",
      "Epoch 30 Total loss: 1745420.375\n",
      "Epoch 31 Total loss: 1738520.8333333333\n",
      "Epoch 32 Total loss: 1731287.5\n",
      "Epoch 33 Total loss: 1725113.6944444445\n",
      "Epoch 34 Total loss: 1719272.2916666665\n",
      "Epoch 35 Total loss: 1712642.5555555555\n",
      "Epoch 36 Total loss: 1706800.3749999998\n",
      "Epoch 37 Total loss: 1701400.166666667\n",
      "Epoch 38 Total loss: 1695345.0833333335\n",
      "Epoch 39 Total loss: 1691334.8750000002\n",
      "Epoch 40 Total loss: 1685820.638888889\n",
      "Epoch 41 Total loss: 1680078.097222222\n",
      "Epoch 42 Total loss: 1675589.9444444445\n",
      "Epoch 43 Total loss: 1670523.1944444443\n",
      "Epoch 44 Total loss: 1664759.0555555557\n",
      "Epoch 45 Total loss: 1659992.0833333335\n",
      "Epoch 46 Total loss: 1656902.361111111\n",
      "Epoch 47 Total loss: 1652452.6805555555\n",
      "Epoch 48 Total loss: 1649356.5833333333\n",
      "Epoch 49 Total loss: 1647760.138888889\n",
      "Epoch 50 Total loss: 1644396.1944444445\n",
      "Epoch 51 Total loss: 1640483.6805555555\n",
      "Epoch 52 Total loss: 1636471.4027777775\n",
      "Epoch 53 Total loss: 1632874.3333333333\n",
      "Epoch 54 Total loss: 1629166.7638888885\n",
      "Epoch 55 Total loss: 1625113.9722222222\n",
      "Epoch 56 Total loss: 1620597.5555555555\n",
      "Epoch 57 Total loss: 1618692.277777778\n",
      "Epoch 58 Total loss: 1616342.875\n",
      "Epoch 59 Total loss: 1612974.3055555555\n",
      "Epoch 60 Total loss: 1609027.6805555555\n",
      "Epoch 61 Total loss: 1606702.9444444445\n",
      "Epoch 62 Total loss: 1603306.0833333335\n",
      "Epoch 63 Total loss: 1599259.1805555555\n",
      "Epoch 64 Total loss: 1595359.2777777775\n",
      "Epoch 65 Total loss: 1592344.263888889\n",
      "Epoch 66 Total loss: 1589152.972222222\n",
      "Epoch 67 Total loss: 1586408.125\n",
      "Epoch 68 Total loss: 1583451.3472222222\n",
      "Epoch 69 Total loss: 1580020.0\n",
      "Epoch 70 Total loss: 1575682.7916666667\n",
      "Epoch 71 Total loss: 1571139.0555555555\n",
      "Epoch 72 Total loss: 1568930.4444444447\n",
      "Epoch 73 Total loss: 1567010.3611111112\n",
      "Epoch 74 Total loss: 1564569.4027777775\n",
      "Epoch 75 Total loss: 1561835.375\n",
      "Epoch 76 Total loss: 1558217.638888889\n",
      "Epoch 77 Total loss: 1554860.1527777775\n",
      "Epoch 78 Total loss: 1552852.75\n",
      "Epoch 79 Total loss: 1550403.8194444445\n",
      "Epoch 80 Total loss: 1547005.527777778\n",
      "Epoch 81 Total loss: 1542979.0416666665\n",
      "Epoch 82 Total loss: 1541332.5277777775\n",
      "Epoch 83 Total loss: 1540915.1666666667\n",
      "Epoch 84 Total loss: 1539318.2916666667\n",
      "Epoch 85 Total loss: 1536949.0138888888\n",
      "Epoch 86 Total loss: 1533782.2222222222\n",
      "Epoch 87 Total loss: 1530561.8472222225\n",
      "Epoch 88 Total loss: 1527471.4861111112\n",
      "Epoch 89 Total loss: 1524165.1388888888\n",
      "Epoch 90 Total loss: 1521332.0833333333\n",
      "Epoch 91 Total loss: 1518814.3611111112\n",
      "Epoch 92 Total loss: 1516774.972222222\n",
      "Epoch 93 Total loss: 1514511.5000000002\n",
      "Epoch 94 Total loss: 1511527.3611111112\n",
      "Epoch 95 Total loss: 1508060.3055555557\n",
      "Epoch 96 Total loss: 1504655.7777777778\n",
      "Epoch 97 Total loss: 1501106.3194444445\n",
      "Epoch 98 Total loss: 1497790.0138888888\n",
      "Epoch 99 Total loss: 1495314.9722222222\n",
      "Gen encoding...\n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 210958643.55555555\n",
      "Epoch 1 Total loss: 59770801.77777779\n",
      "Epoch 2 Total loss: 37051443.11111111\n",
      "Epoch 3 Total loss: 27330837.555555556\n",
      "Epoch 4 Total loss: 21910280.888888888\n",
      "Epoch 5 Total loss: 18442966.22222222\n",
      "Epoch 6 Total loss: 16018109.777777776\n",
      "Epoch 7 Total loss: 14209714.888888888\n",
      "Epoch 8 Total loss: 12797808.222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Total loss: 11663422.333333332\n",
      "Epoch 10 Total loss: 10734816.555555556\n",
      "Epoch 11 Total loss: 9962376.444444444\n",
      "Epoch 12 Total loss: 9310329.111111112\n",
      "Epoch 13 Total loss: 8752985.0\n",
      "Epoch 14 Total loss: 8271071.944444444\n",
      "Epoch 15 Total loss: 7850385.555555554\n",
      "Epoch 16 Total loss: 7479714.166666668\n",
      "Epoch 17 Total loss: 7150564.277777778\n",
      "Epoch 18 Total loss: 6856487.166666666\n",
      "Epoch 19 Total loss: 6592157.888888889\n",
      "Epoch 20 Total loss: 6353387.888888889\n",
      "Epoch 21 Total loss: 6136516.333333333\n",
      "Epoch 22 Total loss: 5938670.166666666\n",
      "Epoch 23 Total loss: 5757357.999999999\n",
      "Epoch 24 Total loss: 5590640.277777778\n",
      "Epoch 25 Total loss: 5436813.555555555\n",
      "Epoch 26 Total loss: 5294514.111111111\n",
      "Epoch 27 Total loss: 5162458.111111111\n",
      "Epoch 28 Total loss: 5039571.166666666\n",
      "Epoch 29 Total loss: 4924826.833333334\n",
      "Epoch 30 Total loss: 4817510.833333334\n",
      "Epoch 31 Total loss: 4716937.611111111\n",
      "Epoch 32 Total loss: 4622615.944444444\n",
      "Epoch 33 Total loss: 4533849.055555556\n",
      "Epoch 34 Total loss: 4450161.333333333\n",
      "Epoch 35 Total loss: 4371077.555555556\n",
      "Epoch 36 Total loss: 4296241.944444444\n",
      "Epoch 37 Total loss: 4225421.111111111\n",
      "Epoch 38 Total loss: 4158161.6666666665\n",
      "Epoch 39 Total loss: 4094314.888888889\n",
      "Epoch 40 Total loss: 4033598.1944444445\n",
      "Epoch 41 Total loss: 3975761.194444444\n",
      "Epoch 42 Total loss: 3920583.222222223\n",
      "Epoch 43 Total loss: 3867905.805555556\n",
      "Epoch 44 Total loss: 3817603.583333334\n",
      "Epoch 45 Total loss: 3769496.444444445\n",
      "Epoch 46 Total loss: 3723487.388888889\n",
      "Epoch 47 Total loss: 3679337.861111111\n",
      "Epoch 48 Total loss: 3636961.166666667\n",
      "Epoch 49 Total loss: 3596301.0277777775\n",
      "Epoch 50 Total loss: 3557243.5555555555\n",
      "Epoch 51 Total loss: 3519671.75\n",
      "Epoch 52 Total loss: 3483563.777777777\n",
      "Epoch 53 Total loss: 3448756.9444444445\n",
      "Epoch 54 Total loss: 3415222.611111111\n",
      "Epoch 55 Total loss: 3382833.1944444445\n",
      "Epoch 56 Total loss: 3351587.222222222\n",
      "Epoch 57 Total loss: 3321473.8888888895\n",
      "Epoch 58 Total loss: 3292338.666666667\n",
      "Epoch 59 Total loss: 3264195.1944444445\n",
      "Epoch 60 Total loss: 3236976.055555555\n",
      "Epoch 61 Total loss: 3210648.1388888895\n",
      "Epoch 62 Total loss: 3185147.055555555\n",
      "Epoch 63 Total loss: 3160427.194444445\n",
      "Epoch 64 Total loss: 3136487.833333333\n",
      "Epoch 65 Total loss: 3113287.75\n",
      "Epoch 66 Total loss: 3090739.388888889\n",
      "Epoch 67 Total loss: 3068872.361111111\n",
      "Epoch 68 Total loss: 3047623.111111111\n",
      "Epoch 69 Total loss: 3027012.6666666665\n",
      "Epoch 70 Total loss: 3006962.4722222225\n",
      "Epoch 71 Total loss: 2987440.3888888885\n",
      "Epoch 72 Total loss: 2968478.25\n",
      "Epoch 73 Total loss: 2950019.611111111\n",
      "Epoch 74 Total loss: 2932074.305555555\n",
      "Epoch 75 Total loss: 2914582.4166666665\n",
      "Epoch 76 Total loss: 2897522.1944444445\n",
      "Epoch 77 Total loss: 2880917.8611111115\n",
      "Epoch 78 Total loss: 2864725.472222222\n",
      "Epoch 79 Total loss: 2848928.916666667\n",
      "Epoch 80 Total loss: 2833524.75\n",
      "Epoch 81 Total loss: 2818465.611111111\n",
      "Epoch 82 Total loss: 2803821.722222222\n",
      "Epoch 83 Total loss: 2789527.027777778\n",
      "Epoch 84 Total loss: 2775539.055555556\n",
      "Epoch 85 Total loss: 2761866.972222222\n",
      "Epoch 86 Total loss: 2748516.9722222225\n",
      "Epoch 87 Total loss: 2735477.472222222\n",
      "Epoch 88 Total loss: 2722744.527777778\n",
      "Epoch 89 Total loss: 2710265.166666667\n",
      "Epoch 90 Total loss: 2698129.9999999995\n",
      "Epoch 91 Total loss: 2686265.333333333\n",
      "Epoch 92 Total loss: 2674627.5555555555\n",
      "Epoch 93 Total loss: 2663200.305555555\n",
      "Epoch 94 Total loss: 2652025.0\n",
      "Epoch 95 Total loss: 2641062.694444445\n",
      "Epoch 96 Total loss: 2630364.027777778\n",
      "Epoch 97 Total loss: 2619866.1944444445\n",
      "Epoch 98 Total loss: 2609556.1666666665\n",
      "Epoch 99 Total loss: 2599486.2777777775\n",
      "Gen encoding...\n",
      "Wall time: 10min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "for model_name in ['efficientnet','inception', 'resnet', 'mobilenet', 'vgg']:\n",
    "    train_dataset, test_dataset = load_data_for_model(model_name)\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(\"Gen encoding...\")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
