{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(f\"gpu: { len(tf.config.list_physical_devices('GPU')) }\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_injected import VAEInjected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(model_name, batch_size=64):\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated.npy')\n",
    "    imgs = np.load('./data/LFSD_imgs.npy')\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    depths_feat = np.load('./data/LFSD_depths_repeated_%s_feat.npy' % model_name)\n",
    "    imgs_feat = np.load('./data/LFSD_imgs_%s_feat.npy' % model_name)\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        img_feat_batch, depth_feat_batch = imgs_feat[idx], depths_feat[idx]\n",
    "        dataset.append((img_batch, img_feat_batch, depth_batch, depth_feat_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'efficientnet': 1280,\n",
    "        'vgg': 512,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAEInjected(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Training Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "       \n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(f\"Epoch {i} Total loss: { losses_across_epochs['loss'][-1]}\")\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_injected/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        np.save('./results/vae_injected/%s_%s' % (model_name, k), np.array(v))\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_injected/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_injected/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "\n",
    "    train_dataset, test_dataset = None, None ## Freeing \n",
    "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "    with tf.device('/cpu:0'):\n",
    "        train_images = tf.image.resize(train_images, (256, 256)).numpy()\n",
    "        test_images = tf.image.resize(test_images, (256, 256)).numpy()\n",
    "    train_feats = np.load('./data/CIFAR100_%s_train_feat.npy' % model_name)\n",
    "    test_feats = np.load('./data/CIFAR100_%s_test_feat.npy' % model_name)\n",
    "\n",
    "    train_result, _, _ = vae.encode((train_images[:128], train_feats[:128], None, None, None), rand_depth=True)\n",
    "    for i in tqdm(range(128, len(train_images), 128)):\n",
    "        img = train_images[i:i+128]\n",
    "        img_feat = train_feats[i:i+128]\n",
    "        activation, _, _ = vae.encode((img, img_feat, None, None, None), rand_depth=True)\n",
    "        train_result = tf.concat((train_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_injected_%s_encoding_train.npy' % model_name, train_result.numpy())\n",
    "\n",
    "    test_result, _, _ = vae.encode((test_images[:128], test_feats[:128], None, None, None), rand_depth=True)\n",
    "    for i in tqdm(range(128, len(test_images), 128)):\n",
    "        img = test_images[i:i+128]\n",
    "        img_feat = test_feats[i:i+128]\n",
    "        activation, _, _ = vae.encode((img, img_feat, None, None, None), rand_depth=True)\n",
    "        test_result = tf.concat((test_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_injected_%s_encoding_test.npy' % model_name, test_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 3641427.888888889\n",
      "Epoch 1 Total loss: 3220873.8333333335\n",
      "Epoch 2 Total loss: 3116310.5\n",
      "Epoch 3 Total loss: 3066843.25\n",
      "Epoch 4 Total loss: 3027007.833333334\n",
      "Epoch 5 Total loss: 2956550.8333333335\n",
      "Epoch 6 Total loss: 2853031.4166666665\n",
      "Epoch 7 Total loss: 2738564.805555556\n",
      "Epoch 8 Total loss: 2624723.6944444445\n",
      "Epoch 9 Total loss: 2517245.4166666665\n",
      "Epoch 10 Total loss: 2418219.361111111\n",
      "Epoch 11 Total loss: 2321482.027777778\n",
      "Epoch 12 Total loss: 2234572.527777778\n",
      "Epoch 13 Total loss: 2157736.388888889\n",
      "Epoch 14 Total loss: 2082242.2222222222\n",
      "Epoch 15 Total loss: 2009541.3333333337\n",
      "Epoch 16 Total loss: 1941497.4166666667\n",
      "Epoch 17 Total loss: 1878469.5555555557\n",
      "Epoch 18 Total loss: 1818076.1805555555\n",
      "Epoch 19 Total loss: 1760910.7222222222\n",
      "Epoch 20 Total loss: 1705791.3333333333\n",
      "Epoch 21 Total loss: 1653429.6805555555\n",
      "Epoch 22 Total loss: 1604523.6944444445\n",
      "Epoch 23 Total loss: 1559394.4583333333\n",
      "Epoch 24 Total loss: 1516281.8194444443\n",
      "Epoch 25 Total loss: 1474833.7916666665\n",
      "Epoch 26 Total loss: 1435370.4722222222\n",
      "Epoch 27 Total loss: 1398868.4583333333\n",
      "Epoch 28 Total loss: 1364103.3194444445\n",
      "Epoch 29 Total loss: 1330933.3055555557\n",
      "Epoch 30 Total loss: 1299668.583333333\n",
      "Epoch 31 Total loss: 1270538.9305555555\n",
      "Epoch 32 Total loss: 1242669.0555555555\n",
      "Epoch 33 Total loss: 1216062.8194444445\n",
      "Epoch 34 Total loss: 1190842.972222222\n",
      "Epoch 35 Total loss: 1166762.6944444443\n",
      "Epoch 36 Total loss: 1143569.125\n",
      "Epoch 37 Total loss: 1121280.0138888888\n",
      "Epoch 38 Total loss: 1099694.847222222\n",
      "Epoch 39 Total loss: 1078963.8888888888\n",
      "Epoch 40 Total loss: 1059176.333333333\n",
      "Epoch 41 Total loss: 1040428.9375\n",
      "Epoch 42 Total loss: 1022651.6458333334\n",
      "Epoch 43 Total loss: 1005812.3194444445\n",
      "Epoch 44 Total loss: 989372.6944444443\n",
      "Epoch 45 Total loss: 973497.8055555555\n",
      "Epoch 46 Total loss: 958171.1736111111\n",
      "Epoch 47 Total loss: 943464.701388889\n",
      "Epoch 48 Total loss: 929340.9027777778\n",
      "Epoch 49 Total loss: 915791.5138888889\n",
      "Epoch 50 Total loss: 902749.1180555555\n",
      "Epoch 51 Total loss: 890070.7569444445\n",
      "Epoch 52 Total loss: 877734.1666666666\n",
      "Epoch 53 Total loss: 865775.9305555556\n",
      "Epoch 54 Total loss: 854315.0555555556\n",
      "Epoch 55 Total loss: 843266.576388889\n",
      "Epoch 56 Total loss: 832525.5763888889\n",
      "Epoch 57 Total loss: 822080.673611111\n",
      "Epoch 58 Total loss: 812031.6458333333\n",
      "Epoch 59 Total loss: 802442.0486111111\n",
      "Epoch 60 Total loss: 793087.4027777778\n",
      "Epoch 61 Total loss: 783902.9930555557\n",
      "Epoch 62 Total loss: 775002.9444444445\n",
      "Epoch 63 Total loss: 766403.3611111111\n",
      "Epoch 64 Total loss: 757922.625\n",
      "Epoch 65 Total loss: 749693.0763888889\n",
      "Epoch 66 Total loss: 741672.3263888888\n",
      "Epoch 67 Total loss: 733880.5625\n",
      "Epoch 68 Total loss: 726236.7777777778\n",
      "Epoch 69 Total loss: 718769.3333333333\n",
      "Epoch 70 Total loss: 711559.6944444444\n",
      "Epoch 71 Total loss: 704511.0694444444\n",
      "Epoch 72 Total loss: 697620.6527777778\n",
      "Epoch 73 Total loss: 690940.9861111111\n",
      "Epoch 74 Total loss: 684486.7152777779\n",
      "Epoch 75 Total loss: 678158.2222222222\n",
      "Epoch 76 Total loss: 671972.7222222222\n",
      "Epoch 77 Total loss: 665919.5416666666\n",
      "Epoch 78 Total loss: 659890.5\n",
      "Epoch 79 Total loss: 653966.2569444444\n",
      "Epoch 80 Total loss: 648131.1805555555\n",
      "Epoch 81 Total loss: 642418.6805555556\n",
      "Epoch 82 Total loss: 636881.8263888889\n",
      "Epoch 83 Total loss: 631452.4375\n",
      "Epoch 84 Total loss: 626095.548611111\n",
      "Epoch 85 Total loss: 620839.8125\n",
      "Epoch 86 Total loss: 615721.7986111111\n",
      "Epoch 87 Total loss: 610766.9652777779\n",
      "Epoch 88 Total loss: 605952.6736111111\n",
      "Epoch 89 Total loss: 601196.388888889\n",
      "Epoch 90 Total loss: 596531.1597222222\n",
      "Epoch 91 Total loss: 591978.1041666667\n",
      "Epoch 92 Total loss: 587536.4097222222\n",
      "Epoch 93 Total loss: 583157.1458333334\n",
      "Epoch 94 Total loss: 578849.6736111112\n",
      "Epoch 95 Total loss: 574624.0347222221\n",
      "Epoch 96 Total loss: 570465.9444444445\n",
      "Epoch 97 Total loss: 566404.5486111111\n",
      "Epoch 98 Total loss: 562434.5208333333\n",
      "Epoch 99 Total loss: 558527.4166666667\n",
      "Gen encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:09<00:00, 41.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 39.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 3062518.6111111115\n",
      "Epoch 1 Total loss: 2997835.9722222225\n",
      "Epoch 2 Total loss: 2972191.7777777775\n",
      "Epoch 3 Total loss: 2932727.5833333335\n",
      "Epoch 4 Total loss: 2839278.25\n",
      "Epoch 5 Total loss: 2707663.472222222\n",
      "Epoch 6 Total loss: 2557740.027777778\n",
      "Epoch 7 Total loss: 2391439.027777778\n",
      "Epoch 8 Total loss: 2234871.8055555555\n",
      "Epoch 9 Total loss: 2097507.625\n",
      "Epoch 10 Total loss: 1977796.7638888892\n",
      "Epoch 11 Total loss: 1868962.111111111\n",
      "Epoch 12 Total loss: 1770880.0\n",
      "Epoch 13 Total loss: 1682536.375\n",
      "Epoch 14 Total loss: 1623829.1944444445\n",
      "Epoch 15 Total loss: 1574405.6388888888\n",
      "Epoch 16 Total loss: 1518828.8749999998\n",
      "Epoch 17 Total loss: 1468627.875\n",
      "Epoch 18 Total loss: 1421624.9444444445\n",
      "Epoch 19 Total loss: 1376074.3888888888\n",
      "Epoch 20 Total loss: 1332965.3888888888\n",
      "Epoch 21 Total loss: 1292259.3472222222\n",
      "Epoch 22 Total loss: 1254306.1527777778\n",
      "Epoch 23 Total loss: 1218616.0555555557\n",
      "Epoch 24 Total loss: 1184666.402777778\n",
      "Epoch 25 Total loss: 1152471.388888889\n",
      "Epoch 26 Total loss: 1122241.1527777778\n",
      "Epoch 27 Total loss: 1094234.9861111112\n",
      "Epoch 28 Total loss: 1068671.0\n",
      "Epoch 29 Total loss: 1045440.9236111112\n",
      "Epoch 30 Total loss: 1022992.6527777779\n",
      "Epoch 31 Total loss: 1001438.3958333333\n",
      "Epoch 32 Total loss: 980777.6388888888\n",
      "Epoch 33 Total loss: 960931.3472222222\n",
      "Epoch 34 Total loss: 941936.0277777779\n",
      "Epoch 35 Total loss: 924082.4722222222\n",
      "Epoch 36 Total loss: 907245.4236111112\n",
      "Epoch 37 Total loss: 890983.0347222221\n",
      "Epoch 38 Total loss: 875282.0347222221\n",
      "Epoch 39 Total loss: 860173.9930555555\n",
      "Epoch 40 Total loss: 845872.625\n",
      "Epoch 41 Total loss: 832304.8888888889\n",
      "Epoch 42 Total loss: 819239.9375\n",
      "Epoch 43 Total loss: 806535.9444444445\n",
      "Epoch 44 Total loss: 794237.9513888889\n",
      "Epoch 45 Total loss: 782399.8125\n",
      "Epoch 46 Total loss: 771062.4305555555\n",
      "Epoch 47 Total loss: 760167.0972222222\n",
      "Epoch 48 Total loss: 749547.6319444445\n",
      "Epoch 49 Total loss: 739384.3194444445\n",
      "Epoch 50 Total loss: 729742.4097222221\n",
      "Epoch 51 Total loss: 720320.4027777778\n",
      "Epoch 52 Total loss: 711146.673611111\n",
      "Epoch 53 Total loss: 702399.8888888889\n",
      "Epoch 54 Total loss: 694069.0625000001\n",
      "Epoch 55 Total loss: 686030.7708333333\n",
      "Epoch 56 Total loss: 678166.5347222222\n",
      "Epoch 57 Total loss: 670427.4027777778\n",
      "Epoch 58 Total loss: 662833.9513888889\n",
      "Epoch 59 Total loss: 655400.1597222221\n",
      "Epoch 60 Total loss: 648187.0555555555\n",
      "Epoch 61 Total loss: 641261.6180555555\n",
      "Epoch 62 Total loss: 634585.298611111\n",
      "Epoch 63 Total loss: 628119.8402777778\n",
      "Epoch 64 Total loss: 621786.1527777779\n",
      "Epoch 65 Total loss: 615521.888888889\n",
      "Epoch 66 Total loss: 609358.6388888889\n",
      "Epoch 67 Total loss: 603355.7986111111\n",
      "Epoch 68 Total loss: 597525.8611111112\n",
      "Epoch 69 Total loss: 591841.5\n",
      "Epoch 70 Total loss: 586318.8055555556\n",
      "Epoch 71 Total loss: 580964.6805555556\n",
      "Epoch 72 Total loss: 575754.4305555555\n",
      "Epoch 73 Total loss: 570691.375\n",
      "Epoch 74 Total loss: 565775.6597222222\n",
      "Epoch 75 Total loss: 560982.5069444444\n",
      "Epoch 76 Total loss: 556322.4305555555\n",
      "Epoch 77 Total loss: 551786.8472222222\n",
      "Epoch 78 Total loss: 547370.9861111111\n",
      "Epoch 79 Total loss: 543114.0763888889\n",
      "Epoch 80 Total loss: 538892.9444444444\n",
      "Epoch 81 Total loss: 534698.0\n",
      "Epoch 82 Total loss: 530622.2916666666\n",
      "Epoch 83 Total loss: 526650.3194444445\n",
      "Epoch 84 Total loss: 522749.5659722222\n",
      "Epoch 85 Total loss: 518896.4479166666\n",
      "Epoch 86 Total loss: 515104.6215277778\n",
      "Epoch 87 Total loss: 511387.4236111111\n",
      "Epoch 88 Total loss: 507757.5104166666\n",
      "Epoch 89 Total loss: 504224.36805555556\n",
      "Epoch 90 Total loss: 500775.8020833333\n",
      "Epoch 91 Total loss: 497384.5277777778\n",
      "Epoch 92 Total loss: 494041.8194444444\n",
      "Epoch 93 Total loss: 490727.46875\n",
      "Epoch 94 Total loss: 487458.18055555556\n",
      "Epoch 95 Total loss: 484243.0520833334\n",
      "Epoch 96 Total loss: 481083.0312500001\n",
      "Epoch 97 Total loss: 477990.29166666674\n",
      "Epoch 98 Total loss: 474981.1493055555\n",
      "Epoch 99 Total loss: 472002.5763888888\n",
      "Gen encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:07<00:00, 49.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 61.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2959549.138888889\n",
      "Epoch 1 Total loss: 2752441.333333333\n",
      "Epoch 2 Total loss: 2676154.1944444445\n",
      "Epoch 3 Total loss: 2573345.5833333335\n",
      "Epoch 4 Total loss: 2451470.611111111\n",
      "Epoch 5 Total loss: 2308682.027777778\n",
      "Epoch 6 Total loss: 2157994.75\n",
      "Epoch 7 Total loss: 2009383.5138888888\n",
      "Epoch 8 Total loss: 1879501.763888889\n",
      "Epoch 9 Total loss: 1765718.1666666665\n",
      "Epoch 10 Total loss: 1663042.6805555555\n",
      "Epoch 11 Total loss: 1573822.9583333335\n",
      "Epoch 12 Total loss: 1532351.986111111\n",
      "Epoch 13 Total loss: 1485052.5\n",
      "Epoch 14 Total loss: 1424120.6944444445\n",
      "Epoch 15 Total loss: 1370117.6944444445\n",
      "Epoch 16 Total loss: 1320951.5\n",
      "Epoch 17 Total loss: 1274491.986111111\n",
      "Epoch 18 Total loss: 1233288.75\n",
      "Epoch 19 Total loss: 1194405.166666667\n",
      "Epoch 20 Total loss: 1157216.6666666667\n",
      "Epoch 21 Total loss: 1121949.0694444445\n",
      "Epoch 22 Total loss: 1088548.3472222222\n",
      "Epoch 23 Total loss: 1057633.4097222222\n",
      "Epoch 24 Total loss: 1029179.4375\n",
      "Epoch 25 Total loss: 1002931.0625\n",
      "Epoch 26 Total loss: 978769.0833333335\n",
      "Epoch 27 Total loss: 956055.4444444444\n",
      "Epoch 28 Total loss: 934786.2361111111\n",
      "Epoch 29 Total loss: 914396.6805555557\n",
      "Epoch 30 Total loss: 894990.3958333333\n",
      "Epoch 31 Total loss: 876240.0208333335\n",
      "Epoch 32 Total loss: 858318.8125\n",
      "Epoch 33 Total loss: 841305.0833333334\n",
      "Epoch 34 Total loss: 825162.4444444444\n",
      "Epoch 35 Total loss: 809748.5138888888\n",
      "Epoch 36 Total loss: 795104.0\n",
      "Epoch 37 Total loss: 781209.4236111112\n",
      "Epoch 38 Total loss: 767807.9027777778\n",
      "Epoch 39 Total loss: 754933.5\n",
      "Epoch 40 Total loss: 742670.7222222222\n",
      "Epoch 41 Total loss: 730903.6666666667\n",
      "Epoch 42 Total loss: 719598.9722222222\n",
      "Epoch 43 Total loss: 708858.486111111\n",
      "Epoch 44 Total loss: 698535.701388889\n",
      "Epoch 45 Total loss: 688612.5833333334\n",
      "Epoch 46 Total loss: 679058.2083333333\n",
      "Epoch 47 Total loss: 669870.3958333333\n",
      "Epoch 48 Total loss: 660910.375\n",
      "Epoch 49 Total loss: 652299.8888888889\n",
      "Epoch 50 Total loss: 643997.9027777778\n",
      "Epoch 51 Total loss: 635897.6875\n",
      "Epoch 52 Total loss: 628062.2500000001\n",
      "Epoch 53 Total loss: 620511.4375\n",
      "Epoch 54 Total loss: 613292.5416666666\n",
      "Epoch 55 Total loss: 606419.4375\n",
      "Epoch 56 Total loss: 599705.423611111\n",
      "Epoch 57 Total loss: 593121.0277777778\n",
      "Epoch 58 Total loss: 586691.3472222222\n",
      "Epoch 59 Total loss: 580461.2152777779\n",
      "Epoch 60 Total loss: 574309.3472222222\n",
      "Epoch 61 Total loss: 568299.7222222222\n",
      "Epoch 62 Total loss: 562446.5694444444\n",
      "Epoch 63 Total loss: 556806.2638888889\n",
      "Epoch 64 Total loss: 551359.7777777778\n",
      "Epoch 65 Total loss: 546042.1597222222\n",
      "Epoch 66 Total loss: 540870.6805555556\n",
      "Epoch 67 Total loss: 535936.7361111111\n",
      "Epoch 68 Total loss: 531183.8541666666\n",
      "Epoch 69 Total loss: 526501.9652777778\n",
      "Epoch 70 Total loss: 521905.06250000006\n",
      "Epoch 71 Total loss: 517361.56944444444\n",
      "Epoch 72 Total loss: 512896.9375\n",
      "Epoch 73 Total loss: 508534.22569444444\n",
      "Epoch 74 Total loss: 504252.2326388889\n",
      "Epoch 75 Total loss: 500072.44444444444\n",
      "Epoch 76 Total loss: 496014.43749999994\n",
      "Epoch 77 Total loss: 492038.4583333333\n",
      "Epoch 78 Total loss: 488124.81944444444\n",
      "Epoch 79 Total loss: 484311.1527777778\n",
      "Epoch 80 Total loss: 480611.6215277777\n",
      "Epoch 81 Total loss: 477030.0520833334\n",
      "Epoch 82 Total loss: 473511.65625\n",
      "Epoch 83 Total loss: 470056.77777777775\n",
      "Epoch 84 Total loss: 466687.9027777778\n",
      "Epoch 85 Total loss: 463373.3020833334\n",
      "Epoch 86 Total loss: 460102.31249999994\n",
      "Epoch 87 Total loss: 456876.5833333333\n",
      "Epoch 88 Total loss: 453746.6736111111\n",
      "Epoch 89 Total loss: 450669.2916666666\n",
      "Epoch 90 Total loss: 447636.8541666667\n",
      "Epoch 91 Total loss: 455271.50347222213\n",
      "Epoch 92 Total loss: 78643776.0\n",
      "Epoch 93 Total loss: 295949304.8888889\n",
      "Epoch 94 Total loss: 295615512.8888889\n",
      "Epoch 95 Total loss: 292672551.1111111\n",
      "Epoch 96 Total loss: 289681468.4444444\n",
      "Epoch 97 Total loss: 286748643.5555555\n",
      "Epoch 98 Total loss: 283873468.4444445\n",
      "Epoch 99 Total loss: 281053962.6666667\n",
      "Gen encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:07<00:00, 52.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 57.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2843050.833333334\n",
      "Epoch 1 Total loss: 2700839.055555555\n",
      "Epoch 2 Total loss: 2664254.944444444\n",
      "Epoch 3 Total loss: 2595385.583333333\n",
      "Epoch 4 Total loss: 2488698.0\n",
      "Epoch 5 Total loss: 2368003.0277777775\n",
      "Epoch 6 Total loss: 2253285.0555555555\n",
      "Epoch 7 Total loss: 2140539.847222222\n",
      "Epoch 8 Total loss: 2031279.222222222\n",
      "Epoch 9 Total loss: 1930344.972222222\n",
      "Epoch 10 Total loss: 1834868.3333333333\n",
      "Epoch 11 Total loss: 1747329.4166666665\n",
      "Epoch 12 Total loss: 1667842.1388888888\n",
      "Epoch 13 Total loss: 1596722.611111111\n",
      "Epoch 14 Total loss: 1532683.375\n",
      "Epoch 15 Total loss: 1474448.8472222222\n",
      "Epoch 16 Total loss: 1420748.875\n",
      "Epoch 17 Total loss: 1371207.6944444445\n",
      "Epoch 18 Total loss: 1324631.513888889\n",
      "Epoch 19 Total loss: 1280574.4305555557\n",
      "Epoch 20 Total loss: 1239375.4583333333\n",
      "Epoch 21 Total loss: 1201352.1805555555\n",
      "Epoch 22 Total loss: 1165977.0\n",
      "Epoch 23 Total loss: 1133723.5833333333\n",
      "Epoch 24 Total loss: 1103684.75\n",
      "Epoch 25 Total loss: 1074862.6944444445\n",
      "Epoch 26 Total loss: 1047515.4513888889\n",
      "Epoch 27 Total loss: 1021628.3402777778\n",
      "Epoch 28 Total loss: 997040.986111111\n",
      "Epoch 29 Total loss: 973606.3611111111\n",
      "Epoch 30 Total loss: 951517.5486111112\n",
      "Epoch 31 Total loss: 931124.9097222224\n",
      "Epoch 32 Total loss: 912122.7152777779\n",
      "Epoch 33 Total loss: 893756.6388888888\n",
      "Epoch 34 Total loss: 876069.6874999999\n",
      "Epoch 35 Total loss: 859027.8055555557\n",
      "Epoch 36 Total loss: 842687.3541666666\n",
      "Epoch 37 Total loss: 827064.3402777779\n",
      "Epoch 38 Total loss: 812149.9097222221\n",
      "Epoch 39 Total loss: 797930.1527777778\n",
      "Epoch 40 Total loss: 784376.1944444444\n",
      "Epoch 41 Total loss: 771296.0138888889\n",
      "Epoch 42 Total loss: 758723.0486111111\n",
      "Epoch 43 Total loss: 746684.7013888888\n",
      "Epoch 44 Total loss: 735195.8472222221\n",
      "Epoch 45 Total loss: 724221.3541666667\n",
      "Epoch 46 Total loss: 713909.6458333333\n",
      "Epoch 47 Total loss: 703909.4166666667\n",
      "Epoch 48 Total loss: 694092.1111111111\n",
      "Epoch 49 Total loss: 684561.4513888889\n",
      "Epoch 50 Total loss: 675445.3888888889\n",
      "Epoch 51 Total loss: 666583.6319444445\n",
      "Epoch 52 Total loss: 657881.0\n",
      "Epoch 53 Total loss: 649411.7916666667\n",
      "Epoch 54 Total loss: 641189.8402777778\n",
      "Epoch 55 Total loss: 633335.2430555556\n",
      "Epoch 56 Total loss: 625784.1597222222\n",
      "Epoch 57 Total loss: 618558.125\n",
      "Epoch 58 Total loss: 611571.9027777778\n",
      "Epoch 59 Total loss: 604714.5208333334\n",
      "Epoch 60 Total loss: 598009.375\n",
      "Epoch 61 Total loss: 591482.673611111\n",
      "Epoch 62 Total loss: 585182.4305555555\n",
      "Epoch 63 Total loss: 579140.3472222221\n",
      "Epoch 64 Total loss: 573272.2847222222\n",
      "Epoch 65 Total loss: 567475.125\n",
      "Epoch 66 Total loss: 561807.0277777778\n",
      "Epoch 67 Total loss: 556286.0069444444\n",
      "Epoch 68 Total loss: 550985.6666666667\n",
      "Epoch 69 Total loss: 545873.7083333333\n",
      "Epoch 70 Total loss: 540917.1944444444\n",
      "Epoch 71 Total loss: 536004.6666666667\n",
      "Epoch 72 Total loss: 531145.2777777778\n",
      "Epoch 73 Total loss: 526362.6631944445\n",
      "Epoch 74 Total loss: 521676.08333333326\n",
      "Epoch 75 Total loss: 517095.01736111107\n",
      "Epoch 76 Total loss: 512630.61805555556\n",
      "Epoch 77 Total loss: 508305.87152777775\n",
      "Epoch 78 Total loss: 504121.64236111107\n",
      "Epoch 79 Total loss: 500014.34375\n",
      "Epoch 80 Total loss: 495946.3368055557\n",
      "Epoch 81 Total loss: 491994.96527777775\n",
      "Epoch 82 Total loss: 488100.63194444444\n",
      "Epoch 83 Total loss: 484297.0902777778\n",
      "Epoch 84 Total loss: 480591.33680555556\n",
      "Epoch 85 Total loss: 477021.00347222225\n",
      "Epoch 86 Total loss: 473531.28819444444\n",
      "Epoch 87 Total loss: 470103.5625\n",
      "Epoch 88 Total loss: 466728.98611111107\n",
      "Epoch 89 Total loss: 463381.2361111111\n",
      "Epoch 90 Total loss: 460089.3611111111\n",
      "Epoch 91 Total loss: 456872.31944444444\n",
      "Epoch 92 Total loss: 453709.83680555556\n",
      "Epoch 93 Total loss: 450570.74305555556\n",
      "Epoch 94 Total loss: 447472.0034722222\n",
      "Epoch 95 Total loss: 444446.2916666667\n",
      "Epoch 96 Total loss: 441442.17013888893\n",
      "Epoch 97 Total loss: 438494.9131944444\n",
      "Epoch 98 Total loss: 435595.6666666667\n",
      "Epoch 99 Total loss: 432743.08333333326\n",
      "Gen encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:07<00:00, 55.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 72.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2689873.305555556\n",
      "Epoch 1 Total loss: 2648610.722222222\n",
      "Epoch 2 Total loss: 2642933.2777777775\n",
      "Epoch 3 Total loss: 2616244.7222222225\n",
      "Epoch 4 Total loss: 2557985.7222222225\n",
      "Epoch 5 Total loss: 2474645.222222222\n",
      "Epoch 6 Total loss: 2379699.2222222225\n",
      "Epoch 7 Total loss: 2283353.722222222\n",
      "Epoch 8 Total loss: 2187786.6944444445\n",
      "Epoch 9 Total loss: 2094159.361111111\n",
      "Epoch 10 Total loss: 2003392.7222222225\n",
      "Epoch 11 Total loss: 1916182.2222222222\n",
      "Epoch 12 Total loss: 1833134.2638888888\n",
      "Epoch 13 Total loss: 1754675.4861111112\n",
      "Epoch 14 Total loss: 1682511.5972222222\n",
      "Epoch 15 Total loss: 1616266.1111111115\n",
      "Epoch 16 Total loss: 1555544.1527777778\n",
      "Epoch 17 Total loss: 1499824.6388888888\n",
      "Epoch 18 Total loss: 1448896.2083333335\n",
      "Epoch 19 Total loss: 1402102.9583333333\n",
      "Epoch 20 Total loss: 1359177.2222222225\n",
      "Epoch 21 Total loss: 1318833.2916666667\n",
      "Epoch 22 Total loss: 1280390.4444444447\n",
      "Epoch 23 Total loss: 1244072.2777777778\n",
      "Epoch 24 Total loss: 1209678.416666667\n",
      "Epoch 25 Total loss: 1177227.8472222222\n",
      "Epoch 26 Total loss: 1146671.3333333333\n",
      "Epoch 27 Total loss: 1118172.486111111\n",
      "Epoch 28 Total loss: 1091427.625\n",
      "Epoch 29 Total loss: 1066069.611111111\n",
      "Epoch 30 Total loss: 1042083.1180555556\n",
      "Epoch 31 Total loss: 1019329.5208333334\n",
      "Epoch 32 Total loss: 997517.0416666667\n",
      "Epoch 33 Total loss: 976640.2291666667\n",
      "Epoch 34 Total loss: 956824.4097222222\n",
      "Epoch 35 Total loss: 938113.4166666667\n",
      "Epoch 36 Total loss: 920365.6041666665\n",
      "Epoch 37 Total loss: 903243.7222222222\n",
      "Epoch 38 Total loss: 886818.25\n",
      "Epoch 39 Total loss: 871000.4583333334\n",
      "Epoch 40 Total loss: 855859.5694444444\n",
      "Epoch 41 Total loss: 841330.6597222221\n",
      "Epoch 42 Total loss: 827347.6805555555\n",
      "Epoch 43 Total loss: 813918.7986111111\n",
      "Epoch 44 Total loss: 800971.9583333333\n",
      "Epoch 45 Total loss: 788538.7708333333\n",
      "Epoch 46 Total loss: 776623.9583333334\n",
      "Epoch 47 Total loss: 765142.2847222222\n",
      "Epoch 48 Total loss: 754078.5347222222\n",
      "Epoch 49 Total loss: 743385.7708333334\n",
      "Epoch 50 Total loss: 733032.5069444445\n",
      "Epoch 51 Total loss: 723076.7222222222\n",
      "Epoch 52 Total loss: 713467.7361111111\n",
      "Epoch 53 Total loss: 704163.6388888888\n",
      "Epoch 54 Total loss: 695162.8819444444\n",
      "Epoch 55 Total loss: 686396.8750000001\n",
      "Epoch 56 Total loss: 677912.9375\n",
      "Epoch 57 Total loss: 669675.298611111\n",
      "Epoch 58 Total loss: 661679.9166666667\n",
      "Epoch 59 Total loss: 653890.7569444445\n",
      "Epoch 60 Total loss: 646317.5902777778\n",
      "Epoch 61 Total loss: 638954.2083333333\n",
      "Epoch 62 Total loss: 631798.4722222222\n",
      "Epoch 63 Total loss: 624855.6805555556\n",
      "Epoch 64 Total loss: 618157.4305555556\n",
      "Epoch 65 Total loss: 611742.2638888889\n",
      "Epoch 66 Total loss: 605431.6875\n",
      "Epoch 67 Total loss: 599219.8402777778\n",
      "Epoch 68 Total loss: 593190.1458333333\n",
      "Epoch 69 Total loss: 587344.4861111111\n",
      "Epoch 70 Total loss: 581676.2083333334\n",
      "Epoch 71 Total loss: 576141.875\n",
      "Epoch 72 Total loss: 570756.7708333334\n",
      "Epoch 73 Total loss: 565493.3888888889\n",
      "Epoch 74 Total loss: 560311.6388888889\n",
      "Epoch 75 Total loss: 555242.4097222222\n",
      "Epoch 76 Total loss: 550254.4097222222\n",
      "Epoch 77 Total loss: 545366.2777777778\n",
      "Epoch 78 Total loss: 540600.5277777778\n",
      "Epoch 79 Total loss: 535961.8472222222\n",
      "Epoch 80 Total loss: 531451.7083333334\n",
      "Epoch 81 Total loss: 527074.75\n",
      "Epoch 82 Total loss: 522820.9756944444\n",
      "Epoch 83 Total loss: 518670.7673611112\n",
      "Epoch 84 Total loss: 514585.15625\n",
      "Epoch 85 Total loss: 510573.8298611111\n",
      "Epoch 86 Total loss: 506612.10763888893\n",
      "Epoch 87 Total loss: 502712.74652777775\n",
      "Epoch 88 Total loss: 498881.2916666667\n",
      "Epoch 89 Total loss: 495114.6840277778\n",
      "Epoch 90 Total loss: 491425.44097222225\n",
      "Epoch 91 Total loss: 487829.71527777775\n",
      "Epoch 92 Total loss: 484308.0104166667\n",
      "Epoch 93 Total loss: 480860.8611111111\n",
      "Epoch 94 Total loss: 477482.4930555555\n",
      "Epoch 95 Total loss: 474161.4826388889\n",
      "Epoch 96 Total loss: 470894.8784722222\n",
      "Epoch 97 Total loss: 467689.84722222225\n",
      "Epoch 98 Total loss: 464552.3333333334\n",
      "Epoch 99 Total loss: 461463.9861111111\n",
      "Gen encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:06<00:00, 56.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 72.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "for model_name in ['efficientnet','inception', 'resnet', 'mobilenet', 'vgg']:\n",
    "    train_dataset, test_dataset = load_data_for_model(model_name)\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(\"Gen encoding...\")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
