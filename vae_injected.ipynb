{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('comp5900porj')",
   "metadata": {
    "interpreter": {
     "hash": "8608f3369e1b7fe199f3056150d4f7c6c375e24eac93172fa646af6324355164"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_injected import VAEInjected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(model_name, batch_size=64):\n",
    "    assert model_name in ['inception', 'vgg', 'resnet', 'mobilenet']\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated.npy')\n",
    "    imgs = np.load('./data/LFSD_imgs.npy')\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    depths_feat = np.load('./data/LFSD_depths_repeated_%s_feat.npy' % model_name)\n",
    "    imgs_feat = np.load('./data/LFSD_imgs_%s_feat.npy' % model_name)\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        img_feat_batch, depth_feat_batch = imgs_feat[idx], depths_feat[idx]\n",
    "        dataset.append((img_batch, img_feat_batch, depth_batch, depth_feat_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'vgg': 512,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAEInjected(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Training Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch %d: \" % i)\n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(\"Total loss: %d\" % losses_across_epochs['loss'][-1])\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_injected/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        np.save('./results/vae_injected/%s_%s' % (model_name, k), np.array(v))\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_injected/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_injected/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "    from skimage.transform import resize\n",
    "    from tensorflow.keras import datasets\n",
    "\n",
    "    train_dataset, test_dataset = None, None ## Freeing \n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "    train_images = tf.image.resize(train_images, (256, 256))\n",
    "    test_images = tf.image.resize(test_images, (256, 256))\n",
    "    train_feats = np.load('./data/CIFAR100_%s_train_feat.npy' % model_name)\n",
    "    test_feats = np.load('./data/CIFAR100_%s_test_feat.npy' % model_name)\n",
    "\n",
    "    train_result, _, _ = vae.encode((train_images[:128], train_feats[:128], None, None, None), rand_depth=True)\n",
    "    for i in tqdm(range(128, len(train_images), 128)):\n",
    "        img = train_images[i:i+128]\n",
    "        img_feat = train_feats[i:i+128]\n",
    "        activation, _, _ = vae.encode((img, img_feat, None, None, None), rand_depth=True)\n",
    "        train_result = tf.concat((train_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_injected_%s_encoding_train.npy' % model_name, train_result.numpy())\n",
    "\n",
    "    test_result, _, _ = vae.encode((test_images[:128], test_feats[:128], None, None, None), rand_depth=True)\n",
    "    for i in tqdm(range(128, len(test_images), 128)):\n",
    "        img = test_images[i:i+128]\n",
    "        img_feat = test_feats[i:i+128]\n",
    "        activation, _, _ = vae.encode((img, img_feat, None, None, None), rand_depth=True)\n",
    "        test_result = tf.concat((test_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_injected_%s_encoding_test.npy' % model_name, test_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 1\n",
    "for model_name in ['inception']:\n",
    "    train_dataset, test_dataset = load_data_for_model(model_name)\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(\"Gen encoding...\")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}