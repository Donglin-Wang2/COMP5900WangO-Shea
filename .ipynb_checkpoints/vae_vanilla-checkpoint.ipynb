{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras import datasets\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_vanilla import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(batch_size=64):\n",
    "    ## Loading dataset\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated.npy')\n",
    "    imgs = np.load('./data/LFSD_imgs.npy')\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        dataset.append((img_batch, depth_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'efficientnet': 1280,\n",
    "        'vgg': 512,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAE(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Train Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(f\"Epoch {i} Total loss: { losses_across_epochs['loss'][-1]}\")\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_vanilla/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        \n",
    "        np.save('./results/vae_vanilla/%s_%s' % (model_name, k), np.array(v))\n",
    "\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_vanilla/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_vanilla/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "\n",
    "    train_dataset, test_dataset = None, None\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "    with tf.device('/cpu:0'):\n",
    "        train_images = tf.image.resize(train_images, (256, 256)).numpy()\n",
    "        test_images = tf.image.resize(test_images, (256, 256)).numpy()\n",
    "    \n",
    "    train_result, _, _ = vae.encode(train_images[:128], None, rand_depth=True)\n",
    "    for i in tqdm(range(128, len(train_images), 128)):\n",
    "        img = train_images[i:i+128]\n",
    "        activation, _, _ = vae.encode(img, None, rand_depth=True)\n",
    "        train_result = tf.concat((train_result, activation), axis=0)   \n",
    "    np.save('./data/CIFAR100_vae_vanilla_%s_encoding_train.npy' % model_name, train_result.numpy())\n",
    "\n",
    "    test_result, _, _ = vae.encode(test_images[:128], None, rand_depth=True)\n",
    "    for i in tqdm(range(128, len(test_images), 128)):\n",
    "        img = test_images[i:i+128]\n",
    "        activation, _, _ = vae.encode(img, None, rand_depth=True)\n",
    "        test_result = tf.concat((test_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_vanilla_%s_encoding_test.npy' % model_name, test_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2766788.194444445\n",
      "Epoch 1 Total loss: 2653520.0555555555\n",
      "Epoch 2 Total loss: 2561606.2222222225\n",
      "Epoch 3 Total loss: 2441411.2777777775\n",
      "Epoch 4 Total loss: 2322682.722222222\n",
      "Epoch 5 Total loss: 2209099.3333333335\n",
      "Epoch 6 Total loss: 2098112.1527777775\n",
      "Epoch 7 Total loss: 1991821.5694444445\n",
      "Epoch 8 Total loss: 1892266.1250000002\n",
      "Epoch 9 Total loss: 1801601.5138888888\n",
      "Epoch 10 Total loss: 1718186.3472222222\n",
      "Epoch 11 Total loss: 1640825.625\n",
      "Epoch 12 Total loss: 1569191.8194444447\n",
      "Epoch 13 Total loss: 1503127.3750000002\n",
      "Epoch 14 Total loss: 1442274.125\n",
      "Epoch 15 Total loss: 1386370.013888889\n",
      "Epoch 16 Total loss: 1335348.375\n",
      "Epoch 17 Total loss: 1287831.5555555555\n",
      "Epoch 18 Total loss: 1243395.4583333333\n",
      "Epoch 19 Total loss: 1202111.8055555557\n",
      "Epoch 20 Total loss: 1163554.1666666667\n",
      "Epoch 21 Total loss: 1127631.4861111112\n",
      "Epoch 22 Total loss: 1094076.0555555555\n",
      "Epoch 23 Total loss: 1062642.4444444445\n",
      "Epoch 24 Total loss: 1033105.111111111\n",
      "Epoch 25 Total loss: 1005446.597222222\n",
      "Epoch 26 Total loss: 979388.9513888889\n",
      "Epoch 27 Total loss: 954845.8333333334\n",
      "Epoch 28 Total loss: 931714.6805555555\n",
      "Epoch 29 Total loss: 909999.2986111112\n",
      "Epoch 30 Total loss: 889793.3611111111\n",
      "Epoch 31 Total loss: 870947.5069444444\n",
      "Epoch 32 Total loss: 852896.0208333334\n",
      "Epoch 33 Total loss: 835332.0069444445\n",
      "Epoch 34 Total loss: 818512.7847222222\n",
      "Epoch 35 Total loss: 802424.8263888889\n",
      "Epoch 36 Total loss: 787085.8263888889\n",
      "Epoch 37 Total loss: 772544.8194444445\n",
      "Epoch 38 Total loss: 758614.0208333335\n",
      "Epoch 39 Total loss: 745233.6319444444\n",
      "Epoch 40 Total loss: 732478.0555555555\n",
      "Epoch 41 Total loss: 720263.4722222222\n",
      "Epoch 42 Total loss: 708588.3958333333\n",
      "Epoch 43 Total loss: 697400.2152777778\n",
      "Epoch 44 Total loss: 686614.4305555555\n",
      "Epoch 45 Total loss: 676321.6944444444\n",
      "Epoch 46 Total loss: 666380.5347222222\n",
      "Epoch 47 Total loss: 656815.1736111111\n",
      "Epoch 48 Total loss: 647623.9097222222\n",
      "Epoch 49 Total loss: 638762.1319444445\n",
      "Epoch 50 Total loss: 630207.4583333333\n",
      "Epoch 51 Total loss: 621922.3194444445\n",
      "Epoch 52 Total loss: 613855.5138888889\n",
      "Epoch 53 Total loss: 606064.375\n",
      "Epoch 54 Total loss: 598511.625\n",
      "Epoch 55 Total loss: 591211.1388888889\n",
      "Epoch 56 Total loss: 584165.4375\n",
      "Epoch 57 Total loss: 577361.1180555556\n",
      "Epoch 58 Total loss: 570799.625\n",
      "Epoch 59 Total loss: 564478.1319444445\n",
      "Epoch 60 Total loss: 558300.3541666666\n",
      "Epoch 61 Total loss: 552250.9583333333\n",
      "Epoch 62 Total loss: 546343.0763888889\n",
      "Epoch 63 Total loss: 540616.7430555556\n",
      "Epoch 64 Total loss: 535062.2569444445\n",
      "Epoch 65 Total loss: 529682.2013888889\n",
      "Epoch 66 Total loss: 524448.7083333334\n",
      "Epoch 67 Total loss: 519346.10763888893\n",
      "Epoch 68 Total loss: 514354.21875000006\n",
      "Epoch 69 Total loss: 509490.53819444444\n",
      "Epoch 70 Total loss: 504714.30208333326\n",
      "Epoch 71 Total loss: 500070.0208333333\n",
      "Epoch 72 Total loss: 495552.16666666674\n",
      "Epoch 73 Total loss: 491165.5277777777\n",
      "Epoch 74 Total loss: 486903.5208333334\n",
      "Epoch 75 Total loss: 482715.0\n",
      "Epoch 76 Total loss: 478612.78819444444\n",
      "Epoch 77 Total loss: 474578.5173611111\n",
      "Epoch 78 Total loss: 470639.44097222225\n",
      "Epoch 79 Total loss: 466797.9791666667\n",
      "Epoch 80 Total loss: 463069.3159722222\n",
      "Epoch 81 Total loss: 459399.1666666667\n",
      "Epoch 82 Total loss: 455798.0381944444\n",
      "Epoch 83 Total loss: 452265.0798611111\n",
      "Epoch 84 Total loss: 448792.86111111107\n",
      "Epoch 85 Total loss: 445385.78819444444\n",
      "Epoch 86 Total loss: 442025.9756944444\n",
      "Epoch 87 Total loss: 438751.75694444444\n",
      "Epoch 88 Total loss: 435546.2395833334\n",
      "Epoch 89 Total loss: 432413.0173611111\n",
      "Epoch 90 Total loss: 429337.5034722222\n",
      "Epoch 91 Total loss: 426336.41319444444\n",
      "Epoch 92 Total loss: 423381.50347222225\n",
      "Epoch 93 Total loss: 420475.46180555556\n",
      "Epoch 94 Total loss: 417622.8541666666\n",
      "Epoch 95 Total loss: 414832.375\n",
      "Epoch 96 Total loss: 412093.34375\n",
      "Epoch 97 Total loss: 409400.7291666666\n",
      "Epoch 98 Total loss: 406755.4930555556\n",
      "Epoch 99 Total loss: 404148.2916666666\n",
      " efficientnet Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:07<00:00, 53.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 48.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2907912.305555556\n",
      "Epoch 1 Total loss: 2898314.083333333\n",
      "Epoch 2 Total loss: 2847795.555555556\n",
      "Epoch 3 Total loss: 2749539.333333333\n",
      "Epoch 4 Total loss: 2628281.7499999995\n",
      "Epoch 5 Total loss: 2493439.777777778\n",
      "Epoch 6 Total loss: 2351622.222222222\n",
      "Epoch 7 Total loss: 2216191.6666666665\n",
      "Epoch 8 Total loss: 2093844.902777778\n",
      "Epoch 9 Total loss: 1983088.6111111112\n",
      "Epoch 10 Total loss: 1882699.0416666667\n",
      "Epoch 11 Total loss: 1791457.9444444445\n",
      "Epoch 12 Total loss: 1708288.486111111\n",
      "Epoch 13 Total loss: 1632231.7916666665\n",
      "Epoch 14 Total loss: 1562538.736111111\n",
      "Epoch 15 Total loss: 1498565.9722222222\n",
      "Epoch 16 Total loss: 1439894.097222222\n",
      "Epoch 17 Total loss: 1386263.0833333335\n",
      "Epoch 18 Total loss: 1337417.2916666667\n",
      "Epoch 19 Total loss: 1292605.6527777778\n",
      "Epoch 20 Total loss: 1250892.4861111112\n",
      "Epoch 21 Total loss: 1211813.1944444445\n",
      "Epoch 22 Total loss: 1175155.4861111112\n",
      "Epoch 23 Total loss: 1140755.5555555555\n",
      "Epoch 24 Total loss: 1108619.2083333333\n",
      "Epoch 25 Total loss: 1078645.013888889\n",
      "Epoch 26 Total loss: 1050545.2847222222\n",
      "Epoch 27 Total loss: 1024104.1319444445\n",
      "Epoch 28 Total loss: 999299.4722222222\n",
      "Epoch 29 Total loss: 975927.7708333335\n",
      "Epoch 30 Total loss: 953848.2777777778\n",
      "Epoch 31 Total loss: 932916.576388889\n",
      "Epoch 32 Total loss: 912980.2152777778\n",
      "Epoch 33 Total loss: 894041.6319444445\n",
      "Epoch 34 Total loss: 876009.7708333334\n",
      "Epoch 35 Total loss: 858903.2291666666\n",
      "Epoch 36 Total loss: 842614.076388889\n",
      "Epoch 37 Total loss: 827080.1458333333\n",
      "Epoch 38 Total loss: 812387.7708333334\n",
      "Epoch 39 Total loss: 798283.5138888888\n",
      "Epoch 40 Total loss: 784873.7638888889\n",
      "Epoch 41 Total loss: 772040.513888889\n",
      "Epoch 42 Total loss: 759693.6041666667\n",
      "Epoch 43 Total loss: 747915.1319444444\n",
      "Epoch 44 Total loss: 736526.9305555556\n",
      "Epoch 45 Total loss: 725486.8541666667\n",
      "Epoch 46 Total loss: 714848.201388889\n",
      "Epoch 47 Total loss: 704569.5624999999\n",
      "Epoch 48 Total loss: 694656.1805555555\n",
      "Epoch 49 Total loss: 685095.5972222222\n",
      "Epoch 50 Total loss: 675837.7430555557\n",
      "Epoch 51 Total loss: 666914.2430555556\n",
      "Epoch 52 Total loss: 658284.0138888889\n",
      "Epoch 53 Total loss: 649960.7222222222\n",
      "Epoch 54 Total loss: 641961.75\n",
      "Epoch 55 Total loss: 634173.1666666666\n",
      "Epoch 56 Total loss: 626633.8472222222\n",
      "Epoch 57 Total loss: 619309.8263888889\n",
      "Epoch 58 Total loss: 612235.9305555555\n",
      "Epoch 59 Total loss: 605402.3402777778\n",
      "Epoch 60 Total loss: 598789.1319444444\n",
      "Epoch 61 Total loss: 592339.5347222221\n",
      "Epoch 62 Total loss: 586083.25\n",
      "Epoch 63 Total loss: 580033.0208333334\n",
      "Epoch 64 Total loss: 574151.4583333334\n",
      "Epoch 65 Total loss: 568404.3680555555\n",
      "Epoch 66 Total loss: 562817.1458333334\n",
      "Epoch 67 Total loss: 557360.2777777778\n",
      "Epoch 68 Total loss: 552029.7361111111\n",
      "Epoch 69 Total loss: 546837.2638888889\n",
      "Epoch 70 Total loss: 541765.2708333334\n",
      "Epoch 71 Total loss: 536820.25\n",
      "Epoch 72 Total loss: 531982.3263888889\n",
      "Epoch 73 Total loss: 527246.0555555555\n",
      "Epoch 74 Total loss: 522628.8784722223\n",
      "Epoch 75 Total loss: 518132.1736111111\n",
      "Epoch 76 Total loss: 513734.2673611112\n",
      "Epoch 77 Total loss: 509429.64236111124\n",
      "Epoch 78 Total loss: 505215.43055555556\n",
      "Epoch 79 Total loss: 501094.72222222225\n",
      "Epoch 80 Total loss: 497064.6493055555\n",
      "Epoch 81 Total loss: 493117.1145833333\n",
      "Epoch 82 Total loss: 489233.46875\n",
      "Epoch 83 Total loss: 485429.1423611111\n",
      "Epoch 84 Total loss: 481721.2604166666\n",
      "Epoch 85 Total loss: 478088.1076388889\n",
      "Epoch 86 Total loss: 474516.96527777775\n",
      "Epoch 87 Total loss: 471004.65972222225\n",
      "Epoch 88 Total loss: 467577.7395833334\n",
      "Epoch 89 Total loss: 464236.6493055555\n",
      "Epoch 90 Total loss: 460949.2986111111\n",
      "Epoch 91 Total loss: 457735.27777777775\n",
      "Epoch 92 Total loss: 454579.34027777787\n",
      "Epoch 93 Total loss: 451469.6874999999\n",
      "Epoch 94 Total loss: 448412.1423611112\n",
      "Epoch 95 Total loss: 445420.05902777775\n",
      "Epoch 96 Total loss: 442478.3506944444\n",
      "Epoch 97 Total loss: 439580.9583333334\n",
      "Epoch 98 Total loss: 436726.87847222225\n",
      "Epoch 99 Total loss: 433921.9201388889\n",
      " mobilenet Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:06<00:00, 58.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 72.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2908722.25\n",
      "Epoch 1 Total loss: 2902265.111111111\n",
      "Epoch 2 Total loss: 2867498.6944444445\n",
      "Epoch 3 Total loss: 2789949.972222222\n",
      "Epoch 4 Total loss: 2682340.611111111\n",
      "Epoch 5 Total loss: 2553250.583333333\n",
      "Epoch 6 Total loss: 2405497.666666667\n",
      "Epoch 7 Total loss: 2258688.111111111\n",
      "Epoch 8 Total loss: 2123157.0416666665\n",
      "Epoch 9 Total loss: 2001494.472222222\n",
      "Epoch 10 Total loss: 1892416.513888889\n",
      "Epoch 11 Total loss: 1794658.6666666667\n",
      "Epoch 12 Total loss: 1706983.75\n",
      "Epoch 13 Total loss: 1628001.888888889\n",
      "Epoch 14 Total loss: 1556542.472222222\n",
      "Epoch 15 Total loss: 1491775.4444444445\n",
      "Epoch 16 Total loss: 1432465.6944444445\n",
      "Epoch 17 Total loss: 1378113.6666666665\n",
      "Epoch 18 Total loss: 1328409.9444444445\n",
      "Epoch 19 Total loss: 1282826.2499999998\n",
      "Epoch 20 Total loss: 1240684.9305555557\n",
      "Epoch 21 Total loss: 1201839.5694444443\n",
      "Epoch 22 Total loss: 1165893.9305555555\n",
      "Epoch 23 Total loss: 1132586.7638888888\n",
      "Epoch 24 Total loss: 1101119.9444444443\n",
      "Epoch 25 Total loss: 1071645.4722222222\n",
      "Epoch 26 Total loss: 1044138.5277777776\n",
      "Epoch 27 Total loss: 1018222.7361111112\n",
      "Epoch 28 Total loss: 993896.423611111\n",
      "Epoch 29 Total loss: 970945.0138888888\n",
      "Epoch 30 Total loss: 949175.7222222222\n",
      "Epoch 31 Total loss: 928538.9166666666\n",
      "Epoch 32 Total loss: 908927.0763888888\n",
      "Epoch 33 Total loss: 890293.8819444443\n",
      "Epoch 34 Total loss: 872610.375\n",
      "Epoch 35 Total loss: 855783.1944444445\n",
      "Epoch 36 Total loss: 839754.0\n",
      "Epoch 37 Total loss: 824564.4791666666\n",
      "Epoch 38 Total loss: 810192.9583333334\n",
      "Epoch 39 Total loss: 796461.6180555555\n",
      "Epoch 40 Total loss: 783210.673611111\n",
      "Epoch 41 Total loss: 770421.2083333333\n",
      "Epoch 42 Total loss: 758174.6111111112\n",
      "Epoch 43 Total loss: 746512.0555555556\n",
      "Epoch 44 Total loss: 735314.9166666666\n",
      "Epoch 45 Total loss: 724549.8472222222\n",
      "Epoch 46 Total loss: 714177.4722222222\n",
      "Epoch 47 Total loss: 704174.5416666667\n",
      "Epoch 48 Total loss: 694541.7430555556\n",
      "Epoch 49 Total loss: 685236.5416666667\n",
      "Epoch 50 Total loss: 676212.1597222222\n",
      "Epoch 51 Total loss: 667555.0069444444\n",
      "Epoch 52 Total loss: 659193.0416666667\n",
      "Epoch 53 Total loss: 651055.25\n",
      "Epoch 54 Total loss: 643234.8402777778\n",
      "Epoch 55 Total loss: 635694.4791666666\n",
      "Epoch 56 Total loss: 628368.0902777778\n",
      "Epoch 57 Total loss: 621276.4305555555\n",
      "Epoch 58 Total loss: 614430.236111111\n",
      "Epoch 59 Total loss: 607836.6666666667\n",
      "Epoch 60 Total loss: 601443.7361111111\n",
      "Epoch 61 Total loss: 595219.3819444446\n",
      "Epoch 62 Total loss: 589128.0069444445\n",
      "Epoch 63 Total loss: 583144.7291666666\n",
      "Epoch 64 Total loss: 577314.5486111111\n",
      "Epoch 65 Total loss: 571600.3888888889\n",
      "Epoch 66 Total loss: 566075.5277777779\n",
      "Epoch 67 Total loss: 560721.6180555556\n",
      "Epoch 68 Total loss: 555490.6666666666\n",
      "Epoch 69 Total loss: 550371.8055555556\n",
      "Epoch 70 Total loss: 545370.1180555555\n",
      "Epoch 71 Total loss: 540505.625\n",
      "Epoch 72 Total loss: 535773.3611111111\n",
      "Epoch 73 Total loss: 531175.9444444445\n",
      "Epoch 74 Total loss: 526707.763888889\n",
      "Epoch 75 Total loss: 522309.1875\n",
      "Epoch 76 Total loss: 517987.9270833333\n",
      "Epoch 77 Total loss: 513785.5833333333\n",
      "Epoch 78 Total loss: 509681.74305555556\n",
      "Epoch 79 Total loss: 505683.35069444444\n",
      "Epoch 80 Total loss: 501747.96875\n",
      "Epoch 81 Total loss: 497871.6979166667\n",
      "Epoch 82 Total loss: 494062.0104166667\n",
      "Epoch 83 Total loss: 490385.1979166667\n",
      "Epoch 84 Total loss: 486824.70486111107\n",
      "Epoch 85 Total loss: 483308.98611111107\n",
      "Epoch 86 Total loss: 479865.90625\n",
      "Epoch 87 Total loss: 476472.5868055556\n",
      "Epoch 88 Total loss: 473134.22569444444\n",
      "Epoch 89 Total loss: 469870.25347222213\n",
      "Epoch 90 Total loss: 466686.9201388889\n",
      "Epoch 91 Total loss: 463530.1215277778\n",
      "Epoch 92 Total loss: 460433.7708333333\n",
      "Epoch 93 Total loss: 457378.39236111107\n",
      "Epoch 94 Total loss: 454394.9305555555\n",
      "Epoch 95 Total loss: 451466.43055555545\n",
      "Epoch 96 Total loss: 448582.1111111111\n",
      "Epoch 97 Total loss: 445745.16666666674\n",
      "Epoch 98 Total loss: 442964.2048611111\n",
      "Epoch 99 Total loss: 440227.9374999999\n",
      " inception Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:05<00:00, 70.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 72.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2177125.8541666665\n",
      "Epoch 1 Total loss: 2524114.027777778\n",
      "Epoch 2 Total loss: 2443766.1111111115\n",
      "Epoch 3 Total loss: 2314435.3333333335\n",
      "Epoch 4 Total loss: 2189648.777777778\n",
      "Epoch 5 Total loss: 2073485.6249999998\n",
      "Epoch 6 Total loss: 1960253.6944444445\n",
      "Epoch 7 Total loss: 1853744.736111111\n",
      "Epoch 8 Total loss: 1755630.3472222225\n",
      "Epoch 9 Total loss: 1665884.5555555555\n",
      "Epoch 10 Total loss: 1584010.7083333333\n",
      "Epoch 11 Total loss: 1508809.361111111\n",
      "Epoch 12 Total loss: 1440031.597222222\n",
      "Epoch 13 Total loss: 1377997.638888889\n",
      "Epoch 14 Total loss: 1322138.4166666665\n",
      "Epoch 15 Total loss: 1270897.4583333335\n",
      "Epoch 16 Total loss: 1223515.3194444443\n",
      "Epoch 17 Total loss: 1179907.375\n",
      "Epoch 18 Total loss: 1139192.3194444445\n",
      "Epoch 19 Total loss: 1101223.8194444443\n",
      "Epoch 20 Total loss: 1065931.402777778\n",
      "Epoch 21 Total loss: 1033017.673611111\n",
      "Epoch 22 Total loss: 1002322.5972222222\n",
      "Epoch 23 Total loss: 974007.5486111112\n",
      "Epoch 24 Total loss: 948268.861111111\n",
      "Epoch 25 Total loss: 924093.3888888889\n",
      "Epoch 26 Total loss: 900952.4583333333\n",
      "Epoch 27 Total loss: 878928.5624999999\n",
      "Epoch 28 Total loss: 858165.0694444445\n",
      "Epoch 29 Total loss: 838569.0694444445\n",
      "Epoch 30 Total loss: 819936.5555555555\n",
      "Epoch 31 Total loss: 802422.2222222221\n",
      "Epoch 32 Total loss: 785905.3125\n",
      "Epoch 33 Total loss: 770237.9305555556\n",
      "Epoch 34 Total loss: 755240.2777777776\n",
      "Epoch 35 Total loss: 740963.7430555555\n",
      "Epoch 36 Total loss: 727325.888888889\n",
      "Epoch 37 Total loss: 714361.0625\n",
      "Epoch 38 Total loss: 701909.5694444445\n",
      "Epoch 39 Total loss: 689968.4652777778\n",
      "Epoch 40 Total loss: 678530.9027777779\n",
      "Epoch 41 Total loss: 667625.6736111111\n",
      "Epoch 42 Total loss: 657119.3472222224\n",
      "Epoch 43 Total loss: 646988.7222222222\n",
      "Epoch 44 Total loss: 637271.6319444445\n",
      "Epoch 45 Total loss: 627936.8680555555\n",
      "Epoch 46 Total loss: 618967.1666666666\n",
      "Epoch 47 Total loss: 610301.8680555555\n",
      "Epoch 48 Total loss: 601995.3888888888\n",
      "Epoch 49 Total loss: 593957.5902777778\n",
      "Epoch 50 Total loss: 586144.6527777778\n",
      "Epoch 51 Total loss: 578668.4444444445\n",
      "Epoch 52 Total loss: 571401.9097222222\n",
      "Epoch 53 Total loss: 564331.0\n",
      "Epoch 54 Total loss: 557567.3680555555\n",
      "Epoch 55 Total loss: 551043.0972222221\n",
      "Epoch 56 Total loss: 544710.5833333334\n",
      "Epoch 57 Total loss: 538564.0\n",
      "Epoch 58 Total loss: 532580.6527777778\n",
      "Epoch 59 Total loss: 526745.5138888889\n",
      "Epoch 60 Total loss: 521068.2986111111\n",
      "Epoch 61 Total loss: 515594.5243055556\n",
      "Epoch 62 Total loss: 510308.40625\n",
      "Epoch 63 Total loss: 505132.0833333333\n",
      "Epoch 64 Total loss: 500068.9826388889\n",
      "Epoch 65 Total loss: 495123.7361111111\n",
      "Epoch 66 Total loss: 490309.46527777775\n",
      "Epoch 67 Total loss: 485641.1527777778\n",
      "Epoch 68 Total loss: 481085.2361111111\n",
      "Epoch 69 Total loss: 476647.21180555556\n",
      "Epoch 70 Total loss: 472333.4201388889\n",
      "Epoch 71 Total loss: 468138.3090277778\n",
      "Epoch 72 Total loss: 464015.1145833333\n",
      "Epoch 73 Total loss: 459959.3958333333\n",
      "Epoch 74 Total loss: 455992.03819444444\n",
      "Epoch 75 Total loss: 452143.2916666666\n",
      "Epoch 76 Total loss: 448362.0972222222\n",
      "Epoch 77 Total loss: 444660.14236111107\n",
      "Epoch 78 Total loss: 441033.84375000006\n",
      "Epoch 79 Total loss: 437476.8993055555\n",
      "Epoch 80 Total loss: 433999.1354166666\n",
      "Epoch 81 Total loss: 430611.4270833333\n",
      "Epoch 82 Total loss: 427314.7847222222\n",
      "Epoch 83 Total loss: 424066.0625\n",
      "Epoch 84 Total loss: 420874.7986111112\n",
      "Epoch 85 Total loss: 417742.7465277778\n",
      "Epoch 86 Total loss: 414679.1111111111\n",
      "Epoch 87 Total loss: 411682.0694444445\n",
      "Epoch 88 Total loss: 408743.50694444444\n",
      "Epoch 89 Total loss: 405866.74305555556\n",
      "Epoch 90 Total loss: 403038.5069444444\n",
      "Epoch 91 Total loss: 400245.99999999994\n",
      "Epoch 92 Total loss: 397514.37847222225\n",
      "Epoch 93 Total loss: 394853.1284722222\n",
      "Epoch 94 Total loss: 392235.6180555555\n",
      "Epoch 95 Total loss: 389660.9236111111\n",
      "Epoch 96 Total loss: 387130.3125\n",
      "Epoch 97 Total loss: 384641.1215277778\n",
      "Epoch 98 Total loss: 382182.2638888889\n",
      "Epoch 99 Total loss: 379764.72222222225\n",
      " resnet Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:05<00:00, 70.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 71.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2907666.333333333\n",
      "Epoch 1 Total loss: 2903294.027777778\n",
      "Epoch 2 Total loss: 2873430.972222222\n",
      "Epoch 3 Total loss: 2785701.4722222225\n",
      "Epoch 4 Total loss: 2667162.277777778\n",
      "Epoch 5 Total loss: 2545947.25\n",
      "Epoch 6 Total loss: 2427074.888888889\n",
      "Epoch 7 Total loss: 2308585.2222222225\n",
      "Epoch 8 Total loss: 2196628.694444444\n",
      "Epoch 9 Total loss: 2093401.236111111\n",
      "Epoch 10 Total loss: 1998766.1666666665\n",
      "Epoch 11 Total loss: 1911421.763888889\n",
      "Epoch 12 Total loss: 1830741.208333333\n",
      "Epoch 13 Total loss: 1755937.4027777778\n",
      "Epoch 14 Total loss: 1686157.236111111\n",
      "Epoch 15 Total loss: 1621085.0833333337\n",
      "Epoch 16 Total loss: 1560487.8888888888\n",
      "Epoch 17 Total loss: 1503894.5416666667\n",
      "Epoch 18 Total loss: 1451234.9722222222\n",
      "Epoch 19 Total loss: 1402086.6388888888\n",
      "Epoch 20 Total loss: 1356044.1805555555\n",
      "Epoch 21 Total loss: 1312958.1388888888\n",
      "Epoch 22 Total loss: 1272566.3611111112\n",
      "Epoch 23 Total loss: 1234727.486111111\n",
      "Epoch 24 Total loss: 1199649.4444444445\n",
      "Epoch 25 Total loss: 1167274.4861111112\n",
      "Epoch 26 Total loss: 1136912.625\n",
      "Epoch 27 Total loss: 1107864.8055555557\n",
      "Epoch 28 Total loss: 1080201.513888889\n",
      "Epoch 29 Total loss: 1054075.6666666667\n",
      "Epoch 30 Total loss: 1029403.8888888889\n",
      "Epoch 31 Total loss: 1005983.6666666665\n",
      "Epoch 32 Total loss: 983784.4027777778\n",
      "Epoch 33 Total loss: 962650.375\n",
      "Epoch 34 Total loss: 942509.7708333334\n",
      "Epoch 35 Total loss: 923406.9166666667\n",
      "Epoch 36 Total loss: 905207.1041666667\n",
      "Epoch 37 Total loss: 887883.6250000001\n",
      "Epoch 38 Total loss: 871377.2361111111\n",
      "Epoch 39 Total loss: 855642.5347222222\n",
      "Epoch 40 Total loss: 840568.2291666667\n",
      "Epoch 41 Total loss: 826020.375\n",
      "Epoch 42 Total loss: 812000.048611111\n",
      "Epoch 43 Total loss: 798542.8125\n",
      "Epoch 44 Total loss: 785621.0972222221\n",
      "Epoch 45 Total loss: 773263.1388888889\n",
      "Epoch 46 Total loss: 761401.3055555556\n",
      "Epoch 47 Total loss: 750007.8611111111\n",
      "Epoch 48 Total loss: 738999.1041666666\n",
      "Epoch 49 Total loss: 728365.2222222222\n",
      "Epoch 50 Total loss: 718066.4513888888\n",
      "Epoch 51 Total loss: 708091.4166666667\n",
      "Epoch 52 Total loss: 698481.7569444444\n",
      "Epoch 53 Total loss: 689258.8402777778\n",
      "Epoch 54 Total loss: 680364.6736111111\n",
      "Epoch 55 Total loss: 671743.8611111111\n",
      "Epoch 56 Total loss: 663406.6666666666\n",
      "Epoch 57 Total loss: 655360.6041666666\n",
      "Epoch 58 Total loss: 647522.2222222222\n",
      "Epoch 59 Total loss: 639962.4722222222\n",
      "Epoch 60 Total loss: 632630.6111111111\n",
      "Epoch 61 Total loss: 625498.0972222222\n",
      "Epoch 62 Total loss: 618566.2430555556\n",
      "Epoch 63 Total loss: 611820.4513888889\n",
      "Epoch 64 Total loss: 605240.8611111111\n",
      "Epoch 65 Total loss: 598811.2499999999\n",
      "Epoch 66 Total loss: 592564.9652777778\n",
      "Epoch 67 Total loss: 586467.0069444445\n",
      "Epoch 68 Total loss: 580523.4166666666\n",
      "Epoch 69 Total loss: 574761.7430555556\n",
      "Epoch 70 Total loss: 569179.4513888889\n",
      "Epoch 71 Total loss: 563723.7638888889\n",
      "Epoch 72 Total loss: 558395.1874999999\n",
      "Epoch 73 Total loss: 553204.7222222222\n",
      "Epoch 74 Total loss: 548147.4652777778\n",
      "Epoch 75 Total loss: 543193.0347222222\n",
      "Epoch 76 Total loss: 538370.125\n",
      "Epoch 77 Total loss: 533662.4513888889\n",
      "Epoch 78 Total loss: 529085.0277777779\n",
      "Epoch 79 Total loss: 524615.6215277779\n",
      "Epoch 80 Total loss: 520214.21527777775\n",
      "Epoch 81 Total loss: 515892.30555555556\n",
      "Epoch 82 Total loss: 511656.3680555556\n",
      "Epoch 83 Total loss: 507510.0173611112\n",
      "Epoch 84 Total loss: 503447.5625\n",
      "Epoch 85 Total loss: 499482.8993055555\n",
      "Epoch 86 Total loss: 495599.14583333326\n",
      "Epoch 87 Total loss: 491804.46874999994\n",
      "Epoch 88 Total loss: 488089.9826388889\n",
      "Epoch 89 Total loss: 484446.6458333333\n",
      "Epoch 90 Total loss: 480866.09027777775\n",
      "Epoch 91 Total loss: 477354.50694444444\n",
      "Epoch 92 Total loss: 473908.0833333333\n",
      "Epoch 93 Total loss: 470531.86805555556\n",
      "Epoch 94 Total loss: 467230.2916666666\n",
      "Epoch 95 Total loss: 463983.71875\n",
      "Epoch 96 Total loss: 460781.3298611111\n",
      "Epoch 97 Total loss: 457627.71875\n",
      "Epoch 98 Total loss: 454532.3194444445\n",
      "Epoch 99 Total loss: 451495.5625\n",
      " vgg Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:05<00:00, 73.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 75.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "for model_name in ['efficientnet', 'mobilenet','inception', 'resnet', 'vgg']:\n",
    "    train_dataset, test_dataset = load_data_for_model()\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(f\" {model_name} Gen encoding... \")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
