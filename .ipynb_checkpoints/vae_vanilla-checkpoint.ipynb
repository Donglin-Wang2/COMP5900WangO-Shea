{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_vanilla import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(batch_size=64):\n",
    "    ## Loading dataset\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated.npy')\n",
    "    imgs = np.load('./data/LFSD_imgs.npy')\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        dataset.append((img_batch, depth_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'efficientnet': 1280,\n",
    "        'vgg': 512,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAE(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Train Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(f\"Epoch {i} Total loss: { losses_across_epochs['loss'][-1]}\")\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_vanilla/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        \n",
    "        np.save('./results/vae_vanilla/%s_%s' % (model_name, k), np.array(v))\n",
    "\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_vanilla/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_vanilla/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "    from skimage.transform import resize\n",
    "    from tensorflow.keras import datasets\n",
    "\n",
    "    train_dataset, test_dataset = None, None\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "    with tf.device('/cpu:0'):\n",
    "        train_images = tf.image.resize(train_images, (256, 256)).numpy()\n",
    "        test_images = tf.image.resize(test_images, (256, 256)).numpy()\n",
    "    \n",
    "    train_result, _, _ = vae.encode(train_images[:128], None, rand_depth=True)\n",
    "    for i in tqdm(range(128, len(train_images), 128)):\n",
    "        img = train_images[i:i+128]\n",
    "        activation, _, _ = vae.encode(img, None, rand_depth=True)\n",
    "        train_result = tf.concat((train_result, activation), axis=0)        \n",
    "    np.save('./data/CIFAR100_vae_vanilla_%s_encoding_train.npy' % model_name, train_result.numpy())\n",
    "\n",
    "    test_result, _, _ = vae.encode(test_images[:128], None, rand_depth=True)\n",
    "    for i in tqdm(range(128, len(test_images), 128)):\n",
    "        img = train_images[i:i+128]\n",
    "        activation, _, _ = vae.encode(img, None, rand_depth=True)\n",
    "        train_result = tf.concat((test_result, activation), axis=0)  \n",
    "    np.save('./data/CIFAR100_vae_vanilla_%s_encoding_test.npy' % model_name, test_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2908242.444444444\n",
      "Epoch 1 Total loss: 2904333.444444445\n",
      "Epoch 2 Total loss: 2880892.8333333335\n",
      "Epoch 3 Total loss: 2814354.833333333\n",
      "Epoch 4 Total loss: 2713130.5277777775\n",
      "Epoch 5 Total loss: 2587788.6111111115\n",
      "Epoch 6 Total loss: 2447528.055555556\n",
      "Epoch 7 Total loss: 2308516.638888889\n",
      "Epoch 8 Total loss: 2182060.6666666665\n",
      "Epoch 9 Total loss: 2067482.4166666665\n",
      "Epoch 10 Total loss: 1963549.944444444\n",
      "Epoch 11 Total loss: 1868865.5694444445\n",
      "Epoch 12 Total loss: 1782617.4999999998\n",
      "Epoch 13 Total loss: 1703761.6944444443\n",
      "Epoch 14 Total loss: 1631415.2638888888\n",
      "Epoch 15 Total loss: 1565192.361111111\n",
      "Epoch 16 Total loss: 1504388.9305555555\n",
      "Epoch 17 Total loss: 1448474.5972222222\n",
      "Epoch 18 Total loss: 1396794.8333333335\n",
      "Epoch 19 Total loss: 1348848.5000000002\n",
      "Epoch 20 Total loss: 1304495.388888889\n",
      "Epoch 21 Total loss: 1263370.4583333335\n",
      "Epoch 22 Total loss: 1225446.5555555555\n",
      "Epoch 23 Total loss: 1190414.3055555555\n",
      "Epoch 24 Total loss: 1157491.4027777778\n",
      "Epoch 25 Total loss: 1126402.0555555555\n",
      "Epoch 26 Total loss: 1097001.0\n",
      "Epoch 27 Total loss: 1069374.2083333333\n",
      "Epoch 28 Total loss: 1043321.9374999999\n",
      "Epoch 29 Total loss: 1018628.5694444445\n",
      "Epoch 30 Total loss: 995266.826388889\n",
      "Epoch 31 Total loss: 973164.4930555556\n",
      "Epoch 32 Total loss: 952254.7152777779\n",
      "Epoch 33 Total loss: 932411.2916666667\n",
      "Epoch 34 Total loss: 913549.2430555555\n",
      "Epoch 35 Total loss: 895572.3055555555\n",
      "Epoch 36 Total loss: 878437.6041666666\n",
      "Epoch 37 Total loss: 862106.6597222221\n",
      "Epoch 38 Total loss: 846445.9305555555\n",
      "Epoch 39 Total loss: 831473.9722222222\n",
      "Epoch 40 Total loss: 817191.4027777779\n",
      "Epoch 41 Total loss: 803602.5972222222\n",
      "Epoch 42 Total loss: 790623.513888889\n",
      "Epoch 43 Total loss: 778173.9791666667\n",
      "Epoch 44 Total loss: 766069.0972222222\n",
      "Epoch 45 Total loss: 754380.5069444445\n",
      "Epoch 46 Total loss: 743124.8125\n",
      "Epoch 47 Total loss: 732325.4375\n",
      "Epoch 48 Total loss: 721902.4583333334\n",
      "Epoch 49 Total loss: 711859.7500000001\n",
      "Epoch 50 Total loss: 702196.8749999999\n",
      "Epoch 51 Total loss: 692920.1597222222\n",
      "Epoch 52 Total loss: 683952.9097222222\n",
      "Epoch 53 Total loss: 675248.7083333335\n",
      "Epoch 54 Total loss: 666811.6527777778\n",
      "Epoch 55 Total loss: 658654.0694444444\n",
      "Epoch 56 Total loss: 650730.3958333335\n",
      "Epoch 57 Total loss: 643070.9305555555\n",
      "Epoch 58 Total loss: 635656.1458333333\n",
      "Epoch 59 Total loss: 628499.2430555556\n",
      "Epoch 60 Total loss: 621503.2777777779\n",
      "Epoch 61 Total loss: 614717.3958333333\n",
      "Epoch 62 Total loss: 608087.548611111\n",
      "Epoch 63 Total loss: 601656.1875\n",
      "Epoch 64 Total loss: 595422.75\n",
      "Epoch 65 Total loss: 589366.0277777779\n",
      "Epoch 66 Total loss: 583516.0069444445\n",
      "Epoch 67 Total loss: 577802.375\n",
      "Epoch 68 Total loss: 572261.8819444445\n",
      "Epoch 69 Total loss: 566898.3888888889\n",
      "Epoch 70 Total loss: 561650.6805555556\n",
      "Epoch 71 Total loss: 556512.2638888889\n",
      "Epoch 72 Total loss: 551451.2152777778\n",
      "Epoch 73 Total loss: 546492.2361111111\n",
      "Epoch 74 Total loss: 541658.3333333334\n",
      "Epoch 75 Total loss: 536936.8333333333\n",
      "Epoch 76 Total loss: 532320.5277777778\n",
      "Epoch 77 Total loss: 527809.2291666667\n",
      "Epoch 78 Total loss: 523439.99652777775\n",
      "Epoch 79 Total loss: 519163.84374999994\n",
      "Epoch 80 Total loss: 514946.68055555556\n",
      "Epoch 81 Total loss: 510809.2256944444\n",
      "Epoch 82 Total loss: 506765.2951388889\n",
      "Epoch 83 Total loss: 502808.59375\n",
      "Epoch 84 Total loss: 498926.045138889\n",
      "Epoch 85 Total loss: 495145.32986111107\n",
      "Epoch 86 Total loss: 491444.21527777775\n",
      "Epoch 87 Total loss: 487818.47222222225\n",
      "Epoch 88 Total loss: 484276.5763888889\n",
      "Epoch 89 Total loss: 480792.78124999994\n",
      "Epoch 90 Total loss: 477356.9895833333\n",
      "Epoch 91 Total loss: 474003.1770833334\n",
      "Epoch 92 Total loss: 470718.04513888893\n",
      "Epoch 93 Total loss: 467488.09027777787\n",
      "Epoch 94 Total loss: 464320.41319444444\n",
      "Epoch 95 Total loss: 461205.5034722222\n",
      "Epoch 96 Total loss: 458152.15625\n",
      "Epoch 97 Total loss: 455150.17013888893\n",
      "Epoch 98 Total loss: 452203.68402777775\n",
      "Epoch 99 Total loss: 449292.22222222225\n",
      "Gen encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:09<00:00, 42.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 50.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2881126.1944444445\n",
      "Epoch 1 Total loss: 2709657.8055555555\n",
      "Epoch 2 Total loss: 2616610.361111111\n",
      "Epoch 3 Total loss: 2487864.2777777775\n",
      "Epoch 4 Total loss: 2347294.361111111\n",
      "Epoch 5 Total loss: 2203783.638888889\n",
      "Epoch 6 Total loss: 2065879.1111111112\n",
      "Epoch 7 Total loss: 1939857.4861111112\n",
      "Epoch 8 Total loss: 1828539.4027777775\n",
      "Epoch 9 Total loss: 1729399.7916666665\n",
      "Epoch 10 Total loss: 1638541.8194444445\n",
      "Epoch 11 Total loss: 1555953.3888888888\n",
      "Epoch 12 Total loss: 1480860.3194444443\n",
      "Epoch 13 Total loss: 1412535.2083333333\n",
      "Epoch 14 Total loss: 1350362.5833333335\n",
      "Epoch 15 Total loss: 1293820.375\n",
      "Epoch 16 Total loss: 1242230.5833333333\n",
      "Epoch 17 Total loss: 1195516.9444444443\n",
      "Epoch 18 Total loss: 1152755.8611111112\n",
      "Epoch 19 Total loss: 1113450.861111111\n",
      "Epoch 20 Total loss: 1077072.9722222222\n",
      "Epoch 21 Total loss: 1043414.1875\n",
      "Epoch 22 Total loss: 1012230.9583333333\n",
      "Epoch 23 Total loss: 983049.4444444445\n",
      "Epoch 24 Total loss: 955603.2152777778\n",
      "Epoch 25 Total loss: 929897.1944444444\n",
      "Epoch 26 Total loss: 905897.7222222221\n",
      "Epoch 27 Total loss: 883366.9930555554\n",
      "Epoch 28 Total loss: 862163.1875000001\n",
      "Epoch 29 Total loss: 842442.1111111111\n",
      "Epoch 30 Total loss: 823893.3263888889\n",
      "Epoch 31 Total loss: 806362.548611111\n",
      "Epoch 32 Total loss: 789530.9930555555\n",
      "Epoch 33 Total loss: 773473.4652777778\n",
      "Epoch 34 Total loss: 758205.6805555556\n",
      "Epoch 35 Total loss: 743698.861111111\n",
      "Epoch 36 Total loss: 729893.0208333334\n",
      "Epoch 37 Total loss: 716703.6944444445\n",
      "Epoch 38 Total loss: 704106.4305555555\n",
      "Epoch 39 Total loss: 692152.5\n",
      "Epoch 40 Total loss: 680755.4444444445\n",
      "Epoch 41 Total loss: 669861.8819444445\n",
      "Epoch 42 Total loss: 659398.2083333333\n",
      "Epoch 43 Total loss: 649304.0625\n",
      "Epoch 44 Total loss: 639624.236111111\n",
      "Epoch 45 Total loss: 630297.4166666667\n",
      "Epoch 46 Total loss: 621322.1527777778\n",
      "Epoch 47 Total loss: 612691.9583333333\n",
      "Epoch 48 Total loss: 604376.875\n",
      "Epoch 49 Total loss: 596349.2777777779\n",
      "Epoch 50 Total loss: 588674.361111111\n",
      "Epoch 51 Total loss: 581185.9375\n",
      "Epoch 52 Total loss: 573976.6180555556\n",
      "Epoch 53 Total loss: 567018.1041666666\n",
      "Epoch 54 Total loss: 560285.6666666667\n",
      "Epoch 55 Total loss: 553776.1805555556\n",
      "Epoch 56 Total loss: 547420.986111111\n",
      "Epoch 57 Total loss: 541273.2708333333\n",
      "Epoch 58 Total loss: 535280.5902777778\n",
      "Epoch 59 Total loss: 529451.8263888888\n",
      "Epoch 60 Total loss: 523743.8888888889\n",
      "Epoch 61 Total loss: 518188.86458333326\n",
      "Epoch 62 Total loss: 512824.22222222225\n",
      "Epoch 63 Total loss: 507626.1111111111\n",
      "Epoch 64 Total loss: 502569.5972222222\n",
      "Epoch 65 Total loss: 497650.09722222213\n",
      "Epoch 66 Total loss: 492850.0902777778\n",
      "Epoch 67 Total loss: 488202.8298611111\n",
      "Epoch 68 Total loss: 483650.15625000006\n",
      "Epoch 69 Total loss: 479208.3680555555\n",
      "Epoch 70 Total loss: 474857.02083333326\n",
      "Epoch 71 Total loss: 470608.9583333333\n",
      "Epoch 72 Total loss: 466445.1770833334\n",
      "Epoch 73 Total loss: 462389.22222222225\n",
      "Epoch 74 Total loss: 458429.6076388889\n",
      "Epoch 75 Total loss: 454566.6979166666\n",
      "Epoch 76 Total loss: 450790.25694444444\n",
      "Epoch 77 Total loss: 447123.59722222225\n",
      "Epoch 78 Total loss: 443537.9479166666\n",
      "Epoch 79 Total loss: 440019.3020833333\n",
      "Epoch 80 Total loss: 436578.2291666667\n",
      "Epoch 81 Total loss: 433206.4131944445\n",
      "Epoch 82 Total loss: 429916.6319444444\n",
      "Epoch 83 Total loss: 426686.15972222225\n",
      "Epoch 84 Total loss: 423510.47222222225\n",
      "Epoch 85 Total loss: 420390.1875\n",
      "Epoch 86 Total loss: 417335.2187499999\n",
      "Epoch 87 Total loss: 414343.8958333333\n",
      "Epoch 88 Total loss: 411422.6770833333\n",
      "Epoch 89 Total loss: 408534.8506944444\n",
      "Epoch 90 Total loss: 405697.65625\n",
      "Epoch 91 Total loss: 402915.35069444444\n",
      "Epoch 92 Total loss: 400190.6111111111\n",
      "Epoch 93 Total loss: 397522.4166666667\n",
      "Epoch 94 Total loss: 394898.8541666667\n",
      "Epoch 95 Total loss: 392334.0138888889\n",
      "Epoch 96 Total loss: 389816.3888888889\n",
      "Epoch 97 Total loss: 387334.15625\n",
      "Epoch 98 Total loss: 384896.6076388889\n",
      "Epoch 99 Total loss: 382486.9270833333\n",
      "Gen encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:07<00:00, 55.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 60.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2849275.222222222\n",
      "Epoch 1 Total loss: 2702035.388888889\n",
      "Epoch 2 Total loss: 2643367.277777778\n",
      "Epoch 3 Total loss: 2551540.6111111115\n",
      "Epoch 4 Total loss: 2434087.583333333\n",
      "Epoch 5 Total loss: 2301143.5\n",
      "Epoch 6 Total loss: 2162925.0694444445\n",
      "Epoch 7 Total loss: 2030672.2083333333\n",
      "Epoch 8 Total loss: 1911214.708333333\n",
      "Epoch 9 Total loss: 1802713.0\n",
      "Epoch 10 Total loss: 1703699.5694444445\n",
      "Epoch 11 Total loss: 1614014.5694444447\n",
      "Epoch 12 Total loss: 1532890.5972222225\n",
      "Epoch 13 Total loss: 1459589.5416666665\n",
      "Epoch 14 Total loss: 1393745.2222222222\n",
      "Epoch 15 Total loss: 1334093.375\n",
      "Epoch 16 Total loss: 1280136.0138888888\n",
      "Epoch 17 Total loss: 1231018.027777778\n",
      "Epoch 18 Total loss: 1185684.125\n",
      "Epoch 19 Total loss: 1143983.3055555555\n",
      "Epoch 20 Total loss: 1105538.9722222222\n",
      "Epoch 21 Total loss: 1070003.9305555557\n",
      "Epoch 22 Total loss: 1037170.4791666667\n",
      "Epoch 23 Total loss: 1006596.3680555556\n",
      "Epoch 24 Total loss: 978181.486111111\n",
      "Epoch 25 Total loss: 951419.3680555555\n",
      "Epoch 26 Total loss: 926467.298611111\n",
      "Epoch 27 Total loss: 903071.4375000001\n",
      "Epoch 28 Total loss: 880964.9861111111\n",
      "Epoch 29 Total loss: 860129.2847222222\n",
      "Epoch 30 Total loss: 840494.1875000001\n",
      "Epoch 31 Total loss: 821976.7152777778\n",
      "Epoch 32 Total loss: 804559.7916666666\n",
      "Epoch 33 Total loss: 788083.9513888889\n",
      "Epoch 34 Total loss: 772491.3541666667\n",
      "Epoch 35 Total loss: 757562.2083333334\n",
      "Epoch 36 Total loss: 743238.826388889\n",
      "Epoch 37 Total loss: 729589.2222222222\n",
      "Epoch 38 Total loss: 716529.9166666665\n",
      "Epoch 39 Total loss: 704092.0\n",
      "Epoch 40 Total loss: 692194.5555555556\n",
      "Epoch 41 Total loss: 680772.4930555556\n",
      "Epoch 42 Total loss: 669834.625\n",
      "Epoch 43 Total loss: 659354.9652777778\n",
      "Epoch 44 Total loss: 649293.5763888889\n",
      "Epoch 45 Total loss: 639647.1180555556\n",
      "Epoch 46 Total loss: 630341.7916666666\n",
      "Epoch 47 Total loss: 621387.6319444444\n",
      "Epoch 48 Total loss: 612781.7986111111\n",
      "Epoch 49 Total loss: 604498.611111111\n",
      "Epoch 50 Total loss: 596503.9652777778\n",
      "Epoch 51 Total loss: 588760.2986111111\n",
      "Epoch 52 Total loss: 581313.8125\n",
      "Epoch 53 Total loss: 574128.1041666666\n",
      "Epoch 54 Total loss: 567163.1805555556\n",
      "Epoch 55 Total loss: 560429.5277777778\n",
      "Epoch 56 Total loss: 553911.1111111111\n",
      "Epoch 57 Total loss: 547567.3125\n",
      "Epoch 58 Total loss: 541437.1111111111\n",
      "Epoch 59 Total loss: 535519.9722222222\n",
      "Epoch 60 Total loss: 529779.5486111111\n",
      "Epoch 61 Total loss: 524183.97222222225\n",
      "Epoch 62 Total loss: 518750.70486111107\n",
      "Epoch 63 Total loss: 513445.7916666667\n",
      "Epoch 64 Total loss: 508311.54861111107\n",
      "Epoch 65 Total loss: 503315.75\n",
      "Epoch 66 Total loss: 498460.97569444444\n",
      "Epoch 67 Total loss: 493716.25694444444\n",
      "Epoch 68 Total loss: 489086.68055555556\n",
      "Epoch 69 Total loss: 484587.9201388889\n",
      "Epoch 70 Total loss: 480195.875\n",
      "Epoch 71 Total loss: 475915.8576388889\n",
      "Epoch 72 Total loss: 471749.1319444445\n",
      "Epoch 73 Total loss: 467654.4583333334\n",
      "Epoch 74 Total loss: 463649.4166666667\n",
      "Epoch 75 Total loss: 459732.2256944445\n",
      "Epoch 76 Total loss: 455919.69444444444\n",
      "Epoch 77 Total loss: 452186.39930555556\n",
      "Epoch 78 Total loss: 448525.5173611111\n",
      "Epoch 79 Total loss: 444952.86111111107\n",
      "Epoch 80 Total loss: 441491.2395833334\n",
      "Epoch 81 Total loss: 438142.8541666666\n",
      "Epoch 82 Total loss: 434870.65625000006\n",
      "Epoch 83 Total loss: 431632.8576388889\n",
      "Epoch 84 Total loss: 428460.3611111111\n",
      "Epoch 85 Total loss: 425331.5243055555\n",
      "Epoch 86 Total loss: 422255.10763888893\n",
      "Epoch 87 Total loss: 419232.3541666666\n",
      "Epoch 88 Total loss: 416261.8854166666\n",
      "Epoch 89 Total loss: 413344.78125\n",
      "Epoch 90 Total loss: 410488.8472222222\n",
      "Epoch 91 Total loss: 407683.09027777775\n",
      "Epoch 92 Total loss: 404931.65625000006\n",
      "Epoch 93 Total loss: 402241.7986111111\n",
      "Epoch 94 Total loss: 399603.72222222225\n",
      "Epoch 95 Total loss: 397002.3368055556\n",
      "Epoch 96 Total loss: 394448.7048611111\n",
      "Epoch 97 Total loss: 391949.4826388889\n",
      "Epoch 98 Total loss: 389475.8125\n",
      "Epoch 99 Total loss: 387039.1458333334\n",
      "Gen encoding...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "for model_name in ['efficientnet','inception', 'resnet', 'mobilenet', 'vgg']:\n",
    "    train_dataset, test_dataset = load_data_for_model()\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(\"Gen encoding...\")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
