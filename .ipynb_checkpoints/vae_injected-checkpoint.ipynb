{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"gpu: { len(tf.config.list_physical_devices('GPU')) }\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_injected import VAEInjected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(model_name, batch_size=64):\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated.npy')\n",
    "    imgs = np.load('./data/LFSD_imgs.npy')\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    depths_feat = np.load('./data/LFSD_depths_repeated_%s_feat.npy' % model_name)\n",
    "    imgs_feat = np.load('./data/LFSD_imgs_%s_feat.npy' % model_name)\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        img_feat_batch, depth_feat_batch = imgs_feat[idx], depths_feat[idx]\n",
    "        dataset.append((img_batch, img_feat_batch, depth_batch, depth_feat_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'efficientnet': 1280,\n",
    "        'vgg': 512,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAEInjected(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Training Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "       \n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(f\"Epoch {i} Total loss: { losses_across_epochs['loss'][-1]}\")\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_injected/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        np.save('./results/vae_injected/%s_%s' % (model_name, k), np.array(v))\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_injected/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_injected/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "\n",
    "    train_dataset, test_dataset = None, None ## Freeing \n",
    "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "    with tf.device('/cpu:0'):\n",
    "        train_images = tf.image.resize(train_images, (256, 256)).numpy()\n",
    "        test_images = tf.image.resize(test_images, (256, 256)).numpy()\n",
    "    train_feats = np.load('./data/CIFAR100_%s_train_feat.npy' % model_name)\n",
    "    test_feats = np.load('./data/CIFAR100_%s_test_feat.npy' % model_name)\n",
    "\n",
    "    train_result, _, _ = vae.encode((train_images[:128], train_feats[:128], None, None, None), rand_depth=True)\n",
    "    for i in tqdm(range(128, len(train_images), 128)):\n",
    "        img = train_images[i:i+128]\n",
    "        img_feat = train_feats[i:i+128]\n",
    "        activation, _, _ = vae.encode((img, img_feat, None, None, None), rand_depth=True)\n",
    "        train_result = tf.concat((train_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_injected_%s_encoding_train.npy' % model_name, train_result.numpy())\n",
    "\n",
    "    test_result, _, _ = vae.encode((test_images[:128], test_feats[:128], None, None, None), rand_depth=True)\n",
    "    for i in tqdm(range(128, len(test_images), 128)):\n",
    "        img = test_images[i:i+128]\n",
    "        img_feat = test_feats[i:i+128]\n",
    "        activation, _, _ = vae.encode((img, img_feat, None, None, None), rand_depth=True)\n",
    "        test_result = tf.concat((test_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_injected_%s_encoding_test.npy' % model_name, test_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2918101.0\n",
      "Epoch 1 Total loss: 2908452.4444444445\n",
      "Epoch 2 Total loss: 2885707.3333333335\n",
      "Epoch 3 Total loss: 2805139.388888889\n",
      "Epoch 4 Total loss: 2684355.8055555555\n",
      "Epoch 5 Total loss: 2558966.0\n",
      "Epoch 6 Total loss: 2445195.388888889\n",
      "Epoch 7 Total loss: 2335355.638888889\n",
      "Epoch 8 Total loss: 2228063.8055555555\n",
      "Epoch 9 Total loss: 2124560.4444444445\n",
      "Epoch 10 Total loss: 2025443.1666666665\n",
      "Epoch 11 Total loss: 1931876.3611111115\n",
      "Epoch 12 Total loss: 1845497.8194444445\n",
      "Epoch 13 Total loss: 1766921.666666667\n",
      "Epoch 14 Total loss: 1695316.8750000002\n",
      "Epoch 15 Total loss: 1629795.7916666665\n",
      "Epoch 16 Total loss: 1569392.4027777778\n",
      "Epoch 17 Total loss: 1513202.138888889\n",
      "Epoch 18 Total loss: 1461356.6111111112\n",
      "Epoch 19 Total loss: 1413029.0138888888\n",
      "Epoch 20 Total loss: 1367807.9583333333\n",
      "Epoch 21 Total loss: 1325614.875\n",
      "Epoch 22 Total loss: 1286084.6249999998\n",
      "Epoch 23 Total loss: 1248924.013888889\n",
      "Epoch 24 Total loss: 1213846.4027777778\n",
      "Epoch 25 Total loss: 1181096.0972222222\n",
      "Epoch 26 Total loss: 1150516.611111111\n",
      "Epoch 27 Total loss: 1121795.5\n",
      "Epoch 28 Total loss: 1094561.0277777778\n",
      "Epoch 29 Total loss: 1068877.3611111112\n",
      "Epoch 30 Total loss: 1044391.8194444444\n",
      "Epoch 31 Total loss: 1021053.3055555557\n",
      "Epoch 32 Total loss: 998920.9513888889\n",
      "Epoch 33 Total loss: 977986.9305555555\n",
      "Epoch 34 Total loss: 958432.0208333333\n",
      "Epoch 35 Total loss: 939835.1388888889\n",
      "Epoch 36 Total loss: 921881.2152777779\n",
      "Epoch 37 Total loss: 904580.1527777778\n",
      "Epoch 38 Total loss: 888151.576388889\n",
      "Epoch 39 Total loss: 872438.4652777778\n",
      "Epoch 40 Total loss: 857401.4930555556\n",
      "Epoch 41 Total loss: 843160.423611111\n",
      "Epoch 42 Total loss: 829498.5138888889\n",
      "Epoch 43 Total loss: 816349.7152777778\n",
      "Epoch 44 Total loss: 803684.4583333333\n",
      "Epoch 45 Total loss: 791435.5416666667\n",
      "Epoch 46 Total loss: 779580.2152777778\n",
      "Epoch 47 Total loss: 768141.6597222221\n",
      "Epoch 48 Total loss: 757067.4791666667\n",
      "Epoch 49 Total loss: 746372.5624999999\n",
      "Epoch 50 Total loss: 736056.1875\n",
      "Epoch 51 Total loss: 726106.2152777778\n",
      "Epoch 52 Total loss: 716469.8194444445\n",
      "Epoch 53 Total loss: 707180.5555555556\n",
      "Epoch 54 Total loss: 698188.2986111112\n",
      "Epoch 55 Total loss: 689577.486111111\n",
      "Epoch 56 Total loss: 681270.2083333335\n",
      "Epoch 57 Total loss: 673193.5277777778\n",
      "Epoch 58 Total loss: 665350.5902777779\n",
      "Epoch 59 Total loss: 657717.3680555555\n",
      "Epoch 60 Total loss: 650290.6180555555\n",
      "Epoch 61 Total loss: 643050.9791666667\n",
      "Epoch 62 Total loss: 635990.8888888889\n",
      "Epoch 63 Total loss: 629138.6527777778\n",
      "Epoch 64 Total loss: 622478.8055555555\n",
      "Epoch 65 Total loss: 616021.298611111\n",
      "Epoch 66 Total loss: 609737.6180555555\n",
      "Epoch 67 Total loss: 603602.6597222222\n",
      "Epoch 68 Total loss: 597630.9583333334\n",
      "Epoch 69 Total loss: 591805.0624999999\n",
      "Epoch 70 Total loss: 586139.9375\n",
      "Epoch 71 Total loss: 580614.2777777778\n",
      "Epoch 72 Total loss: 575243.4097222222\n",
      "Epoch 73 Total loss: 570030.5416666666\n",
      "Epoch 74 Total loss: 564921.6736111111\n",
      "Epoch 75 Total loss: 559912.2430555555\n",
      "Epoch 76 Total loss: 555034.4791666666\n",
      "Epoch 77 Total loss: 550234.8194444445\n",
      "Epoch 78 Total loss: 545540.3611111111\n",
      "Epoch 79 Total loss: 540922.0902777779\n",
      "Epoch 80 Total loss: 536390.5486111111\n",
      "Epoch 81 Total loss: 531983.5833333334\n",
      "Epoch 82 Total loss: 527670.7222222222\n",
      "Epoch 83 Total loss: 523438.875\n",
      "Epoch 84 Total loss: 519314.31249999994\n",
      "Epoch 85 Total loss: 515296.7465277778\n",
      "Epoch 86 Total loss: 511373.6701388889\n",
      "Epoch 87 Total loss: 507529.2361111111\n",
      "Epoch 88 Total loss: 503759.0\n",
      "Epoch 89 Total loss: 500065.3229166667\n",
      "Epoch 90 Total loss: 496446.42708333326\n",
      "Epoch 91 Total loss: 492898.71875\n",
      "Epoch 92 Total loss: 489404.04513888893\n",
      "Epoch 93 Total loss: 485973.00347222225\n",
      "Epoch 94 Total loss: 482604.7708333334\n",
      "Epoch 95 Total loss: 479293.4895833333\n",
      "Epoch 96 Total loss: 476047.1354166666\n",
      "Epoch 97 Total loss: 472867.3854166666\n",
      "Epoch 98 Total loss: 469749.46875\n",
      "Epoch 99 Total loss: 466689.31597222225\n",
      " mobilenet Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:08<00:00, 45.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 40.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2868214.4166666665\n",
      "Epoch 1 Total loss: 2710182.6111111115\n",
      "Epoch 2 Total loss: 2681330.694444444\n",
      "Epoch 3 Total loss: 2639985.8055555555\n",
      "Epoch 4 Total loss: 2561795.0833333335\n",
      "Epoch 5 Total loss: 2463370.277777778\n",
      "Epoch 6 Total loss: 2365100.0555555555\n",
      "Epoch 7 Total loss: 2272080.4444444445\n",
      "Epoch 8 Total loss: 2180643.3888888885\n",
      "Epoch 9 Total loss: 2090913.736111111\n",
      "Epoch 10 Total loss: 2003916.1944444447\n",
      "Epoch 11 Total loss: 1921397.2499999998\n",
      "Epoch 12 Total loss: 1843086.4027777778\n",
      "Epoch 13 Total loss: 1772001.4027777778\n",
      "Epoch 14 Total loss: 1708900.861111111\n",
      "Epoch 15 Total loss: 1649659.666666667\n",
      "Epoch 16 Total loss: 1592867.2222222222\n",
      "Epoch 17 Total loss: 1539536.1944444445\n",
      "Epoch 18 Total loss: 1490126.986111111\n",
      "Epoch 19 Total loss: 1443010.027777778\n",
      "Epoch 20 Total loss: 1398757.4305555555\n",
      "Epoch 21 Total loss: 1357136.4444444443\n",
      "Epoch 22 Total loss: 1318104.361111111\n",
      "Epoch 23 Total loss: 1281728.6111111112\n",
      "Epoch 24 Total loss: 1247278.736111111\n",
      "Epoch 25 Total loss: 1214285.1666666665\n",
      "Epoch 26 Total loss: 1183061.0555555555\n",
      "Epoch 27 Total loss: 1154061.5972222222\n",
      "Epoch 28 Total loss: 1127483.8194444443\n",
      "Epoch 29 Total loss: 1101914.1666666667\n",
      "Epoch 30 Total loss: 1077495.1666666667\n",
      "Epoch 31 Total loss: 1054331.6736111112\n",
      "Epoch 32 Total loss: 1032054.0763888888\n",
      "Epoch 33 Total loss: 1010691.2361111112\n",
      "Epoch 34 Total loss: 990068.8819444445\n",
      "Epoch 35 Total loss: 970164.5208333333\n",
      "Epoch 36 Total loss: 951187.2569444444\n",
      "Epoch 37 Total loss: 933097.5902777778\n",
      "Epoch 38 Total loss: 915837.138888889\n",
      "Epoch 39 Total loss: 899338.6041666667\n",
      "Epoch 40 Total loss: 883476.888888889\n",
      "Epoch 41 Total loss: 868241.4374999999\n",
      "Epoch 42 Total loss: 853613.7916666666\n",
      "Epoch 43 Total loss: 839707.4375\n",
      "Epoch 44 Total loss: 826450.6319444445\n",
      "Epoch 45 Total loss: 813855.1666666667\n",
      "Epoch 46 Total loss: 801630.5555555556\n",
      "Epoch 47 Total loss: 789686.75\n",
      "Epoch 48 Total loss: 778121.8125000001\n",
      "Epoch 49 Total loss: 767079.923611111\n",
      "Epoch 50 Total loss: 756477.5625000001\n",
      "Epoch 51 Total loss: 746179.9791666666\n",
      "Epoch 52 Total loss: 736159.5902777779\n",
      "Epoch 53 Total loss: 726457.3125000001\n",
      "Epoch 54 Total loss: 717197.3125\n",
      "Epoch 55 Total loss: 708204.7152777779\n",
      "Epoch 56 Total loss: 699549.5486111111\n",
      "Epoch 57 Total loss: 691088.4166666666\n",
      "Epoch 58 Total loss: 682843.1597222222\n",
      "Epoch 59 Total loss: 674859.9236111111\n",
      "Epoch 60 Total loss: 667098.4375\n",
      "Epoch 61 Total loss: 659547.5763888889\n",
      "Epoch 62 Total loss: 652228.9652777779\n",
      "Epoch 63 Total loss: 645108.9722222222\n",
      "Epoch 64 Total loss: 638114.3888888889\n",
      "Epoch 65 Total loss: 631287.9513888889\n",
      "Epoch 66 Total loss: 624678.7500000001\n",
      "Epoch 67 Total loss: 618267.4722222222\n",
      "Epoch 68 Total loss: 612050.9166666667\n",
      "Epoch 69 Total loss: 606093.0138888889\n",
      "Epoch 70 Total loss: 600289.7291666667\n",
      "Epoch 71 Total loss: 594556.5\n",
      "Epoch 72 Total loss: 588914.2777777778\n",
      "Epoch 73 Total loss: 583394.3263888889\n",
      "Epoch 74 Total loss: 578004.2638888889\n",
      "Epoch 75 Total loss: 572737.3472222222\n",
      "Epoch 76 Total loss: 567610.6666666667\n",
      "Epoch 77 Total loss: 562599.9722222222\n",
      "Epoch 78 Total loss: 557725.5555555555\n",
      "Epoch 79 Total loss: 552934.576388889\n",
      "Epoch 80 Total loss: 548278.4652777778\n",
      "Epoch 81 Total loss: 543693.2986111111\n",
      "Epoch 82 Total loss: 539190.625\n",
      "Epoch 83 Total loss: 534788.625\n",
      "Epoch 84 Total loss: 530451.2430555555\n",
      "Epoch 85 Total loss: 526202.7430555555\n",
      "Epoch 86 Total loss: 522104.75347222213\n",
      "Epoch 87 Total loss: 518130.53819444444\n",
      "Epoch 88 Total loss: 514209.1076388889\n",
      "Epoch 89 Total loss: 510325.42013888893\n",
      "Epoch 90 Total loss: 506546.12152777775\n",
      "Epoch 91 Total loss: 502834.7916666666\n",
      "Epoch 92 Total loss: 499164.1979166666\n",
      "Epoch 93 Total loss: 495558.72569444444\n",
      "Epoch 94 Total loss: 492020.07291666657\n",
      "Epoch 95 Total loss: 488547.3645833334\n",
      "Epoch 96 Total loss: 485123.06944444444\n",
      "Epoch 97 Total loss: 481776.8958333333\n",
      "Epoch 98 Total loss: 478491.875\n",
      "Epoch 99 Total loss: 475251.94791666674\n",
      " efficientnet Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:07<00:00, 52.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 61.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2911092.388888889\n",
      "Epoch 1 Total loss: 2883826.7222222225\n",
      "Epoch 2 Total loss: 2782636.4722222225\n",
      "Epoch 3 Total loss: 2642597.0277777775\n",
      "Epoch 4 Total loss: 2504549.3055555555\n",
      "Epoch 5 Total loss: 2372569.1944444445\n",
      "Epoch 6 Total loss: 2245844.222222222\n",
      "Epoch 7 Total loss: 2121757.861111111\n",
      "Epoch 8 Total loss: 2005185.6666666665\n",
      "Epoch 9 Total loss: 1896105.611111111\n",
      "Epoch 10 Total loss: 1797391.6666666667\n",
      "Epoch 11 Total loss: 1710089.0416666665\n",
      "Epoch 12 Total loss: 1633626.5972222222\n",
      "Epoch 13 Total loss: 1564585.9166666667\n",
      "Epoch 14 Total loss: 1502588.5972222222\n",
      "Epoch 15 Total loss: 1445433.5416666665\n",
      "Epoch 16 Total loss: 1392201.4583333333\n",
      "Epoch 17 Total loss: 1343143.1527777778\n",
      "Epoch 18 Total loss: 1298205.652777778\n",
      "Epoch 19 Total loss: 1256928.0694444445\n",
      "Epoch 20 Total loss: 1218908.513888889\n",
      "Epoch 21 Total loss: 1182657.6944444445\n",
      "Epoch 22 Total loss: 1148274.4722222222\n",
      "Epoch 23 Total loss: 1116187.722222222\n",
      "Epoch 24 Total loss: 1086164.3472222222\n",
      "Epoch 25 Total loss: 1058000.4166666665\n",
      "Epoch 26 Total loss: 1031416.798611111\n",
      "Epoch 27 Total loss: 1006442.2222222221\n",
      "Epoch 28 Total loss: 982987.8402777778\n",
      "Epoch 29 Total loss: 960872.25\n",
      "Epoch 30 Total loss: 940044.7777777779\n",
      "Epoch 31 Total loss: 920199.6944444444\n",
      "Epoch 32 Total loss: 901241.0833333334\n",
      "Epoch 33 Total loss: 883213.2291666665\n",
      "Epoch 34 Total loss: 866034.5208333335\n",
      "Epoch 35 Total loss: 849782.0972222221\n",
      "Epoch 36 Total loss: 834269.0\n",
      "Epoch 37 Total loss: 819401.5555555556\n",
      "Epoch 38 Total loss: 805121.326388889\n",
      "Epoch 39 Total loss: 791370.5833333334\n",
      "Epoch 40 Total loss: 778161.9166666667\n",
      "Epoch 41 Total loss: 765503.5\n",
      "Epoch 42 Total loss: 753395.4791666666\n",
      "Epoch 43 Total loss: 741843.1180555555\n",
      "Epoch 44 Total loss: 730741.1805555555\n",
      "Epoch 45 Total loss: 720049.7916666667\n",
      "Epoch 46 Total loss: 709695.2569444444\n",
      "Epoch 47 Total loss: 699798.6944444445\n",
      "Epoch 48 Total loss: 690260.9166666667\n",
      "Epoch 49 Total loss: 681012.4583333333\n",
      "Epoch 50 Total loss: 672083.8819444445\n",
      "Epoch 51 Total loss: 663440.5763888889\n",
      "Epoch 52 Total loss: 655079.7847222222\n",
      "Epoch 53 Total loss: 646983.2777777776\n",
      "Epoch 54 Total loss: 639161.0833333334\n",
      "Epoch 55 Total loss: 631617.9791666667\n",
      "Epoch 56 Total loss: 624328.9722222222\n",
      "Epoch 57 Total loss: 617244.138888889\n",
      "Epoch 58 Total loss: 610346.1597222222\n",
      "Epoch 59 Total loss: 603659.2152777778\n",
      "Epoch 60 Total loss: 597149.6805555555\n",
      "Epoch 61 Total loss: 590806.7708333334\n",
      "Epoch 62 Total loss: 584644.7777777778\n",
      "Epoch 63 Total loss: 578665.5694444445\n",
      "Epoch 64 Total loss: 572864.9444444445\n",
      "Epoch 65 Total loss: 567180.2916666667\n",
      "Epoch 66 Total loss: 561652.6805555555\n",
      "Epoch 67 Total loss: 556257.1527777778\n",
      "Epoch 68 Total loss: 551005.7222222222\n",
      "Epoch 69 Total loss: 545869.5486111111\n",
      "Epoch 70 Total loss: 540840.701388889\n",
      "Epoch 71 Total loss: 535929.0486111111\n",
      "Epoch 72 Total loss: 531166.3333333333\n",
      "Epoch 73 Total loss: 526515.5208333333\n",
      "Epoch 74 Total loss: 521967.1979166666\n",
      "Epoch 75 Total loss: 517504.4027777778\n",
      "Epoch 76 Total loss: 513141.25\n",
      "Epoch 77 Total loss: 508896.13194444444\n",
      "Epoch 78 Total loss: 504749.2951388889\n",
      "Epoch 79 Total loss: 500709.30555555556\n",
      "Epoch 80 Total loss: 496758.47222222213\n",
      "Epoch 81 Total loss: 492873.7534722222\n",
      "Epoch 82 Total loss: 489055.7881944444\n",
      "Epoch 83 Total loss: 485336.8576388888\n",
      "Epoch 84 Total loss: 481690.8715277777\n",
      "Epoch 85 Total loss: 478123.2083333334\n",
      "Epoch 86 Total loss: 474605.9791666666\n",
      "Epoch 87 Total loss: 471155.6006944445\n",
      "Epoch 88 Total loss: 467764.0833333334\n",
      "Epoch 89 Total loss: 464446.7013888889\n",
      "Epoch 90 Total loss: 461184.97916666674\n",
      "Epoch 91 Total loss: 457989.88888888893\n",
      "Epoch 92 Total loss: 454856.7256944445\n",
      "Epoch 93 Total loss: 451801.0763888889\n",
      "Epoch 94 Total loss: 448800.54513888893\n",
      "Epoch 95 Total loss: 445844.57986111107\n",
      "Epoch 96 Total loss: 442952.4861111111\n",
      "Epoch 97 Total loss: 440091.6944444444\n",
      "Epoch 98 Total loss: 437275.37500000006\n",
      "Epoch 99 Total loss: 434514.9826388889\n",
      " inception Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:06<00:00, 57.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 66.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2900884.194444444\n",
      "Epoch 1 Total loss: 2724581.5\n",
      "Epoch 2 Total loss: 2664438.027777778\n",
      "Epoch 3 Total loss: 2581711.472222222\n",
      "Epoch 4 Total loss: 2476153.111111111\n",
      "Epoch 5 Total loss: 2358423.833333333\n",
      "Epoch 6 Total loss: 2239801.9444444445\n",
      "Epoch 7 Total loss: 2123851.6805555555\n",
      "Epoch 8 Total loss: 2011996.3888888892\n",
      "Epoch 9 Total loss: 1906332.5555555557\n",
      "Epoch 10 Total loss: 1808465.8194444445\n",
      "Epoch 11 Total loss: 1719791.6805555555\n",
      "Epoch 12 Total loss: 1640715.7916666667\n",
      "Epoch 13 Total loss: 1569707.138888889\n",
      "Epoch 14 Total loss: 1505413.486111111\n",
      "Epoch 15 Total loss: 1447182.0138888888\n",
      "Epoch 16 Total loss: 1394902.5\n",
      "Epoch 17 Total loss: 1349127.4722222222\n",
      "Epoch 18 Total loss: 1305897.013888889\n",
      "Epoch 19 Total loss: 1264384.1944444445\n",
      "Epoch 20 Total loss: 1225013.736111111\n",
      "Epoch 21 Total loss: 1187999.611111111\n",
      "Epoch 22 Total loss: 1153279.9722222222\n",
      "Epoch 23 Total loss: 1120659.7916666667\n",
      "Epoch 24 Total loss: 1090110.5\n",
      "Epoch 25 Total loss: 1061630.25\n",
      "Epoch 26 Total loss: 1035177.1111111111\n",
      "Epoch 27 Total loss: 1010551.5972222222\n",
      "Epoch 28 Total loss: 986853.5972222222\n",
      "Epoch 29 Total loss: 963980.5138888889\n",
      "Epoch 30 Total loss: 942240.8402777778\n",
      "Epoch 31 Total loss: 921671.6111111111\n",
      "Epoch 32 Total loss: 902130.7222222222\n",
      "Epoch 33 Total loss: 883555.4097222222\n",
      "Epoch 34 Total loss: 865833.0902777778\n",
      "Epoch 35 Total loss: 848956.875\n",
      "Epoch 36 Total loss: 832890.5555555556\n",
      "Epoch 37 Total loss: 817594.076388889\n",
      "Epoch 38 Total loss: 803081.9444444445\n",
      "Epoch 39 Total loss: 789216.861111111\n",
      "Epoch 40 Total loss: 775883.111111111\n",
      "Epoch 41 Total loss: 763112.7291666666\n",
      "Epoch 42 Total loss: 750899.0277777778\n",
      "Epoch 43 Total loss: 739130.6527777778\n",
      "Epoch 44 Total loss: 727796.0833333333\n",
      "Epoch 45 Total loss: 716982.2777777778\n",
      "Epoch 46 Total loss: 706593.3194444444\n",
      "Epoch 47 Total loss: 696621.1944444444\n",
      "Epoch 48 Total loss: 687015.1041666666\n",
      "Epoch 49 Total loss: 677648.7083333333\n",
      "Epoch 50 Total loss: 668518.2916666667\n",
      "Epoch 51 Total loss: 659686.4305555556\n",
      "Epoch 52 Total loss: 651119.6527777778\n",
      "Epoch 53 Total loss: 642828.4930555556\n",
      "Epoch 54 Total loss: 634806.0902777779\n",
      "Epoch 55 Total loss: 627076.8125\n",
      "Epoch 56 Total loss: 619660.8611111112\n",
      "Epoch 57 Total loss: 612493.2708333333\n",
      "Epoch 58 Total loss: 605538.611111111\n",
      "Epoch 59 Total loss: 598774.9513888889\n",
      "Epoch 60 Total loss: 592195.0833333335\n",
      "Epoch 61 Total loss: 585789.3402777778\n",
      "Epoch 62 Total loss: 579604.4027777778\n",
      "Epoch 63 Total loss: 573596.5416666666\n",
      "Epoch 64 Total loss: 567730.6666666666\n",
      "Epoch 65 Total loss: 561973.0347222222\n",
      "Epoch 66 Total loss: 556359.7638888889\n",
      "Epoch 67 Total loss: 550936.7777777778\n",
      "Epoch 68 Total loss: 545648.2430555555\n",
      "Epoch 69 Total loss: 540497.9791666666\n",
      "Epoch 70 Total loss: 535506.8819444445\n",
      "Epoch 71 Total loss: 530586.8888888889\n",
      "Epoch 72 Total loss: 525775.5798611111\n",
      "Epoch 73 Total loss: 521065.11805555556\n",
      "Epoch 74 Total loss: 516471.4791666667\n",
      "Epoch 75 Total loss: 511955.31249999994\n",
      "Epoch 76 Total loss: 507531.59375000006\n",
      "Epoch 77 Total loss: 503199.1180555555\n",
      "Epoch 78 Total loss: 498965.06249999994\n",
      "Epoch 79 Total loss: 494871.85069444444\n",
      "Epoch 80 Total loss: 490877.9861111111\n",
      "Epoch 81 Total loss: 486963.8645833334\n",
      "Epoch 82 Total loss: 483126.3541666666\n",
      "Epoch 83 Total loss: 479359.2013888889\n",
      "Epoch 84 Total loss: 475666.8576388888\n",
      "Epoch 85 Total loss: 472032.0416666667\n",
      "Epoch 86 Total loss: 468500.71875\n",
      "Epoch 87 Total loss: 465055.24305555556\n",
      "Epoch 88 Total loss: 461671.8645833333\n",
      "Epoch 89 Total loss: 458339.2847222222\n",
      "Epoch 90 Total loss: 455056.8541666666\n",
      "Epoch 91 Total loss: 451848.1319444445\n",
      "Epoch 92 Total loss: 448686.72569444444\n",
      "Epoch 93 Total loss: 445584.4548611111\n",
      "Epoch 94 Total loss: 442531.03472222225\n",
      "Epoch 95 Total loss: 439559.7395833333\n",
      "Epoch 96 Total loss: 436639.7604166667\n",
      "Epoch 97 Total loss: 433773.99999999994\n",
      "Epoch 98 Total loss: 430962.68750000006\n",
      "Epoch 99 Total loss: 428211.40625000006\n",
      " resnet Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:06<00:00, 58.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 63.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2865435.3055555555\n",
      "Epoch 1 Total loss: 2708445.0\n",
      "Epoch 2 Total loss: 2669652.6666666665\n",
      "Epoch 3 Total loss: 2593712.638888889\n",
      "Epoch 4 Total loss: 2484932.9166666665\n",
      "Epoch 5 Total loss: 2372940.0\n",
      "Epoch 6 Total loss: 2272894.5833333335\n",
      "Epoch 7 Total loss: 2183917.222222222\n",
      "Epoch 8 Total loss: 2100720.9583333335\n",
      "Epoch 9 Total loss: 2017666.6249999998\n",
      "Epoch 10 Total loss: 1934743.3194444443\n",
      "Epoch 11 Total loss: 1855231.7083333335\n",
      "Epoch 12 Total loss: 1779585.3333333335\n",
      "Epoch 13 Total loss: 1708178.75\n",
      "Epoch 14 Total loss: 1641368.4722222222\n",
      "Epoch 15 Total loss: 1579138.513888889\n",
      "Epoch 16 Total loss: 1521089.7916666667\n",
      "Epoch 17 Total loss: 1467215.888888889\n",
      "Epoch 18 Total loss: 1417333.4305555553\n",
      "Epoch 19 Total loss: 1371113.9861111112\n",
      "Epoch 20 Total loss: 1327775.416666667\n",
      "Epoch 21 Total loss: 1287641.013888889\n",
      "Epoch 22 Total loss: 1249816.0833333333\n",
      "Epoch 23 Total loss: 1214234.7222222222\n",
      "Epoch 24 Total loss: 1180719.7083333335\n",
      "Epoch 25 Total loss: 1149025.7222222222\n",
      "Epoch 26 Total loss: 1119115.3194444445\n",
      "Epoch 27 Total loss: 1090768.6805555555\n",
      "Epoch 28 Total loss: 1063803.1805555557\n",
      "Epoch 29 Total loss: 1038143.388888889\n",
      "Epoch 30 Total loss: 1013768.875\n",
      "Epoch 31 Total loss: 990615.8333333334\n",
      "Epoch 32 Total loss: 968606.7291666666\n",
      "Epoch 33 Total loss: 947731.6388888889\n",
      "Epoch 34 Total loss: 927843.6875\n",
      "Epoch 35 Total loss: 908975.7916666666\n",
      "Epoch 36 Total loss: 891023.6111111111\n",
      "Epoch 37 Total loss: 873936.888888889\n",
      "Epoch 38 Total loss: 857570.3333333334\n",
      "Epoch 39 Total loss: 841927.0277777779\n",
      "Epoch 40 Total loss: 826996.8680555555\n",
      "Epoch 41 Total loss: 812631.986111111\n",
      "Epoch 42 Total loss: 798835.513888889\n",
      "Epoch 43 Total loss: 785561.3402777779\n",
      "Epoch 44 Total loss: 772698.6736111111\n",
      "Epoch 45 Total loss: 760380.9930555555\n",
      "Epoch 46 Total loss: 748550.9930555555\n",
      "Epoch 47 Total loss: 737162.8541666666\n",
      "Epoch 48 Total loss: 726189.8125\n",
      "Epoch 49 Total loss: 715581.3402777778\n",
      "Epoch 50 Total loss: 705342.8402777778\n",
      "Epoch 51 Total loss: 695496.4513888889\n",
      "Epoch 52 Total loss: 685993.5972222224\n",
      "Epoch 53 Total loss: 676818.173611111\n",
      "Epoch 54 Total loss: 667922.0833333334\n",
      "Epoch 55 Total loss: 659297.9722222222\n",
      "Epoch 56 Total loss: 650922.9930555556\n",
      "Epoch 57 Total loss: 642758.9930555555\n",
      "Epoch 58 Total loss: 634840.7847222222\n",
      "Epoch 59 Total loss: 627133.2777777778\n",
      "Epoch 60 Total loss: 619669.9166666666\n",
      "Epoch 61 Total loss: 612456.5\n",
      "Epoch 62 Total loss: 605477.6319444445\n",
      "Epoch 63 Total loss: 598677.9722222222\n",
      "Epoch 64 Total loss: 592051.0\n",
      "Epoch 65 Total loss: 585609.8055555555\n",
      "Epoch 66 Total loss: 579358.3194444445\n",
      "Epoch 67 Total loss: 573264.4444444445\n",
      "Epoch 68 Total loss: 567303.6736111112\n",
      "Epoch 69 Total loss: 561522.8194444445\n",
      "Epoch 70 Total loss: 555910.423611111\n",
      "Epoch 71 Total loss: 550433.1458333334\n",
      "Epoch 72 Total loss: 545088.9097222222\n",
      "Epoch 73 Total loss: 539849.6319444445\n",
      "Epoch 74 Total loss: 534747.4930555556\n",
      "Epoch 75 Total loss: 529755.5972222222\n",
      "Epoch 76 Total loss: 524881.2777777778\n",
      "Epoch 77 Total loss: 520099.77777777775\n",
      "Epoch 78 Total loss: 515416.13888888893\n",
      "Epoch 79 Total loss: 510857.81250000006\n",
      "Epoch 80 Total loss: 506412.0937499999\n",
      "Epoch 81 Total loss: 502069.2638888888\n",
      "Epoch 82 Total loss: 497802.5520833334\n",
      "Epoch 83 Total loss: 493621.44097222213\n",
      "Epoch 84 Total loss: 489547.7673611112\n",
      "Epoch 85 Total loss: 485569.0138888889\n",
      "Epoch 86 Total loss: 481673.9166666666\n",
      "Epoch 87 Total loss: 477864.5798611111\n",
      "Epoch 88 Total loss: 474137.7638888889\n",
      "Epoch 89 Total loss: 470484.42013888893\n",
      "Epoch 90 Total loss: 466908.77430555556\n",
      "Epoch 91 Total loss: 463377.8090277778\n",
      "Epoch 92 Total loss: 459899.01736111107\n",
      "Epoch 93 Total loss: 456473.9166666667\n",
      "Epoch 94 Total loss: 453122.8333333333\n",
      "Epoch 95 Total loss: 449852.7291666666\n",
      "Epoch 96 Total loss: 446646.5833333333\n",
      "Epoch 97 Total loss: 443500.09722222213\n",
      "Epoch 98 Total loss: 440413.07986111107\n",
      "Epoch 99 Total loss: 437392.1840277778\n",
      " vgg Gen encoding... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [00:05<00:00, 67.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 69.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "for model_name in [ 'mobilenet', 'efficientnet','inception', 'resnet', 'vgg']:\n",
    "    train_dataset, test_dataset = load_data_for_model(model_name)\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(f\" {model_name} Gen encoding... \")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
