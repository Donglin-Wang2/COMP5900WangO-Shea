{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(f\"gpu: { len(tf.config.list_physical_devices('GPU')) }\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_adapted import VAEAdapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(model_name, batch_size=64):\n",
    "    assert model_name in ['efficientnet', 'inception', 'resnet', 'mobilenet', 'vgg']\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated_%s_feat.npy' % model_name)\n",
    "    imgs = np.load('./data/LFSD_imgs_%s_feat.npy' % model_name)\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        dataset.append((img_batch, depth_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'vgg': 512,\n",
    "        'efficientnet': 1280,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAEAdapted(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Training Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "\n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(f\"Epoch {i} Total loss: { losses_across_epochs['loss'][-1]}\")\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_adapted/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        np.save('./results/vae_adapted/%s_%s' % (model_name, k), np.array(v))\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_adapted/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_adapted/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "    from skimage.transform import resize\n",
    "    from tensorflow.keras import datasets\n",
    "\n",
    "    train_dataset, test_dataset = None, None\n",
    " \n",
    "    train_feats = np.load('./data/CIFAR100_%s_train_feat.npy' % model_name)\n",
    "    test_feats = np.load('./data/CIFAR100_%s_test_feat.npy' % model_name)\n",
    "\n",
    "    train_result, _, _ = vae.encode(train_feats[:128], tf.random.normal(train_feats[:128].shape))\n",
    "    for i in range(128, len(train_feats), 128):\n",
    "        activation, _, _ = vae.encode(train_feats[i:i+128], tf.random.normal(train_feats[i:i+128].shape))\n",
    "        train_result = tf.concat((train_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_adapted_%s_encoding_train.npy' % model_name, np.array(train_result))\n",
    "\n",
    "    test_result, _, _ = vae.encode(test_feats[:128], tf.random.normal(test_feats[:128].shape))\n",
    "    for i in range(128, len(test_feats), 128):\n",
    "        activation, _, _ = vae.encode(test_feats[i:i+128], tf.random.normal(test_feats[i:i+128].shape))\n",
    "        test_result = tf.concat((test_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_adapted_%s_encoding_test.npy' % model_name, np.array(test_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 9664419356.444445\n",
      "Epoch 1 Total loss: 2548685311.9999995\n",
      "Epoch 2 Total loss: 1520894919.1111112\n",
      "Epoch 3 Total loss: 1087149077.3333335\n",
      "Epoch 4 Total loss: 846926293.3333333\n",
      "Epoch 5 Total loss: 694060928.0\n",
      "Epoch 6 Total loss: 588156088.8888888\n",
      "Epoch 7 Total loss: 510426140.4444445\n",
      "Epoch 8 Total loss: 450935960.8888889\n",
      "Epoch 9 Total loss: 403933340.4444444\n",
      "Epoch 10 Total loss: 365854933.3333334\n",
      "Epoch 11 Total loss: 334374193.77777773\n",
      "Epoch 12 Total loss: 307908064.0\n",
      "Epoch 13 Total loss: 285343264.0\n",
      "Epoch 14 Total loss: 265872622.2222222\n",
      "Epoch 15 Total loss: 248898551.11111107\n",
      "Epoch 16 Total loss: 233970714.66666663\n",
      "Epoch 17 Total loss: 220740881.77777776\n",
      "Epoch 18 Total loss: 208935900.44444442\n",
      "Epoch 19 Total loss: 198337692.44444442\n",
      "Epoch 20 Total loss: 188770540.44444445\n",
      "Epoch 21 Total loss: 180090620.44444445\n",
      "Epoch 22 Total loss: 172180289.77777776\n",
      "Epoch 23 Total loss: 164941601.7777778\n",
      "Epoch 24 Total loss: 158292593.7777778\n",
      "Epoch 25 Total loss: 152163804.44444445\n",
      "Epoch 26 Total loss: 146496624.0\n",
      "Epoch 27 Total loss: 141240832.0\n",
      "Epoch 28 Total loss: 136353141.33333334\n",
      "Epoch 29 Total loss: 131796085.33333333\n",
      "Epoch 30 Total loss: 127537310.22222222\n",
      "Epoch 31 Total loss: 123548408.8888889\n",
      "Epoch 32 Total loss: 119804594.66666667\n",
      "Epoch 33 Total loss: 116283865.77777779\n",
      "Epoch 34 Total loss: 112966927.99999999\n",
      "Epoch 35 Total loss: 109836689.77777778\n",
      "Epoch 36 Total loss: 106877619.55555555\n",
      "Epoch 37 Total loss: 104076051.55555555\n",
      "Epoch 38 Total loss: 101419824.88888888\n",
      "Epoch 39 Total loss: 98897917.33333333\n",
      "Epoch 40 Total loss: 96500336.8888889\n",
      "Epoch 41 Total loss: 94218176.88888888\n",
      "Epoch 42 Total loss: 92043294.22222222\n",
      "Epoch 43 Total loss: 89968345.77777779\n",
      "Epoch 44 Total loss: 87986376.00000001\n",
      "Epoch 45 Total loss: 86091447.1111111\n",
      "Epoch 46 Total loss: 84278015.11111112\n",
      "Epoch 47 Total loss: 82540714.66666667\n",
      "Epoch 48 Total loss: 80875016.88888888\n",
      "Epoch 49 Total loss: 79276514.66666666\n",
      "Epoch 50 Total loss: 77741257.77777778\n",
      "Epoch 51 Total loss: 76265644.44444445\n",
      "Epoch 52 Total loss: 74846042.66666667\n",
      "Epoch 53 Total loss: 73479523.55555557\n",
      "Epoch 54 Total loss: 72163113.77777778\n",
      "Epoch 55 Total loss: 70894100.44444445\n",
      "Epoch 56 Total loss: 69669887.11111112\n",
      "Epoch 57 Total loss: 68488254.22222221\n",
      "Epoch 58 Total loss: 67346971.55555555\n",
      "Epoch 59 Total loss: 66243935.99999999\n",
      "Epoch 60 Total loss: 65177363.111111104\n",
      "Epoch 61 Total loss: 64145421.33333333\n",
      "Epoch 62 Total loss: 63146508.44444445\n",
      "Epoch 63 Total loss: 62179026.66666667\n",
      "Epoch 64 Total loss: 61241500.44444444\n",
      "Epoch 65 Total loss: 60332666.66666667\n",
      "Epoch 66 Total loss: 59451036.888888896\n",
      "Epoch 67 Total loss: 58595535.11111112\n",
      "Epoch 68 Total loss: 57764981.777777776\n",
      "Epoch 69 Total loss: 56958294.222222224\n",
      "Epoch 70 Total loss: 56174444.44444443\n",
      "Epoch 71 Total loss: 55412489.777777776\n",
      "Epoch 72 Total loss: 54671569.33333333\n",
      "Epoch 73 Total loss: 53950795.55555555\n",
      "Epoch 74 Total loss: 53249364.44444445\n",
      "Epoch 75 Total loss: 52566491.11111111\n",
      "Epoch 76 Total loss: 51901462.666666664\n",
      "Epoch 77 Total loss: 51253588.44444445\n",
      "Epoch 78 Total loss: 50622186.22222222\n",
      "Epoch 79 Total loss: 50006680.00000001\n",
      "Epoch 80 Total loss: 49406432.44444445\n",
      "Epoch 81 Total loss: 48820938.66666667\n",
      "Epoch 82 Total loss: 48249596.0\n",
      "Epoch 83 Total loss: 47691924.888888896\n",
      "Epoch 84 Total loss: 47147455.11111111\n",
      "Epoch 85 Total loss: 46615716.888888896\n",
      "Epoch 86 Total loss: 46096257.33333334\n",
      "Epoch 87 Total loss: 45588677.33333333\n",
      "Epoch 88 Total loss: 45092519.55555556\n",
      "Epoch 89 Total loss: 44607410.222222224\n",
      "Epoch 90 Total loss: 44133002.66666667\n",
      "Epoch 91 Total loss: 43668982.666666664\n",
      "Epoch 92 Total loss: 43214976.44444444\n",
      "Epoch 93 Total loss: 42770702.666666664\n",
      "Epoch 94 Total loss: 42335793.777777776\n",
      "Epoch 95 Total loss: 41909988.0\n",
      "Epoch 96 Total loss: 41493000.0\n",
      "Epoch 97 Total loss: 41084556.44444445\n",
      "Epoch 98 Total loss: 40684430.66666667\n",
      "Epoch 99 Total loss: 40292368.88888889\n",
      "Gen encoding...\n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 3984171.111111111\n",
      "Epoch 1 Total loss: 3197635.833333333\n",
      "Epoch 2 Total loss: 3041447.027777778\n",
      "Epoch 3 Total loss: 2953345.833333333\n",
      "Epoch 4 Total loss: 2835450.472222222\n",
      "Epoch 5 Total loss: 2672014.8611111115\n",
      "Epoch 6 Total loss: 2481780.3333333335\n",
      "Epoch 7 Total loss: 2302106.472222222\n",
      "Epoch 8 Total loss: 2148136.9722222225\n",
      "Epoch 9 Total loss: 2009669.375\n",
      "Epoch 10 Total loss: 1890997.3333333335\n",
      "Epoch 11 Total loss: 1783896.347222222\n",
      "Epoch 12 Total loss: 1689384.1111111112\n",
      "Epoch 13 Total loss: 1607402.333333333\n",
      "Epoch 14 Total loss: 1533589.625\n",
      "Epoch 15 Total loss: 1466266.3333333333\n",
      "Epoch 16 Total loss: 1406410.4444444445\n",
      "Epoch 17 Total loss: 1352312.6666666665\n",
      "Epoch 18 Total loss: 1303335.6805555555\n",
      "Epoch 19 Total loss: 1257747.9027777778\n",
      "Epoch 20 Total loss: 1215850.4166666667\n",
      "Epoch 21 Total loss: 1176985.5555555555\n",
      "Epoch 22 Total loss: 1140825.6388888888\n",
      "Epoch 23 Total loss: 1106832.7638888888\n",
      "Epoch 24 Total loss: 1075190.5972222222\n",
      "Epoch 25 Total loss: 1045787.4722222222\n",
      "Epoch 26 Total loss: 1018436.2222222222\n",
      "Epoch 27 Total loss: 993466.7569444445\n",
      "Epoch 28 Total loss: 970310.6458333333\n",
      "Epoch 29 Total loss: 948003.2916666666\n",
      "Epoch 30 Total loss: 927088.9791666666\n",
      "Epoch 31 Total loss: 907216.3958333333\n",
      "Epoch 32 Total loss: 888587.5277777778\n",
      "Epoch 33 Total loss: 870856.7986111111\n",
      "Epoch 34 Total loss: 854126.0833333334\n",
      "Epoch 35 Total loss: 837859.0347222222\n",
      "Epoch 36 Total loss: 822234.2777777778\n",
      "Epoch 37 Total loss: 807326.4652777778\n",
      "Epoch 38 Total loss: 793159.1597222222\n",
      "Epoch 39 Total loss: 779654.9027777778\n",
      "Epoch 40 Total loss: 766791.2083333333\n",
      "Epoch 41 Total loss: 754582.548611111\n",
      "Epoch 42 Total loss: 742880.0555555556\n",
      "Epoch 43 Total loss: 731667.3125\n",
      "Epoch 44 Total loss: 720924.7222222222\n",
      "Epoch 45 Total loss: 710647.9513888889\n",
      "Epoch 46 Total loss: 700778.25\n",
      "Epoch 47 Total loss: 691213.4375\n",
      "Epoch 48 Total loss: 681918.2638888889\n",
      "Epoch 49 Total loss: 672844.4444444445\n",
      "Epoch 50 Total loss: 664056.1180555555\n",
      "Epoch 51 Total loss: 655593.2708333334\n",
      "Epoch 52 Total loss: 647448.6180555555\n",
      "Epoch 53 Total loss: 639542.8541666666\n",
      "Epoch 54 Total loss: 632036.4166666666\n",
      "Epoch 55 Total loss: 624972.9930555555\n",
      "Epoch 56 Total loss: 618145.8263888889\n",
      "Epoch 57 Total loss: 611438.0347222222\n",
      "Epoch 58 Total loss: 604857.6319444444\n",
      "Epoch 59 Total loss: 598432.076388889\n",
      "Epoch 60 Total loss: 592209.1666666667\n",
      "Epoch 61 Total loss: 586153.125\n",
      "Epoch 62 Total loss: 580178.6944444445\n",
      "Epoch 63 Total loss: 574328.5625\n",
      "Epoch 64 Total loss: 568649.5694444445\n",
      "Epoch 65 Total loss: 563123.0833333334\n",
      "Epoch 66 Total loss: 557722.2847222222\n",
      "Epoch 67 Total loss: 552451.9444444445\n",
      "Epoch 68 Total loss: 547372.0208333334\n",
      "Epoch 69 Total loss: 542463.0694444444\n",
      "Epoch 70 Total loss: 537717.2638888889\n",
      "Epoch 71 Total loss: 533022.1388888889\n",
      "Epoch 72 Total loss: 528416.7847222222\n",
      "Epoch 73 Total loss: 523884.86458333326\n",
      "Epoch 74 Total loss: 519460.37152777775\n",
      "Epoch 75 Total loss: 515112.22222222225\n",
      "Epoch 76 Total loss: 510866.8298611111\n",
      "Epoch 77 Total loss: 506694.9270833334\n",
      "Epoch 78 Total loss: 502631.8715277778\n",
      "Epoch 79 Total loss: 498662.6354166667\n",
      "Epoch 80 Total loss: 494828.7534722222\n",
      "Epoch 81 Total loss: 491084.46180555556\n",
      "Epoch 82 Total loss: 487447.54861111107\n",
      "Epoch 83 Total loss: 483877.3784722222\n",
      "Epoch 84 Total loss: 480334.5868055555\n",
      "Epoch 85 Total loss: 476842.1805555556\n",
      "Epoch 86 Total loss: 473412.8263888889\n",
      "Epoch 87 Total loss: 470053.9861111111\n",
      "Epoch 88 Total loss: 466762.9479166667\n",
      "Epoch 89 Total loss: 463639.1354166667\n",
      "Epoch 90 Total loss: 460691.2048611111\n",
      "Epoch 91 Total loss: 457742.36458333326\n",
      "Epoch 92 Total loss: 454814.8541666667\n",
      "Epoch 93 Total loss: 451962.9791666667\n",
      "Epoch 94 Total loss: 449045.9305555556\n",
      "Epoch 95 Total loss: 446138.94097222225\n",
      "Epoch 96 Total loss: 443274.1944444444\n",
      "Epoch 97 Total loss: 440467.1076388889\n",
      "Epoch 98 Total loss: 437705.03472222225\n",
      "Epoch 99 Total loss: 434981.5555555555\n",
      "Gen encoding...\n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 5349915.472222222\n",
      "Epoch 1 Total loss: 3719435.111111112\n",
      "Epoch 2 Total loss: 3412686.638888889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Total loss: 3197148.583333333\n",
      "Epoch 4 Total loss: 2972662.055555556\n",
      "Epoch 5 Total loss: 2746482.972222223\n",
      "Epoch 6 Total loss: 2529458.5555555555\n",
      "Epoch 7 Total loss: 2336879.9166666665\n",
      "Epoch 8 Total loss: 2172561.8055555555\n",
      "Epoch 9 Total loss: 2031269.541666667\n",
      "Epoch 10 Total loss: 1908737.8749999998\n",
      "Epoch 11 Total loss: 1800606.1527777775\n",
      "Epoch 12 Total loss: 1705552.7083333335\n",
      "Epoch 13 Total loss: 1621303.3055555557\n",
      "Epoch 14 Total loss: 1545741.902777778\n",
      "Epoch 15 Total loss: 1477585.847222222\n",
      "Epoch 16 Total loss: 1415447.5277777778\n",
      "Epoch 17 Total loss: 1359146.0833333333\n",
      "Epoch 18 Total loss: 1308511.0694444445\n",
      "Epoch 19 Total loss: 1263105.722222222\n",
      "Epoch 20 Total loss: 1221251.25\n",
      "Epoch 21 Total loss: 1182369.0833333333\n",
      "Epoch 22 Total loss: 1146372.375\n",
      "Epoch 23 Total loss: 1113312.0416666667\n",
      "Epoch 24 Total loss: 1082745.513888889\n",
      "Epoch 25 Total loss: 1054734.034722222\n",
      "Epoch 26 Total loss: 1028664.75\n",
      "Epoch 27 Total loss: 1003783.8402777778\n",
      "Epoch 28 Total loss: 979933.9027777778\n",
      "Epoch 29 Total loss: 957478.9305555556\n",
      "Epoch 30 Total loss: 936453.9375\n",
      "Epoch 31 Total loss: 916678.0902777778\n",
      "Epoch 32 Total loss: 897935.6597222222\n",
      "Epoch 33 Total loss: 880076.2361111111\n",
      "Epoch 34 Total loss: 862908.8055555555\n",
      "Epoch 35 Total loss: 846541.4166666666\n",
      "Epoch 36 Total loss: 830934.1111111112\n",
      "Epoch 37 Total loss: 816185.326388889\n",
      "Epoch 38 Total loss: 802329.9722222222\n",
      "Epoch 39 Total loss: 789267.5277777779\n",
      "Epoch 40 Total loss: 776677.8472222222\n",
      "Epoch 41 Total loss: 764389.8472222222\n",
      "Epoch 42 Total loss: 752551.9166666665\n",
      "Epoch 43 Total loss: 741152.4236111112\n",
      "Epoch 44 Total loss: 730288.8472222222\n",
      "Epoch 45 Total loss: 719879.1805555556\n",
      "Epoch 46 Total loss: 709883.3819444443\n",
      "Epoch 47 Total loss: 700152.7847222221\n",
      "Epoch 48 Total loss: 690766.5555555555\n",
      "Epoch 49 Total loss: 681762.5000000001\n",
      "Epoch 50 Total loss: 673103.0624999999\n",
      "Epoch 51 Total loss: 664826.2291666666\n",
      "Epoch 52 Total loss: 656879.7777777778\n",
      "Epoch 53 Total loss: 649177.9375\n",
      "Epoch 54 Total loss: 641623.0902777779\n",
      "Epoch 55 Total loss: 634248.1527777778\n",
      "Epoch 56 Total loss: 627071.3819444445\n",
      "Epoch 57 Total loss: 620059.2430555555\n",
      "Epoch 58 Total loss: 613237.9166666666\n",
      "Epoch 59 Total loss: 606653.8402777779\n",
      "Epoch 60 Total loss: 600366.4583333333\n",
      "Epoch 61 Total loss: 594323.4097222221\n",
      "Epoch 62 Total loss: 588437.4791666667\n",
      "Epoch 63 Total loss: 582665.5\n",
      "Epoch 64 Total loss: 576992.3819444444\n",
      "Epoch 65 Total loss: 571429.2152777778\n",
      "Epoch 66 Total loss: 565972.3402777779\n",
      "Epoch 67 Total loss: 560671.2569444445\n",
      "Epoch 68 Total loss: 555528.3333333334\n",
      "Epoch 69 Total loss: 550498.4791666666\n",
      "Epoch 70 Total loss: 545595.3958333334\n",
      "Epoch 71 Total loss: 540811.1875\n",
      "Epoch 72 Total loss: 536121.5625\n",
      "Epoch 73 Total loss: 531546.6319444445\n",
      "Epoch 74 Total loss: 527099.3611111111\n",
      "Epoch 75 Total loss: 522738.125\n",
      "Epoch 76 Total loss: 518443.34722222225\n",
      "Epoch 77 Total loss: 514240.51388888893\n",
      "Epoch 78 Total loss: 510138.7847222223\n",
      "Epoch 79 Total loss: 506148.28472222225\n",
      "Epoch 80 Total loss: 502266.7673611111\n",
      "Epoch 81 Total loss: 498487.7465277778\n",
      "Epoch 82 Total loss: 494739.0833333334\n",
      "Epoch 83 Total loss: 491061.39583333326\n",
      "Epoch 84 Total loss: 487437.81597222225\n",
      "Epoch 85 Total loss: 483881.47569444444\n",
      "Epoch 86 Total loss: 480395.5208333334\n",
      "Epoch 87 Total loss: 476946.875\n",
      "Epoch 88 Total loss: 473562.31944444444\n",
      "Epoch 89 Total loss: 470236.7708333333\n",
      "Epoch 90 Total loss: 466967.4270833334\n",
      "Epoch 91 Total loss: 463777.9652777778\n",
      "Epoch 92 Total loss: 460669.6388888889\n",
      "Epoch 93 Total loss: 457610.77430555556\n",
      "Epoch 94 Total loss: 454601.1319444445\n",
      "Epoch 95 Total loss: 451656.4965277778\n",
      "Epoch 96 Total loss: 448766.7118055555\n",
      "Epoch 97 Total loss: 445933.75347222225\n",
      "Epoch 98 Total loss: 443143.0451388889\n",
      "Epoch 99 Total loss: 440393.3645833334\n",
      "Gen encoding...\n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 3249981.888888889\n",
      "Epoch 1 Total loss: 3015584.9722222225\n",
      "Epoch 2 Total loss: 2910647.5\n",
      "Epoch 3 Total loss: 2758496.7777777775\n",
      "Epoch 4 Total loss: 2614487.472222222\n",
      "Epoch 5 Total loss: 2498712.3611111115\n",
      "Epoch 6 Total loss: 2407662.055555555\n",
      "Epoch 7 Total loss: 2335599.638888889\n",
      "Epoch 8 Total loss: 2277723.027777778\n",
      "Epoch 9 Total loss: 2230530.916666667\n",
      "Epoch 10 Total loss: 2190732.4444444445\n",
      "Epoch 11 Total loss: 2156452.6944444445\n",
      "Epoch 12 Total loss: 2126747.7222222225\n",
      "Epoch 13 Total loss: 2100724.611111111\n",
      "Epoch 14 Total loss: 2077732.0416666665\n",
      "Epoch 15 Total loss: 2057324.861111111\n",
      "Epoch 16 Total loss: 2039156.486111111\n",
      "Epoch 17 Total loss: 2022824.4166666667\n",
      "Epoch 18 Total loss: 2008112.9999999998\n",
      "Epoch 19 Total loss: 1994588.7777777778\n",
      "Epoch 20 Total loss: 1982387.25\n",
      "Epoch 21 Total loss: 1970768.0833333335\n",
      "Epoch 22 Total loss: 1959968.8888888885\n",
      "Epoch 23 Total loss: 1950472.0416666663\n",
      "Epoch 24 Total loss: 1941406.9999999998\n",
      "Epoch 25 Total loss: 1932421.4305555555\n",
      "Epoch 26 Total loss: 1923488.236111111\n",
      "Epoch 27 Total loss: 1915876.7499999998\n",
      "Epoch 28 Total loss: 1909047.8888888885\n",
      "Epoch 29 Total loss: 1902193.3055555555\n",
      "Epoch 30 Total loss: 1894773.6805555557\n",
      "Epoch 31 Total loss: 1887515.8194444445\n",
      "Epoch 32 Total loss: 1881289.736111111\n",
      "Epoch 33 Total loss: 1875243.5694444445\n",
      "Epoch 34 Total loss: 1868247.597222222\n",
      "Epoch 35 Total loss: 1861371.0416666667\n",
      "Epoch 36 Total loss: 1854800.291666667\n",
      "Epoch 37 Total loss: 1848382.7638888888\n",
      "Epoch 38 Total loss: 1842976.8611111112\n",
      "Epoch 39 Total loss: 1837246.7638888888\n",
      "Epoch 40 Total loss: 1831652.7222222222\n",
      "Epoch 41 Total loss: 1825979.3888888892\n",
      "Epoch 42 Total loss: 1819938.125\n",
      "Epoch 43 Total loss: 1814362.9583333333\n",
      "Epoch 44 Total loss: 1809014.7916666665\n",
      "Epoch 45 Total loss: 1803522.4444444443\n",
      "Epoch 46 Total loss: 1797793.9444444443\n",
      "Epoch 47 Total loss: 1792492.388888889\n",
      "Epoch 48 Total loss: 1786991.9027777778\n",
      "Epoch 49 Total loss: 1781040.8194444445\n",
      "Epoch 50 Total loss: 1776089.736111111\n",
      "Epoch 51 Total loss: 1770850.597222222\n",
      "Epoch 52 Total loss: 1766346.1944444445\n",
      "Epoch 53 Total loss: 1761397.6805555555\n",
      "Epoch 54 Total loss: 1757106.1666666667\n",
      "Epoch 55 Total loss: 1752857.5555555555\n",
      "Epoch 56 Total loss: 1748664.819444444\n",
      "Epoch 57 Total loss: 1743801.6805555555\n",
      "Epoch 58 Total loss: 1738769.25\n",
      "Epoch 59 Total loss: 1733691.3472222222\n",
      "Epoch 60 Total loss: 1729401.486111111\n",
      "Epoch 61 Total loss: 1724231.986111111\n",
      "Epoch 62 Total loss: 1718953.7083333333\n",
      "Epoch 63 Total loss: 1716484.8611111108\n",
      "Epoch 64 Total loss: 1712787.611111111\n",
      "Epoch 65 Total loss: 1708598.3888888892\n",
      "Epoch 66 Total loss: 1703884.625\n",
      "Epoch 67 Total loss: 1699802.9444444445\n",
      "Epoch 68 Total loss: 1696475.2638888888\n",
      "Epoch 69 Total loss: 1692703.2083333335\n",
      "Epoch 70 Total loss: 1688123.7222222218\n",
      "Epoch 71 Total loss: 1683487.333333333\n",
      "Epoch 72 Total loss: 1679987.0694444445\n",
      "Epoch 73 Total loss: 1676677.5833333333\n",
      "Epoch 74 Total loss: 1672353.9444444445\n",
      "Epoch 75 Total loss: 1667662.5416666665\n",
      "Epoch 76 Total loss: 1663454.4444444445\n",
      "Epoch 77 Total loss: 1661157.611111111\n",
      "Epoch 78 Total loss: 1657834.236111111\n",
      "Epoch 79 Total loss: 1653876.4583333335\n",
      "Epoch 80 Total loss: 1650349.541666667\n",
      "Epoch 81 Total loss: 1646603.9166666667\n",
      "Epoch 82 Total loss: 1642660.5694444445\n",
      "Epoch 83 Total loss: 1638885.291666667\n",
      "Epoch 84 Total loss: 1634533.333333333\n",
      "Epoch 85 Total loss: 1630553.236111111\n",
      "Epoch 86 Total loss: 1626808.9166666665\n",
      "Epoch 87 Total loss: 1623538.972222222\n",
      "Epoch 88 Total loss: 1619742.4444444443\n",
      "Epoch 89 Total loss: 1615756.347222222\n",
      "Epoch 90 Total loss: 1612685.3333333333\n",
      "Epoch 91 Total loss: 1609859.4027777778\n",
      "Epoch 92 Total loss: 1606678.361111111\n",
      "Epoch 93 Total loss: 1602868.5972222222\n",
      "Epoch 94 Total loss: 1599274.847222222\n",
      "Epoch 95 Total loss: 1595757.638888889\n",
      "Epoch 96 Total loss: 1592526.3194444445\n",
      "Epoch 97 Total loss: 1589392.3888888888\n",
      "Epoch 98 Total loss: 1585522.861111111\n",
      "Epoch 99 Total loss: 1582081.1944444445\n",
      "Gen encoding...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5b1ec1de1ad6>\u001b[0m in \u001b[0;36mload_data_for_model\u001b[1;34m(model_name, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data_for_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'efficientnet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inception'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'resnet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mobilenet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/LFSD_labels.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdepths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/LFSD_depths_repeated_%s_feat.npy'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/LFSD_imgs_%s_feat.npy'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "for model_name in ['efficientnet','inception', 'resnet', 'mobilenet', 'vgg']:\n",
    "    train_dataset, test_dataset = load_data_for_model(model_name)\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(\"Gen encoding...\")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
