{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"gpu: { len(tf.config.list_physical_devices('GPU')) }\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vae_adapted import VAEAdapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_model(model_name, batch_size=64):\n",
    "    labels = np.load('./data/LFSD_labels.npy')\n",
    "    depths = np.load('./data/LFSD_depths_repeated_%s_feat.npy' % model_name)\n",
    "    imgs = np.load('./data/LFSD_imgs_%s_feat.npy' % model_name)\n",
    "    masks = np.load('./data/LFSD_masks_single.npy')\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    batch_idxs = [idx[i:i + batch_size] for i in range(0, len(labels), batch_size)]\n",
    "    dataset = []\n",
    "    for idx in batch_idxs:\n",
    "        img_batch, depth_batch, mask_batch = imgs[idx], depths[idx], masks[idx]\n",
    "        dataset.append((img_batch, depth_batch, mask_batch))\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.3)\n",
    "    print(\"Train dataset contains %d batches of %d samples each\" % (len(train_dataset), batch_size))\n",
    "    print(\"Test dataset contains %d batches of %d samples each\" % (len(test_dataset), batch_size))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(image_batch, size):\n",
    "    h,w = image_batch.shape[1], image_batch.shape[2]\n",
    "    img = np.zeros((int(h*size[0]), w*size[1]))\n",
    "    for idx, im in enumerate(image_batch):\n",
    "        im = np.squeeze(im, axis=2)\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(train_dataset, test_dataset, learning_rate, model_name, epochs):\n",
    "    latent_lookup = {\n",
    "        'inception': 2048,\n",
    "        'vgg': 512,\n",
    "        'efficientnet': 1280,\n",
    "        'mobilenet': 1280,\n",
    "        'resnet': 2048,\n",
    "    }\n",
    "    latent_dim = latent_lookup[model_name]\n",
    "    vae = VAEAdapted(latent_dim)\n",
    "    vae.compile(optimizer=Adam(learning_rate))\n",
    "    epochs = epochs\n",
    "    # Training Step\n",
    "    losses_across_epochs = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "    }\n",
    "    batch_num = len(train_dataset)\n",
    "    for i in range(epochs):\n",
    "\n",
    "        for k, v in losses_across_epochs.items():\n",
    "            losses_across_epochs[k].append(0)\n",
    "        for data in train_dataset:\n",
    "            cur_loss = vae.train_step(data)\n",
    "            for k, v in cur_loss.items():\n",
    "                losses_across_epochs[k][-1] += cur_loss[k].numpy() / batch_num\n",
    "            generated_image = vae.sample(data)\n",
    "        print(f\"Epoch {i} Total loss: { losses_across_epochs['loss'][-1]}\")\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_adapted/%d.png' % i, im_merged, cmap='gray')\n",
    "    for k, v in losses_across_epochs.items():\n",
    "        np.save('./results/vae_adapted/%s_%s' % (model_name, k), np.array(v))\n",
    "    # Testing Step\n",
    "    test_loss = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        _, _, mask_batch = data\n",
    "        generated_image = vae.sample(data)\n",
    "        reconstruction_loss = tf.reduce_sum(\n",
    "            tf.keras.losses.binary_crossentropy(mask_batch, generated_image), [1,2]\n",
    "        )\n",
    "        test_loss += tf.reduce_mean(reconstruction_loss).numpy()\n",
    "        im_merged = merge_images(generated_image.numpy(), [8,8])\n",
    "        plt.imsave('./images/vae_adapted/test_batch_%d.png' % i, im_merged, cmap='gray')\n",
    "        \n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    np.save('./results/vae_adapted/%s_test_loss' % model_name, np.array([test_loss]))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_for_model(vae, model_name):\n",
    "    from skimage.transform import resize\n",
    "    from tensorflow.keras import datasets\n",
    "\n",
    "    train_dataset, test_dataset = None, None\n",
    " \n",
    "    train_feats = np.load('./data/CIFAR100_%s_train_feat.npy' % model_name)\n",
    "    test_feats = np.load('./data/CIFAR100_%s_test_feat.npy' % model_name)\n",
    "\n",
    "    train_result, _, _ = vae.encode(train_feats[:128], tf.random.normal(train_feats[:128].shape))\n",
    "    for i in range(128, len(train_feats), 128):\n",
    "        activation, _, _ = vae.encode(train_feats[i:i+128], tf.random.normal(train_feats[i:i+128].shape))\n",
    "        train_result = tf.concat((train_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_adapted_%s_encoding_train.npy' % model_name, np.array(train_result))\n",
    "\n",
    "    test_result, _, _ = vae.encode(test_feats[:128], tf.random.normal(test_feats[:128].shape))\n",
    "    for i in range(128, len(test_feats), 128):\n",
    "        activation, _, _ = vae.encode(test_feats[i:i+128], tf.random.normal(test_feats[i:i+128].shape))\n",
    "        test_result = tf.concat((test_result, activation), axis=0)\n",
    "    np.save('./data/CIFAR100_vae_adapted_%s_encoding_test.npy' % model_name, np.array(test_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2633721.861111111\n",
      "Epoch 1 Total loss: 2633065.944444445\n",
      "Epoch 2 Total loss: 2609566.1666666665\n",
      "Epoch 3 Total loss: 2525977.916666667\n",
      "Epoch 4 Total loss: 2416643.388888889\n",
      "Epoch 5 Total loss: 2312822.8333333335\n",
      "Epoch 6 Total loss: 2225860.583333333\n",
      "Epoch 7 Total loss: 2155152.0555555555\n",
      "Epoch 8 Total loss: 2097084.5833333335\n",
      "Epoch 9 Total loss: 2048095.6944444445\n",
      "Epoch 10 Total loss: 2006120.4444444445\n",
      "Epoch 11 Total loss: 1970055.4027777778\n",
      "Epoch 12 Total loss: 1938567.7222222225\n",
      "Epoch 13 Total loss: 1910380.125\n",
      "Epoch 14 Total loss: 1885724.4166666667\n",
      "Epoch 15 Total loss: 1863045.0416666665\n",
      "Epoch 16 Total loss: 1840966.9722222222\n",
      "Epoch 17 Total loss: 1820218.9444444445\n",
      "Epoch 18 Total loss: 1798372.3888888888\n",
      "Epoch 19 Total loss: 1777743.0694444445\n",
      "Epoch 20 Total loss: 1757126.3333333333\n",
      "Epoch 21 Total loss: 1735857.4166666665\n",
      "Epoch 22 Total loss: 1720367.125\n",
      "Epoch 23 Total loss: 1703003.333333333\n",
      "Epoch 24 Total loss: 1684390.708333333\n",
      "Epoch 25 Total loss: 1666602.9166666665\n",
      "Epoch 26 Total loss: 1648721.2638888888\n",
      "Epoch 27 Total loss: 1629741.402777778\n",
      "Epoch 28 Total loss: 1612389.638888889\n",
      "Epoch 29 Total loss: 1598626.5694444445\n",
      "Epoch 30 Total loss: 1584493.5277777778\n",
      "Epoch 31 Total loss: 1567848.986111111\n",
      "Epoch 32 Total loss: 1550966.125\n",
      "Epoch 33 Total loss: 1535590.138888889\n",
      "Epoch 34 Total loss: 1520936.4305555555\n",
      "Epoch 35 Total loss: 1505496.3055555555\n",
      "Epoch 36 Total loss: 1490372.4027777778\n",
      "Epoch 37 Total loss: 1475491.0694444445\n",
      "Epoch 38 Total loss: 1461161.9166666667\n",
      "Epoch 39 Total loss: 1447557.388888889\n",
      "Epoch 40 Total loss: 1435573.4166666667\n",
      "Epoch 41 Total loss: 1423210.555555556\n",
      "Epoch 42 Total loss: 1410734.4305555553\n",
      "Epoch 43 Total loss: 1399050.1805555553\n",
      "Epoch 44 Total loss: 1390002.027777778\n",
      "Epoch 45 Total loss: 1380784.0833333335\n",
      "Epoch 46 Total loss: 1370239.0416666667\n",
      "Epoch 47 Total loss: 1358598.5\n",
      "Epoch 48 Total loss: 1347144.0277777778\n",
      "Epoch 49 Total loss: 1335300.4722222222\n",
      "Epoch 50 Total loss: 1324192.0972222222\n",
      "Epoch 51 Total loss: 1313887.0555555555\n",
      "Epoch 52 Total loss: 1303797.861111111\n",
      "Epoch 53 Total loss: 1295202.888888889\n",
      "Epoch 54 Total loss: 1286731.0555555557\n",
      "Epoch 55 Total loss: 1277743.652777778\n",
      "Epoch 56 Total loss: 1268154.0416666665\n",
      "Epoch 57 Total loss: 1258474.9722222222\n",
      "Epoch 58 Total loss: 1248833.2361111108\n",
      "Epoch 59 Total loss: 1239266.944444444\n",
      "Epoch 60 Total loss: 1229748.0000000002\n",
      "Epoch 61 Total loss: 1220755.847222222\n",
      "Epoch 62 Total loss: 1212230.2916666665\n",
      "Epoch 63 Total loss: 1204464.9305555555\n",
      "Epoch 64 Total loss: 1197427.2499999998\n",
      "Epoch 65 Total loss: 1190717.638888889\n",
      "Epoch 66 Total loss: 1184649.5416666667\n",
      "Epoch 67 Total loss: 1178792.6944444445\n",
      "Epoch 68 Total loss: 1172874.5277777778\n",
      "Epoch 69 Total loss: 1166154.4583333333\n",
      "Epoch 70 Total loss: 1160284.6944444443\n",
      "Epoch 71 Total loss: 1155112.9583333333\n",
      "Epoch 72 Total loss: 1149238.5\n",
      "Epoch 73 Total loss: 1142415.9444444445\n",
      "Epoch 74 Total loss: 1135374.0555555555\n",
      "Epoch 75 Total loss: 1128420.5277777778\n",
      "Epoch 76 Total loss: 1122335.2916666665\n",
      "Epoch 77 Total loss: 1116516.75\n",
      "Epoch 78 Total loss: 1110130.0694444445\n",
      "Epoch 79 Total loss: 1103582.4027777778\n",
      "Epoch 80 Total loss: 1097370.0555555555\n",
      "Epoch 81 Total loss: 1091805.1388888888\n",
      "Epoch 82 Total loss: 1086674.138888889\n",
      "Epoch 83 Total loss: 1081434.1527777778\n",
      "Epoch 84 Total loss: 1075604.0555555555\n",
      "Epoch 85 Total loss: 1069625.486111111\n",
      "Epoch 86 Total loss: 1063917.6944444445\n",
      "Epoch 87 Total loss: 1058036.9305555555\n",
      "Epoch 88 Total loss: 1051970.9583333333\n",
      "Epoch 89 Total loss: 1046216.5763888889\n",
      "Epoch 90 Total loss: 1041655.8680555557\n",
      "Epoch 91 Total loss: 1037274.0208333334\n",
      "Epoch 92 Total loss: 1032176.7083333335\n",
      "Epoch 93 Total loss: 1026944.5763888891\n",
      "Epoch 94 Total loss: 1022011.0069444445\n",
      "Epoch 95 Total loss: 1017090.2569444445\n",
      "Epoch 96 Total loss: 1012030.8611111114\n",
      "Epoch 97 Total loss: 1006916.3472222222\n",
      "Epoch 98 Total loss: 1001616.625\n",
      "Epoch 99 Total loss: 996403.5277777776\n",
      " mobilenet Gen encoding... \n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 2798722.1805555555\n",
      "Epoch 1 Total loss: 2790273.722222222\n",
      "Epoch 2 Total loss: 2719081.6944444445\n",
      "Epoch 3 Total loss: 2621831.4166666665\n",
      "Epoch 4 Total loss: 2508595.944444444\n",
      "Epoch 5 Total loss: 2401605.1666666665\n",
      "Epoch 6 Total loss: 2309456.3055555555\n",
      "Epoch 7 Total loss: 2232685.9722222225\n",
      "Epoch 8 Total loss: 2169145.0555555555\n",
      "Epoch 9 Total loss: 2116366.9444444445\n",
      "Epoch 10 Total loss: 2072285.388888889\n",
      "Epoch 11 Total loss: 2034951.9722222225\n",
      "Epoch 12 Total loss: 2002998.333333333\n",
      "Epoch 13 Total loss: 1975237.861111111\n",
      "Epoch 14 Total loss: 1951187.3888888888\n",
      "Epoch 15 Total loss: 1930041.222222222\n",
      "Epoch 16 Total loss: 1911157.0416666667\n",
      "Epoch 17 Total loss: 1894269.8055555555\n",
      "Epoch 18 Total loss: 1879029.736111111\n",
      "Epoch 19 Total loss: 1865220.6250000002\n",
      "Epoch 20 Total loss: 1852720.1527777778\n",
      "Epoch 21 Total loss: 1841242.625\n",
      "Epoch 22 Total loss: 1830794.736111111\n",
      "Epoch 23 Total loss: 1821215.513888889\n",
      "Epoch 24 Total loss: 1812430.6388888888\n",
      "Epoch 25 Total loss: 1804281.8055555555\n",
      "Epoch 26 Total loss: 1796729.6527777778\n",
      "Epoch 27 Total loss: 1789723.875\n",
      "Epoch 28 Total loss: 1783050.8750000002\n",
      "Epoch 29 Total loss: 1776830.0416666665\n",
      "Epoch 30 Total loss: 1771047.8333333335\n",
      "Epoch 31 Total loss: 1765494.9999999998\n",
      "Epoch 32 Total loss: 1760309.111111111\n",
      "Epoch 33 Total loss: 1755534.8055555557\n",
      "Epoch 34 Total loss: 1750967.9722222222\n",
      "Epoch 35 Total loss: 1746713.0694444443\n",
      "Epoch 36 Total loss: 1742697.902777778\n",
      "Epoch 37 Total loss: 1738785.8611111112\n",
      "Epoch 38 Total loss: 1735063.8194444445\n",
      "Epoch 39 Total loss: 1731473.6666666663\n",
      "Epoch 40 Total loss: 1728084.8055555555\n",
      "Epoch 41 Total loss: 1724891.0416666667\n",
      "Epoch 42 Total loss: 1721786.0416666667\n",
      "Epoch 43 Total loss: 1718822.1944444443\n",
      "Epoch 44 Total loss: 1716029.4583333333\n",
      "Epoch 45 Total loss: 1713410.2500000002\n",
      "Epoch 46 Total loss: 1710838.5833333333\n",
      "Epoch 47 Total loss: 1708325.0138888892\n",
      "Epoch 48 Total loss: 1705974.0277777775\n",
      "Epoch 49 Total loss: 1703735.6666666667\n",
      "Epoch 50 Total loss: 1701483.2916666667\n",
      "Epoch 51 Total loss: 1699336.2916666665\n",
      "Epoch 52 Total loss: 1697337.3194444443\n",
      "Epoch 53 Total loss: 1695394.2222222222\n",
      "Epoch 54 Total loss: 1693504.763888889\n",
      "Epoch 55 Total loss: 1691652.75\n",
      "Epoch 56 Total loss: 1689870.0694444445\n",
      "Epoch 57 Total loss: 1688168.4305555557\n",
      "Epoch 58 Total loss: 1686555.8472222222\n",
      "Epoch 59 Total loss: 1684971.9166666665\n",
      "Epoch 60 Total loss: 1683405.847222222\n",
      "Epoch 61 Total loss: 1681890.763888889\n",
      "Epoch 62 Total loss: 1680474.9444444445\n",
      "Epoch 63 Total loss: 1679091.8194444445\n",
      "Epoch 64 Total loss: 1677741.4166666665\n",
      "Epoch 65 Total loss: 1676478.763888889\n",
      "Epoch 66 Total loss: 1675248.4027777778\n",
      "Epoch 67 Total loss: 1674057.9861111112\n",
      "Epoch 68 Total loss: 1672867.3611111112\n",
      "Epoch 69 Total loss: 1671714.5277777778\n",
      "Epoch 70 Total loss: 1670581.8055555555\n",
      "Epoch 71 Total loss: 1669474.9027777775\n",
      "Epoch 72 Total loss: 1668390.5694444447\n",
      "Epoch 73 Total loss: 1667337.125\n",
      "Epoch 74 Total loss: 1666318.236111111\n",
      "Epoch 75 Total loss: 1665379.111111111\n",
      "Epoch 76 Total loss: 1664436.2361111108\n",
      "Epoch 77 Total loss: 1663528.5694444445\n",
      "Epoch 78 Total loss: 1662631.236111111\n",
      "Epoch 79 Total loss: 1661717.5972222222\n",
      "Epoch 80 Total loss: 1660815.0694444445\n",
      "Epoch 81 Total loss: 1659987.6111111112\n",
      "Epoch 82 Total loss: 1659138.9444444447\n",
      "Epoch 83 Total loss: 1658348.333333333\n",
      "Epoch 84 Total loss: 1657578.8888888888\n",
      "Epoch 85 Total loss: 1656788.8055555557\n",
      "Epoch 86 Total loss: 1656009.1944444443\n",
      "Epoch 87 Total loss: 1655233.1805555557\n",
      "Epoch 88 Total loss: 1654481.5416666665\n",
      "Epoch 89 Total loss: 1653720.0\n",
      "Epoch 90 Total loss: 1652984.0555555555\n",
      "Epoch 91 Total loss: 1652247.2777777778\n",
      "Epoch 92 Total loss: 1651577.2500000002\n",
      "Epoch 93 Total loss: 1650928.3888888888\n",
      "Epoch 94 Total loss: 1650280.4166666667\n",
      "Epoch 95 Total loss: 1649650.3194444443\n",
      "Epoch 96 Total loss: 1648999.763888889\n",
      "Epoch 97 Total loss: 1648339.8611111112\n",
      "Epoch 98 Total loss: 1647708.097222222\n",
      "Epoch 99 Total loss: 1647124.152777778\n",
      " efficientnet Gen encoding... \n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Total loss: 2920473.083333333\n",
      "Epoch 1 Total loss: 2905832.5833333335\n",
      "Epoch 2 Total loss: 2860749.861111112\n",
      "Epoch 3 Total loss: 2758303.888888889\n",
      "Epoch 4 Total loss: 2635412.861111111\n",
      "Epoch 5 Total loss: 2522940.583333334\n",
      "Epoch 6 Total loss: 2430036.722222222\n",
      "Epoch 7 Total loss: 2354925.333333333\n",
      "Epoch 8 Total loss: 2293664.277777778\n",
      "Epoch 9 Total loss: 2243189.555555556\n",
      "Epoch 10 Total loss: 2201238.8333333335\n",
      "Epoch 11 Total loss: 2165644.8055555555\n",
      "Epoch 12 Total loss: 2135420.0555555555\n",
      "Epoch 13 Total loss: 2109193.611111111\n",
      "Epoch 14 Total loss: 2086165.6666666665\n",
      "Epoch 15 Total loss: 2065987.8333333335\n",
      "Epoch 16 Total loss: 2048170.1805555555\n",
      "Epoch 17 Total loss: 2032212.6666666665\n",
      "Epoch 18 Total loss: 2017924.9722222222\n",
      "Epoch 19 Total loss: 2004957.9305555553\n",
      "Epoch 20 Total loss: 1993137.8611111112\n",
      "Epoch 21 Total loss: 1982449.4861111115\n",
      "Epoch 22 Total loss: 1972570.6805555555\n",
      "Epoch 23 Total loss: 1963318.125\n",
      "Epoch 24 Total loss: 1954890.972222222\n",
      "Epoch 25 Total loss: 1947140.0833333335\n",
      "Epoch 26 Total loss: 1939927.736111111\n",
      "Epoch 27 Total loss: 1933201.0833333335\n",
      "Epoch 28 Total loss: 1926905.9583333333\n",
      "Epoch 29 Total loss: 1921046.7083333333\n",
      "Epoch 30 Total loss: 1915528.9166666667\n",
      "Epoch 31 Total loss: 1910263.861111111\n",
      "Epoch 32 Total loss: 1905387.5694444443\n",
      "Epoch 33 Total loss: 1900760.0416666665\n",
      "Epoch 34 Total loss: 1896489.3194444445\n",
      "Epoch 35 Total loss: 1892333.3055555555\n",
      "Epoch 36 Total loss: 1888413.0555555557\n",
      "Epoch 37 Total loss: 1884705.736111111\n",
      "Epoch 38 Total loss: 1881188.7916666667\n",
      "Epoch 39 Total loss: 1877840.4305555553\n",
      "Epoch 40 Total loss: 1874675.8194444445\n",
      "Epoch 41 Total loss: 1871708.7499999998\n",
      "Epoch 42 Total loss: 1868879.7222222225\n",
      "Epoch 43 Total loss: 1866119.8194444443\n",
      "Epoch 44 Total loss: 1863410.4027777775\n",
      "Epoch 45 Total loss: 1860846.3194444443\n",
      "Epoch 46 Total loss: 1858411.4722222222\n",
      "Epoch 47 Total loss: 1856001.4305555553\n",
      "Epoch 48 Total loss: 1853711.5972222222\n",
      "Epoch 49 Total loss: 1851564.486111111\n",
      "Epoch 50 Total loss: 1849566.1111111108\n",
      "Epoch 51 Total loss: 1847590.0416666665\n",
      "Epoch 52 Total loss: 1845699.3055555555\n",
      "Epoch 53 Total loss: 1843884.75\n",
      "Epoch 54 Total loss: 1842106.8194444445\n",
      "Epoch 55 Total loss: 1840399.6944444445\n",
      "Epoch 56 Total loss: 1838769.8749999998\n",
      "Epoch 57 Total loss: 1837198.9027777778\n",
      "Epoch 58 Total loss: 1835668.152777778\n",
      "Epoch 59 Total loss: 1834125.7916666667\n",
      "Epoch 60 Total loss: 1832680.5833333335\n",
      "Epoch 61 Total loss: 1831300.013888889\n",
      "Epoch 62 Total loss: 1829966.5277777778\n",
      "Epoch 63 Total loss: 1828619.2083333333\n",
      "Epoch 64 Total loss: 1827317.3333333333\n",
      "Epoch 65 Total loss: 1826150.7916666665\n",
      "Epoch 66 Total loss: 1824913.3333333335\n",
      "Epoch 67 Total loss: 1823729.625\n",
      "Epoch 68 Total loss: 1822598.111111111\n",
      "Epoch 69 Total loss: 1821483.0694444445\n",
      "Epoch 70 Total loss: 1820372.4444444443\n",
      "Epoch 71 Total loss: 1819334.402777778\n",
      "Epoch 72 Total loss: 1818279.111111111\n",
      "Epoch 73 Total loss: 1817293.722222222\n",
      "Epoch 74 Total loss: 1816314.7222222222\n",
      "Epoch 75 Total loss: 1815362.666666667\n",
      "Epoch 76 Total loss: 1814428.9305555557\n",
      "Epoch 77 Total loss: 1813500.3333333333\n",
      "Epoch 78 Total loss: 1812593.8472222225\n",
      "Epoch 79 Total loss: 1811695.986111111\n",
      "Epoch 80 Total loss: 1810841.3194444445\n",
      "Epoch 81 Total loss: 1810002.0833333335\n",
      "Epoch 82 Total loss: 1809225.402777778\n",
      "Epoch 83 Total loss: 1808455.8611111115\n",
      "Epoch 84 Total loss: 1807683.4444444445\n",
      "Epoch 85 Total loss: 1806964.5833333335\n",
      "Epoch 86 Total loss: 1806282.2361111112\n",
      "Epoch 87 Total loss: 1805607.2083333335\n",
      "Epoch 88 Total loss: 1804907.625\n",
      "Epoch 89 Total loss: 1804226.652777778\n",
      "Epoch 90 Total loss: 1803539.6944444445\n",
      "Epoch 91 Total loss: 1802893.5555555555\n",
      "Epoch 92 Total loss: 1802260.0\n",
      "Epoch 93 Total loss: 1801661.625\n",
      "Epoch 94 Total loss: 1801065.5\n",
      "Epoch 95 Total loss: 1800473.138888889\n",
      "Epoch 96 Total loss: 1799882.1527777778\n",
      "Epoch 97 Total loss: 1799289.1944444443\n",
      "Epoch 98 Total loss: 1798709.3055555555\n",
      "Epoch 99 Total loss: 1798135.097222222\n",
      " inception Gen encoding... \n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 3012875.1944444445\n",
      "Epoch 1 Total loss: 2935684.0277777775\n",
      "Epoch 2 Total loss: 2872695.777777778\n",
      "Epoch 3 Total loss: 2759520.0\n",
      "Epoch 4 Total loss: 2642022.361111111\n",
      "Epoch 5 Total loss: 2539021.5555555555\n",
      "Epoch 6 Total loss: 2452929.5555555555\n",
      "Epoch 7 Total loss: 2381896.1111111115\n",
      "Epoch 8 Total loss: 2322904.277777778\n",
      "Epoch 9 Total loss: 2274140.9722222225\n",
      "Epoch 10 Total loss: 2233471.222222222\n",
      "Epoch 11 Total loss: 2199244.0277777775\n",
      "Epoch 12 Total loss: 2169841.9444444445\n",
      "Epoch 13 Total loss: 2144026.416666667\n",
      "Epoch 14 Total loss: 2121240.5\n",
      "Epoch 15 Total loss: 2100811.791666667\n",
      "Epoch 16 Total loss: 2082479.097222222\n",
      "Epoch 17 Total loss: 2066048.388888889\n",
      "Epoch 18 Total loss: 2051146.4444444445\n",
      "Epoch 19 Total loss: 2037543.3194444447\n",
      "Epoch 20 Total loss: 2025088.0555555557\n",
      "Epoch 21 Total loss: 2013833.0833333333\n",
      "Epoch 22 Total loss: 2003607.5555555553\n",
      "Epoch 23 Total loss: 1994231.3611111112\n",
      "Epoch 24 Total loss: 1985424.3333333335\n",
      "Epoch 25 Total loss: 1977304.8194444443\n",
      "Epoch 26 Total loss: 1969821.3472222222\n",
      "Epoch 27 Total loss: 1962905.2083333335\n",
      "Epoch 28 Total loss: 1956447.277777778\n",
      "Epoch 29 Total loss: 1950438.7222222222\n",
      "Epoch 30 Total loss: 1944841.152777778\n",
      "Epoch 31 Total loss: 1939489.930555556\n",
      "Epoch 32 Total loss: 1934358.277777778\n",
      "Epoch 33 Total loss: 1929704.25\n",
      "Epoch 34 Total loss: 1925270.7777777778\n",
      "Epoch 35 Total loss: 1921127.7222222222\n",
      "Epoch 36 Total loss: 1917123.763888889\n",
      "Epoch 37 Total loss: 1913287.9583333335\n",
      "Epoch 38 Total loss: 1909679.8055555557\n",
      "Epoch 39 Total loss: 1906286.736111111\n",
      "Epoch 40 Total loss: 1903025.513888889\n",
      "Epoch 41 Total loss: 1899928.6666666667\n",
      "Epoch 42 Total loss: 1896909.5277777782\n",
      "Epoch 43 Total loss: 1894038.0277777778\n",
      "Epoch 44 Total loss: 1891284.9722222225\n",
      "Epoch 45 Total loss: 1888646.6527777775\n",
      "Epoch 46 Total loss: 1886165.1388888888\n",
      "Epoch 47 Total loss: 1883740.4166666667\n",
      "Epoch 48 Total loss: 1881478.8888888888\n",
      "Epoch 49 Total loss: 1879246.0694444445\n",
      "Epoch 50 Total loss: 1877100.013888889\n",
      "Epoch 51 Total loss: 1874995.416666667\n",
      "Epoch 52 Total loss: 1873056.763888889\n",
      "Epoch 53 Total loss: 1871170.9027777778\n",
      "Epoch 54 Total loss: 1869302.3611111112\n",
      "Epoch 55 Total loss: 1867510.7777777778\n",
      "Epoch 56 Total loss: 1865744.8611111112\n",
      "Epoch 57 Total loss: 1864102.7222222225\n",
      "Epoch 58 Total loss: 1862535.375\n",
      "Epoch 59 Total loss: 1860959.4861111112\n",
      "Epoch 60 Total loss: 1859397.6805555557\n",
      "Epoch 61 Total loss: 1857916.9166666665\n",
      "Epoch 62 Total loss: 1856571.486111111\n",
      "Epoch 63 Total loss: 1855178.9027777778\n",
      "Epoch 64 Total loss: 1853839.5833333335\n",
      "Epoch 65 Total loss: 1852482.8194444445\n",
      "Epoch 66 Total loss: 1851296.375\n",
      "Epoch 67 Total loss: 1850058.0416666667\n",
      "Epoch 68 Total loss: 1848915.4305555557\n",
      "Epoch 69 Total loss: 1847785.6805555557\n",
      "Epoch 70 Total loss: 1846639.1805555555\n",
      "Epoch 71 Total loss: 1845541.7916666665\n",
      "Epoch 72 Total loss: 1844509.3055555557\n",
      "Epoch 73 Total loss: 1843494.138888889\n",
      "Epoch 74 Total loss: 1842482.4583333333\n",
      "Epoch 75 Total loss: 1841461.5833333337\n",
      "Epoch 76 Total loss: 1840465.958333333\n",
      "Epoch 77 Total loss: 1839547.5277777778\n",
      "Epoch 78 Total loss: 1838657.777777778\n",
      "Epoch 79 Total loss: 1837724.888888889\n",
      "Epoch 80 Total loss: 1836839.5138888888\n",
      "Epoch 81 Total loss: 1835992.3888888885\n",
      "Epoch 82 Total loss: 1835175.7083333335\n",
      "Epoch 83 Total loss: 1834407.9305555555\n",
      "Epoch 84 Total loss: 1833609.111111111\n",
      "Epoch 85 Total loss: 1832849.3194444445\n",
      "Epoch 86 Total loss: 1832082.5694444445\n",
      "Epoch 87 Total loss: 1831315.9444444447\n",
      "Epoch 88 Total loss: 1830608.5694444445\n",
      "Epoch 89 Total loss: 1829888.916666667\n",
      "Epoch 90 Total loss: 1829202.3333333333\n",
      "Epoch 91 Total loss: 1828532.8472222222\n",
      "Epoch 92 Total loss: 1827870.9583333335\n",
      "Epoch 93 Total loss: 1827239.958333333\n",
      "Epoch 94 Total loss: 1826589.861111111\n",
      "Epoch 95 Total loss: 1825907.75\n",
      "Epoch 96 Total loss: 1825290.763888889\n",
      "Epoch 97 Total loss: 1824684.0972222222\n",
      "Epoch 98 Total loss: 1824095.3055555555\n",
      "Epoch 99 Total loss: 1823506.6805555555\n",
      " resnet Gen encoding... \n",
      "Train dataset contains 9 batches of 64 samples each\n",
      "Test dataset contains 5 batches of 64 samples each\n",
      "Epoch 0 Total loss: 122649505791.99998\n",
      "Epoch 1 Total loss: 31465126798.22222\n",
      "Epoch 2 Total loss: 18773590357.333332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Total loss: 13423144618.666666\n",
      "Epoch 4 Total loss: 10456446520.88889\n",
      "Epoch 5 Total loss: 8567547278.222221\n",
      "Epoch 6 Total loss: 7258502371.555556\n",
      "Epoch 7 Total loss: 6297508750.222222\n",
      "Epoch 8 Total loss: 5561894798.222222\n",
      "Epoch 9 Total loss: 4980616305.777778\n",
      "Epoch 10 Total loss: 4509669148.444445\n",
      "Epoch 11 Total loss: 4120333937.7777786\n",
      "Epoch 12 Total loss: 3793067719.1111116\n",
      "Epoch 13 Total loss: 3514107050.666667\n",
      "Epoch 14 Total loss: 3273479196.444444\n",
      "Epoch 15 Total loss: 3063779953.7777777\n",
      "Epoch 16 Total loss: 2879399367.1111116\n",
      "Epoch 17 Total loss: 2716010467.5555553\n",
      "Epoch 18 Total loss: 2570218097.7777777\n",
      "Epoch 19 Total loss: 2439323989.333333\n",
      "Epoch 20 Total loss: 2321153934.2222223\n",
      "Epoch 21 Total loss: 2213937066.666667\n",
      "Epoch 22 Total loss: 2116216803.5555556\n",
      "Epoch 23 Total loss: 2026783843.5555553\n",
      "Epoch 24 Total loss: 1944625720.8888888\n",
      "Epoch 25 Total loss: 1868889230.2222223\n",
      "Epoch 26 Total loss: 1798849294.2222223\n",
      "Epoch 27 Total loss: 1733885824.0000002\n",
      "Epoch 28 Total loss: 1673465628.4444447\n",
      "Epoch 29 Total loss: 1617127893.3333333\n",
      "Epoch 30 Total loss: 1564471921.7777777\n",
      "Epoch 31 Total loss: 1515148032.0\n",
      "Epoch 32 Total loss: 1468849322.666667\n",
      "Epoch 33 Total loss: 1425305287.1111112\n",
      "Epoch 34 Total loss: 1384277233.777778\n",
      "Epoch 35 Total loss: 1345552768.0\n",
      "Epoch 36 Total loss: 1308943217.777778\n",
      "Epoch 37 Total loss: 1274279793.7777777\n",
      "Epoch 38 Total loss: 1241411000.888889\n",
      "Epoch 39 Total loss: 1210200547.5555556\n",
      "Epoch 40 Total loss: 1180526165.3333335\n",
      "Epoch 41 Total loss: 1152277205.3333333\n",
      "Epoch 42 Total loss: 1125353315.5555556\n",
      "Epoch 43 Total loss: 1099663132.4444447\n",
      "Epoch 44 Total loss: 1075123889.7777777\n",
      "Epoch 45 Total loss: 1051659811.5555555\n",
      "Epoch 46 Total loss: 1029201664.0\n",
      "Epoch 47 Total loss: 1007685859.5555556\n",
      "Epoch 48 Total loss: 987054428.4444445\n",
      "Epoch 49 Total loss: 967253852.4444444\n",
      "Epoch 50 Total loss: 948235079.111111\n",
      "Epoch 51 Total loss: 929952355.5555556\n",
      "Epoch 52 Total loss: 912363960.888889\n",
      "Epoch 53 Total loss: 895431032.8888888\n",
      "Epoch 54 Total loss: 879117546.6666666\n",
      "Epoch 55 Total loss: 863390037.3333335\n",
      "Epoch 56 Total loss: 848217528.8888888\n",
      "Epoch 57 Total loss: 833571114.6666666\n",
      "Epoch 58 Total loss: 819423893.3333334\n",
      "Epoch 59 Total loss: 805750684.4444445\n",
      "Epoch 60 Total loss: 792528078.2222223\n",
      "Epoch 61 Total loss: 779734080.0\n",
      "Epoch 62 Total loss: 767348060.4444445\n",
      "Epoch 63 Total loss: 755350926.2222223\n",
      "Epoch 64 Total loss: 743724714.6666666\n",
      "Epoch 65 Total loss: 732452430.2222221\n",
      "Epoch 66 Total loss: 721518030.2222222\n",
      "Epoch 67 Total loss: 710906716.4444444\n",
      "Epoch 68 Total loss: 700604202.6666667\n",
      "Epoch 69 Total loss: 690597248.0\n",
      "Epoch 70 Total loss: 680873287.111111\n",
      "Epoch 71 Total loss: 671420536.888889\n",
      "Epoch 72 Total loss: 662227761.7777777\n",
      "Epoch 73 Total loss: 653284302.2222223\n",
      "Epoch 74 Total loss: 644580245.3333334\n",
      "Epoch 75 Total loss: 636106104.888889\n",
      "Epoch 76 Total loss: 627852807.1111109\n",
      "Epoch 77 Total loss: 619811847.1111112\n",
      "Epoch 78 Total loss: 611975203.5555556\n",
      "Epoch 79 Total loss: 604335104.0\n",
      "Epoch 80 Total loss: 596884245.3333334\n",
      "Epoch 81 Total loss: 589615701.3333334\n",
      "Epoch 82 Total loss: 582522887.1111112\n",
      "Epoch 83 Total loss: 575599466.6666666\n",
      "Epoch 84 Total loss: 568839345.7777778\n",
      "Epoch 85 Total loss: 562236842.6666666\n",
      "Epoch 86 Total loss: 555786531.5555556\n",
      "Epoch 87 Total loss: 549483256.8888888\n",
      "Epoch 88 Total loss: 543321998.2222222\n",
      "Epoch 89 Total loss: 537297994.6666666\n",
      "Epoch 90 Total loss: 531406769.7777778\n",
      "Epoch 91 Total loss: 525643854.22222227\n",
      "Epoch 92 Total loss: 520005212.44444436\n",
      "Epoch 93 Total loss: 514486848.0\n",
      "Epoch 94 Total loss: 509084970.6666667\n",
      "Epoch 95 Total loss: 503795857.77777773\n",
      "Epoch 96 Total loss: 498616078.2222222\n",
      "Epoch 97 Total loss: 493542243.5555556\n",
      "Epoch 98 Total loss: 488571125.3333333\n",
      "Epoch 99 Total loss: 483699616.0\n",
      " vgg Gen encoding... \n",
      "Wall time: 11min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "for model_name in [ 'mobilenet', 'efficientnet','inception', 'resnet', 'vgg']:\n",
    "    train_dataset, test_dataset = load_data_for_model(model_name)\n",
    "    trained_model = train_round(train_dataset, test_dataset, learning_rate, model_name, epochs)\n",
    "    print(f\" {model_name} Gen encoding... \")\n",
    "    get_encoding_for_model(trained_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
