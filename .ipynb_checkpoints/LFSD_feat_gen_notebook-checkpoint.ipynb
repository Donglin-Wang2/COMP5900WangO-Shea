{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import InceptionV3, VGG16, MobileNetV2, ResNet101, EfficientNetB1\n",
    "from tqdm import tqdm\n",
    "print(f\"gpu: { len(tf.config.list_physical_devices('GPU')) }\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feat_with_model(model_name, imgs, depths):\n",
    "     with tf.device('/cpu:0'):\n",
    "        if model_name == 'inception':\n",
    "            depths = tf.keras.applications.inception_v3.preprocess_input(depths)\n",
    "            imgs = tf.keras.applications.inception_v3.preprocess_input(imgs)\n",
    "            # Min input size for inceptionnet is 75\n",
    "            model = InceptionV3(\n",
    "                include_top=False,\n",
    "                weights=\"imagenet\",\n",
    "                input_shape=(256, 256, 3),\n",
    "                pooling='max'\n",
    "            )\n",
    "        elif model_name == 'vgg':\n",
    "            with tf.device('/cpu:0'):\n",
    "                depths = tf.keras.applications.vgg16.preprocess_input(depths)\n",
    "                imgs = tf.keras.applications.vgg16.preprocess_input(imgs)\n",
    "            model = VGG16(\n",
    "                include_top=False,\n",
    "                weights=\"imagenet\",\n",
    "                input_shape=(256, 256, 3),\n",
    "                pooling='max'\n",
    "            )\n",
    "        elif model_name == 'mobilenet':\n",
    "            depths = tf.keras.applications.mobilenet_v2.preprocess_input(depths)\n",
    "            imgs = tf.keras.applications.mobilenet_v2.preprocess_input(imgs)\n",
    "            \n",
    "            depths = tf.image.resize(depths, (224, 224))\n",
    "            imgs = tf.image.resize(imgs, (224, 224))\n",
    "            \n",
    "            model = MobileNetV2(\n",
    "                include_top=False,\n",
    "                weights=\"imagenet\",\n",
    "                input_shape=(256, 256, 3),\n",
    "                pooling='max'\n",
    "            )\n",
    "        elif model_name == 'resnet':\n",
    "            depths = tf.keras.applications.resnet.preprocess_input(depths)\n",
    "            imgs = tf.keras.applications.resnet.preprocess_input(imgs)\n",
    "            model = ResNet101(\n",
    "                include_top=False,\n",
    "                weights=\"imagenet\",\n",
    "                input_shape=(224, 224, 3),\n",
    "                pooling='max'\n",
    "            )\n",
    "        elif model_name == 'efficientnet':\n",
    "            depths = tf.keras.applications.efficientnet.preprocess_input(depths)\n",
    "            imgs = tf.keras.applications.efficientnet.preprocess_input(imgs)\n",
    "            model = EfficientNetB1(\n",
    "                include_top=False,\n",
    "                weights=\"imagenet\",\n",
    "                input_shape=(256, 256, 3),\n",
    "                pooling='max'\n",
    "            )\n",
    "        else:\n",
    "            print('Invalid model name')\n",
    "            return\n",
    "\n",
    "        model.trainable = False\n",
    "\n",
    "        print('Forwarding...')\n",
    "        \n",
    "        img_result = model(imgs).numpy()\n",
    "            \n",
    "        depth_result = model(depths).numpy()\n",
    "            \n",
    "        print(f\"Img and depths feature shapes are: {img_result.shape} and {depth_result.shape}\")\n",
    "        print(\"Saving...\")\n",
    "        np.save('./data/LFSD_imgs_%s_feat.npy' % model_name, img_result)\n",
    "        np.save('./data/LFSD_depths_repeated_%s_feat.npy' % model_name, depth_result)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LFSD feature for mobilenet\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'size' must be a 1-D Tensor of 2 elements: new_height, new_width",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8d69f4fa0a0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mobilenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inception'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'resnet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'efficientnet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vgg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Generating LFSD feature for %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mgen_feat_with_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-838222dadcf1>\u001b[0m in \u001b[0;36mgen_feat_with_model\u001b[1;34m(model_name, imgs, depths)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmobilenet_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mdepths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images_v2\u001b[1;34m(images, size, method, preserve_aspect_ratio, antialias, name)\u001b[0m\n\u001b[0;32m   1639\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Resize method is not implemented: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1641\u001b[1;33m   return _resize_images_common(\n\u001b[0m\u001b[0;32m   1642\u001b[0m       \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m       \u001b[0mresize_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_images_common\u001b[1;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[0;32m   1327\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\'size\\' must be a 1-D int32 Tensor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m       raise ValueError('\\'size\\' must be a 1-D Tensor of 2 elements: '\n\u001b[0m\u001b[0;32m   1330\u001b[0m                        'new_height, new_width')\n\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'size' must be a 1-D Tensor of 2 elements: new_height, new_width"
     ]
    }
   ],
   "source": [
    "depths = np.load('./data/LFSD_depths_repeated.npy')\n",
    "imgs = np.load('./data/LFSD_imgs.npy')\n",
    "for model_name in ['mobilenet', 'inception', 'resnet', 'efficientnet', 'vgg']:\n",
    "    print(\"Generating LFSD feature for %s\" % model_name)\n",
    "    gen_feat_with_model(model_name, imgs, depths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
